{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train llama3 with LoRA\n[Source of this notebook](https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa)\n\nThis notebook is designed for Kaggle notebook with 2 Nvidia T4 GPUs","metadata":{}},{"cell_type":"markdown","source":"### Enviornment setup\n- Set your `HF_TOKEN` at `Add-ons -> Secrets`","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-09T20:32:07.123584Z","iopub.execute_input":"2024-05-09T20:32:07.123917Z","iopub.status.idle":"2024-05-09T20:32:07.274388Z","shell.execute_reply.started":"2024-05-09T20:32:07.123891Z","shell.execute_reply":"2024-05-09T20:32:07.273419Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Install Pytorch for FSDP and FA/SDPA\n%pip install \"torch==2.2.2\" tensorboard\n \n# Install Hugging Face libraries\n%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-09T20:32:07.275934Z","iopub.execute_input":"2024-05-09T20:32:07.276177Z","iopub.status.idle":"2024-05-09T20:40:48.169742Z","shell.execute_reply.started":"2024-05-09T20:32:07.276155Z","shell.execute_reply":"2024-05-09T20:40:48.168627Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torch==2.2.2\n  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.2)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m941.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: accelerate==0.29.3 in /opt/conda/lib/python3.10/site-packages (0.29.3)\nCollecting evaluate==0.4.1\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting bitsandbytes==0.43.1\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: huggingface_hub==0.22.2 in /opt/conda/lib/python3.10/site-packages (0.22.2)\nCollecting trl==0.8.6\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting peft==0.10.0\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (2.2.2)\nCollecting responses<0.19 (from evaluate==0.4.1)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2) (4.9.0)\nCollecting tyro>=0.5.11 (from trl==0.8.6)\n  Downloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.4.127)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, responses, tyro, tokenizers, transformers, bitsandbytes, trl, peft, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed bitsandbytes-0.43.1 evaluate-0.4.1 peft-0.10.0 responses-0.18.0 shtab-1.7.1 tokenizers-0.19.1 transformers-4.40.0 trl-0.8.6 tyro-0.8.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login --token $HF_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-05-09T20:40:48.171423Z","iopub.execute_input":"2024-05-09T20:40:48.171806Z","iopub.status.idle":"2024-05-09T20:40:49.769221Z","shell.execute_reply.started":"2024-05-09T20:40:48.171767Z","shell.execute_reply":"2024-05-09T20:40:49.768075Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load and prepare the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n \n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are a seasoned stock market analyst. What is the summary of this financial text\"\"\"\n\ndef create_conversation(sample):\n#     return {\n#         \"messages\": [\n#             {\"role\": \"system\", \"content\": system_message},\n#             {\"role\": \"user\", \"content\": sample[\"document\"]},\n#             {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n#         ]\n#     }\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"Instruction\"]},\n            {\"role\": \"user\", \"content\": sample[\"Input\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"Output\"]}\n        ]\n    }\n \n# Load dataset from the hub\ndataset = load_dataset(\"ECS289L/Stocksense-Prediction-Next-Week\", split=\"train\")\n# print(dataset)\n# dataset = dataset.select(range(0, 323))\n \n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=30, seed=42)\n\nprint(dataset[\"train\"][123][\"messages\"])\n\n# save datasets to disk\ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-09T20:40:49.772524Z","iopub.execute_input":"2024-05-09T20:40:49.772978Z","iopub.status.idle":"2024-05-09T20:40:54.343872Z","shell.execute_reply.started":"2024-05-09T20:40:49.772936Z","shell.execute_reply":"2024-05-09T20:40:54.342976Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 279k/279k [00:00<00:00, 3.01MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc2d04e4e64468b9a32ea91d5de4f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c145e5d5903e44e7b904c3f161b09de8"}},"metadata":{}},{"name":"stdout","text":"[{'content': \"You are a seasoned stock market analyst. Your task is to predict the companies' stock price movement next week based on this week's positive headlines and negative headlines. Give me answer in this format {increase/decrease} in {percentange} or flat.\", 'role': 'system'}, {'content': 'Company news during this period are listed below:\\n\\nPositive Headlines:\\n* Apple is Quietly Expanding its Artificial Intelligence Ecosystem\\n* Apple Planning to Make Original Podcasts Promoting Its TV Shows\\n* Buy These Top Mobile Payment Providers To Benefit From One Of \\n* DIA To QQQ And SPY: Anything You Can Do - I Can Do Better \\n\\nNegative Headlines:\\n* Apple dropped plan for encrypting backups after FBI complained \\n* Mad Dogs Of The Dow For 2020\\n* Big Tech\\'s \"Political Reckoning\" May Fuel A Stock Market Crash In \\n* Apple\\'s Stock Price And 5G: The Hype And Substance Time Lag \\n* Low-Cost iPhone (AAPL) to Begin Production in February\\n\\n', 'role': 'user'}, {'content': 'increased in 0.18%', 'role': 'assistant'}]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"512f979ada6744aeac7ad1e79923bac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd58bae8075e48f5889d3bfded393f5a"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"32244"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load base model and setup training parameters","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T20:40:54.345132Z","iopub.execute_input":"2024-05-09T20:40:54.345398Z","iopub.status.idle":"2024-05-09T20:40:54.520643Z","shell.execute_reply.started":"2024-05-09T20:40:54.345375Z","shell.execute_reply":"2024-05-09T20:40:54.519774Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e685e09d32404afab0becc2959cee3cf"}},"metadata":{}}]},{"cell_type":"code","source":"%%writefile llama_3_8b_fsdp_qlora.yaml\n# script parameters\nmodel_id: \"meta-llama/Meta-Llama-3-8B-Instruct\" # Hugging Face model id\ndataset_path: \".\"                      # path to dataset\nmax_seq_len:  3072 # 2048              # max sequence length for model and packing of the dataset\n# training parameters\noutput_dir: \"/home/jupyter/llama-3-8b-FinGPT\" # Temporary output directory for model checkpoints\nreport_to: \"tensorboard\"               # report metrics to tensorboard\nlearning_rate: 0.0005                  # learning rate 2e-4\nlr_scheduler_type: \"constant\"          # learning rate scheduler\nnum_train_epochs: 3                    # number of training epochs\nper_device_train_batch_size: 1         # batch size per device during training\nper_device_eval_batch_size: 1          # batch size for evaluation\ngradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\noptim: adamw_torch                     # use torch adamw optimizer\nlogging_steps: 10                      # log every 10 steps\nsave_strategy: epoch                   # save checkpoint every epoch\nevaluation_strategy: epoch             # evaluate every epoch\nmax_grad_norm: 0.3                     # max gradient norm\nwarmup_ratio: 0.03                     # warmup ratio\nbf16: false                             # use bfloat16 precision\ntf32: false                             # use tf32 precision\ngradient_checkpointing: true           # use gradient checkpointing to save memory\nhub_private_repo: true\n# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\nfsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\nfsdp_config:\n  backward_prefetch: \"backward_pre\"\n  forward_prefetch: \"false\"\n  use_orig_params: \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-09T20:40:54.521988Z","iopub.execute_input":"2024-05-09T20:40:54.522394Z","iopub.status.idle":"2024-05-09T20:40:54.530403Z","shell.execute_reply.started":"2024-05-09T20:40:54.522363Z","shell.execute_reply":"2024-05-09T20:40:54.529383Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing llama_3_8b_fsdp_qlora.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_fsdp_qlora.py\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom trl.commands.cli_utils import  TrlParser\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n        set_seed,\n\n)\nfrom trl import setup_chat_format\nfrom peft import LoraConfig\n\n\nfrom trl import (\n   SFTTrainer)\n\n# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n\n# Anthropic/Vicuna like template without the need for special tokens\nLLAMA_3_CHAT_TEMPLATE = (\n    \"{% for message in messages %}\"\n        \"{% if message['role'] == 'system' %}\"\n            \"{{ message['content'] }}\"\n        \"{% elif message['role'] == 'user' %}\"\n            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n        \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n        \"{% endif %}\"\n    \"{% endfor %}\"\n    \"{% if add_generation_prompt %}\"\n    \"{{ '\\n\\nAssistant: ' }}\"\n    \"{% endif %}\"\n)\n\n\n# ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n\n@dataclass\nclass ScriptArguments:\n    dataset_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to the dataset\"\n        },\n    )\n    model_id: str = field(\n        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n    )\n    max_seq_length: int = field(\n        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n    )\n\n\ndef training_function(script_args, training_args):\n    ################\n    # Dataset\n    ################\n    \n    train_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n        split=\"train\",\n    )\n    test_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n        split=\"train\",\n    )\n\n    ################\n    # Model & Tokenizer\n    ################\n\n    # Tokenizer        \n    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n    \n    # template dataset\n    def template_dataset(examples):\n        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n    \n    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    \n    # print random sample\n    with training_args.main_process_first(\n        desc=\"Log a few random samples from the processed training set\"\n    ):\n        for index in random.sample(range(len(train_dataset)), 2):\n            print(train_dataset[index][\"text\"])\n\n    # Model    \n    torch_dtype = torch.bfloat16\n    quant_storage_dtype = torch.bfloat16\n\n    quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n            bnb_4bit_quant_storage=quant_storage_dtype,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.model_id,\n        quantization_config=quantization_config,\n        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n        torch_dtype=quant_storage_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n    )\n    \n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    ################\n    # PEFT\n    ################\n\n    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n    peft_config = LoraConfig(\n        lora_alpha=8,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n#         modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n    )\n\n    ################\n    # Training\n    ################\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        dataset_text_field=\"text\",\n        eval_dataset=test_dataset,\n        peft_config=peft_config,\n        max_seq_length=script_args.max_seq_length,\n        tokenizer=tokenizer,\n        packing=True,\n        dataset_kwargs={\n            \"add_special_tokens\": False,  # We template with special tokens\n            \"append_concat_token\": False,  # No need to add additional separator token\n        },\n    )\n    if trainer.accelerator.is_main_process:\n        trainer.model.print_trainable_parameters()\n\n    ##########################\n    # Train model\n    ##########################\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    ##########################\n    # SAVE MODEL FOR SAGEMAKER\n    ##########################\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n    \nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, TrainingArguments))\n    script_args, training_args = parser.parse_args_and_config()    \n    \n    # set use reentrant to False\n    if training_args.gradient_checkpointing:\n        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n    # set seed\n    set_seed(training_args.seed)\n  \n    # launch training\n    training_function(script_args, training_args)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T20:40:54.531679Z","iopub.execute_input":"2024-05-09T20:40:54.531954Z","iopub.status.idle":"2024-05-09T20:40:54.547958Z","shell.execute_reply.started":"2024-05-09T20:40:54.531931Z","shell.execute_reply":"2024-05-09T20:40:54.547117Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing run_fsdp_qlora.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"markdown","source":"##### Release unreferenced memory in Python","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T20:40:54.549194Z","iopub.execute_input":"2024-05-09T20:40:54.549550Z","iopub.status.idle":"2024-05-09T20:40:54.635597Z","shell.execute_reply.started":"2024-05-09T20:40:54.549518Z","shell.execute_reply":"2024-05-09T20:40:54.634644Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"133"},"metadata":{}}]},{"cell_type":"markdown","source":"##### Start training with torchrun","metadata":{}},{"cell_type":"code","source":"!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=2 ./run_fsdp_qlora.py --config llama_3_8b_fsdp_qlora.yaml","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-09T20:40:54.636803Z","iopub.execute_input":"2024-05-09T20:40:54.637118Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"[2024-05-09 20:40:57,383] torch.distributed.run: [WARNING] \n[2024-05-09 20:40:57,383] torch.distributed.run: [WARNING] *****************************************\n[2024-05-09 20:40:57,383] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-05-09 20:40:57,383] torch.distributed.run: [WARNING] *****************************************\n2024-05-09 20:41:05.893515: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 20:41:05.893508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 20:41:05.893571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 20:41:05.893627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 20:41:06.014642: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-09 20:41:06.014651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 268 examples [00:00, 45621.72 examples/s]\nGenerating train split: 30 examples [00:00, 12843.64 examples/s]\ntokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 5.44MB/s]\ntokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 27.3MB/s]\nspecial_tokens_map.json: 100%|████████████████| 73.0/73.0 [00:00<00:00, 474kB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nMap: 100%|███████████████████████████| 268/268 [00:00<00:00, 3592.20 examples/s]\nMap: 100%|████████████████████████████| 268/268 [00:00<00:00, 726.60 examples/s]\nMap: 100%|█████████████████████████████| 30/30 [00:00<00:00, 3454.47 examples/s]\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement next week based on this week's positive headlines and negative headlines. Give me answer in this format {increase/decrease} in {percentange} or flat.\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Hot Mutual Fund Loves Facebook, Amazon, Apple, Netflix, Google\n* Apple Car may automatically tint windows for better safety and privacy\n* Apple's MagSafe wallet review: finally solves the unified iPhone and wallet issue\n* 3 Index Funds Perfect for Your IRA\n* How to watch 'A Charlie Brown Thanksgiving' free on Apple TV+\n\nNegative Headlines:\n* France sends ‘Apple tax’ demands; sparks new trade war fears\n* Apple investigating using Apple Watch to continually measure blood pressure\n* UK blames Apple and Amazon for 'tsunami' of electronic waste\n* M1 Mac mini can drive six displays with peripherals - but you shouldn't bother\n* Apple Stock Is Falling Because the Price Already Reflects iPhone 12 Demand\n\n<|end_of_text|>\n\nAssistant: increased in 2.69%<|end_of_text|>\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement next week based on this week's positive headlines and negative headlines. Give me answer in this format {increase/decrease} in {percentange} or flat.\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Apple Stock Set to Gain Because iPhone 11 Is Just Like iPhone 7\n* How to identify U.S. stocks with attractive growth potential\n* Apple's sustainability goals continue to grow, encouraging others to follow its lead\n* iPhone 11 versus Pixel 4 — Benchmark and hands on comparison\n* Apple urges customers to keep data safe in new 'Privacy on iPhone' ad\n\nNegative Headlines:\n* Antitrust fears muted as Amazon, Apple, Google and Facebook approach earnings\n* iPhone owners across US impacted by weeks-long AT&T voicemail outage\n* Tesla Stock Is on Fire and Shorts Are Feeling the Heat\n* Selling Puts is not as Risky as it Sounds\n\n<|end_of_text|>\n\nAssistant: increased in 2.72%<|end_of_text|>\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement next week based on this week's positive headlines and negative headlines. Give me answer in this format {increase/decrease} in {percentange} or flat.\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Hot Mutual Fund Loves Facebook, Amazon, Apple, Netflix, Google\n* Apple Car may automatically tint windows for better safety and privacy\n* Apple's MagSafe wallet review: finally solves the unified iPhone and wallet issue\n* 3 Index Funds Perfect for Your IRA\n* How to watch 'A Charlie Brown Thanksgiving' free on Apple TV+\n\nNegative Headlines:\n* France sends ‘Apple tax’ demands; sparks new trade war fears\n* Apple investigating using Apple Watch to continually measure blood pressure\n* UK blames Apple and Amazon for 'tsunami' of electronic waste\n* M1 Mac mini can drive six displays with peripherals - but you shouldn't bother\n* Apple Stock Is Falling Because the Price Already Reflects iPhone 12 Demand\n\n<|end_of_text|>\n\nAssistant: increased in 2.69%<|end_of_text|>\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement next week based on this week's positive headlines and negative headlines. Give me answer in this format {increase/decrease} in {percentange} or flat.\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Apple Stock Set to Gain Because iPhone 11 Is Just Like iPhone 7\n* How to identify U.S. stocks with attractive growth potential\n* Apple's sustainability goals continue to grow, encouraging others to follow its lead\n* iPhone 11 versus Pixel 4 — Benchmark and hands on comparison\n* Apple urges customers to keep data safe in new 'Privacy on iPhone' ad\n\nNegative Headlines:\n* Antitrust fears muted as Amazon, Apple, Google and Facebook approach earnings\n* iPhone owners across US impacted by weeks-long AT&T voicemail outage\n* Tesla Stock Is on Fire and Shorts Are Feeling the Heat\n* Selling Puts is not as Risky as it Sounds\n\n<|end_of_text|>\n\nAssistant: increased in 2.72%<|end_of_text|>\nconfig.json: 100%|█████████████████████████████| 654/654 [00:00<00:00, 3.07MB/s]\nmodel.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 66.9MB/s]\nDownloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\nDownloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\u001b[A\nmodel-00001-of-00004.safetensors:   0%|     | 21.0M/4.98G [00:00<00:33, 148MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   1%|     | 52.4M/4.98G [00:00<00:22, 222MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   2%|     | 83.9M/4.98G [00:00<00:19, 250MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   2%|▏     | 115M/4.98G [00:00<00:18, 266MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   3%|▏     | 147M/4.98G [00:00<00:17, 269MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   4%|▏     | 178M/4.98G [00:00<00:17, 270MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   4%|▎     | 210M/4.98G [00:00<00:17, 272MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   5%|▎     | 241M/4.98G [00:00<00:17, 271MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   5%|▎     | 273M/4.98G [00:01<00:16, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   6%|▎     | 304M/4.98G [00:01<00:16, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   7%|▍     | 336M/4.98G [00:01<00:16, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   7%|▍     | 367M/4.98G [00:01<00:16, 279MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   8%|▍     | 398M/4.98G [00:01<00:16, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   9%|▌     | 430M/4.98G [00:01<00:16, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   9%|▌     | 461M/4.98G [00:01<00:15, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  10%|▌     | 503M/4.98G [00:01<00:15, 291MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  11%|▋     | 535M/4.98G [00:01<00:15, 291MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  11%|▋     | 566M/4.98G [00:02<00:15, 285MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  12%|▋     | 598M/4.98G [00:02<00:15, 287MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  13%|▊     | 629M/4.98G [00:02<00:15, 286MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  13%|▊     | 661M/4.98G [00:02<00:15, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  14%|▊     | 692M/4.98G [00:02<00:15, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  15%|▊     | 724M/4.98G [00:02<00:14, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  15%|▉     | 755M/4.98G [00:02<00:14, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  16%|▉     | 786M/4.98G [00:02<00:14, 287MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  16%|▉     | 818M/4.98G [00:02<00:14, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  17%|█     | 849M/4.98G [00:03<00:14, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  18%|█     | 881M/4.98G [00:03<00:14, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  18%|█     | 912M/4.98G [00:03<00:14, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  19%|█▏    | 944M/4.98G [00:03<00:14, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  20%|█▏    | 975M/4.98G [00:03<00:14, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  20%|█    | 1.01G/4.98G [00:03<00:14, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  21%|█    | 1.04G/4.98G [00:03<00:13, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  21%|█    | 1.07G/4.98G [00:03<00:13, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  22%|█    | 1.10G/4.98G [00:03<00:13, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  23%|█▏   | 1.13G/4.98G [00:04<00:13, 277MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  23%|█▏   | 1.16G/4.98G [00:04<00:13, 277MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  24%|█▏   | 1.20G/4.98G [00:04<00:13, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  25%|█▏   | 1.23G/4.98G [00:04<00:13, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  25%|█▎   | 1.26G/4.98G [00:04<00:13, 277MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  26%|█▎   | 1.29G/4.98G [00:04<00:13, 279MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  27%|█▎   | 1.32G/4.98G [00:04<00:13, 279MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  27%|█▎   | 1.35G/4.98G [00:04<00:13, 274MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  28%|█▍   | 1.38G/4.98G [00:04<00:13, 276MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  28%|█▍   | 1.42G/4.98G [00:05<00:12, 276MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  29%|█▍   | 1.45G/4.98G [00:05<00:12, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  30%|█▍   | 1.48G/4.98G [00:05<00:12, 280MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  30%|█▌   | 1.51G/4.98G [00:05<00:12, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  31%|█▌   | 1.54G/4.98G [00:05<00:12, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  32%|█▌   | 1.57G/4.98G [00:05<00:11, 286MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  32%|█▌   | 1.60G/4.98G [00:05<00:11, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  33%|█▋   | 1.64G/4.98G [00:05<00:11, 285MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  34%|█▋   | 1.67G/4.98G [00:05<00:11, 286MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  34%|█▋   | 1.71G/4.98G [00:06<00:10, 298MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  35%|█▋   | 1.74G/4.98G [00:06<00:10, 298MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  36%|█▊   | 1.77G/4.98G [00:06<00:10, 298MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  36%|█▊   | 1.81G/4.98G [00:06<00:10, 303MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  37%|█▊   | 1.85G/4.98G [00:06<00:10, 304MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  38%|█▉   | 1.89G/4.98G [00:06<00:10, 308MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  39%|█▉   | 1.92G/4.98G [00:06<00:09, 308MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  39%|█▉   | 1.95G/4.98G [00:06<00:09, 305MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  40%|█▉   | 1.98G/4.98G [00:07<00:10, 295MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  40%|██   | 2.01G/4.98G [00:07<00:10, 290MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  41%|██   | 2.04G/4.98G [00:07<00:10, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  42%|██   | 2.08G/4.98G [00:07<00:10, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  42%|██   | 2.11G/4.98G [00:07<00:10, 279MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  43%|██▏  | 2.14G/4.98G [00:07<00:10, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  44%|██▏  | 2.17G/4.98G [00:07<00:09, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  44%|██▏  | 2.20G/4.98G [00:07<00:09, 290MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  45%|██▎  | 2.24G/4.98G [00:07<00:09, 301MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  46%|██▎  | 2.28G/4.98G [00:08<00:08, 302MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  46%|██▎  | 2.31G/4.98G [00:08<00:09, 296MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  47%|██▎  | 2.34G/4.98G [00:08<00:09, 293MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  48%|██▍  | 2.37G/4.98G [00:08<00:09, 287MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  48%|██▍  | 2.40G/4.98G [00:08<00:09, 285MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  49%|██▍  | 2.43G/4.98G [00:08<00:08, 289MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  50%|██▍  | 2.46G/4.98G [00:08<00:08, 293MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  50%|██▌  | 2.50G/4.98G [00:08<00:08, 290MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  51%|██▌  | 2.53G/4.98G [00:08<00:08, 294MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  51%|██▌  | 2.56G/4.98G [00:09<00:08, 295MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  52%|██▌  | 2.59G/4.98G [00:09<00:07, 299MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  53%|██▋  | 2.63G/4.98G [00:09<00:07, 307MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  54%|██▋  | 2.66G/4.98G [00:09<00:07, 309MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  54%|██▋  | 2.69G/4.98G [00:09<00:07, 309MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  55%|██▋  | 2.73G/4.98G [00:09<00:07, 308MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  55%|██▊  | 2.76G/4.98G [00:09<00:07, 309MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  56%|██▊  | 2.79G/4.98G [00:09<00:07, 306MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  57%|██▊  | 2.83G/4.98G [00:09<00:06, 313MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  58%|██▉  | 2.86G/4.98G [00:09<00:07, 302MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  58%|██▉  | 2.89G/4.98G [00:10<00:07, 288MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  59%|██▉  | 2.93G/4.98G [00:10<00:07, 277MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  59%|██▉  | 2.96G/4.98G [00:10<00:07, 275MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  60%|███  | 2.99G/4.98G [00:10<00:07, 270MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  61%|███  | 3.02G/4.98G [00:10<00:07, 251MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  61%|███  | 3.05G/4.98G [00:10<00:07, 254MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  62%|███  | 3.08G/4.98G [00:10<00:07, 255MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  63%|███▏ | 3.11G/4.98G [00:10<00:07, 258MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  63%|███▏ | 3.15G/4.98G [00:11<00:07, 259MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  64%|███▏ | 3.18G/4.98G [00:11<00:06, 259MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  64%|███▏ | 3.21G/4.98G [00:11<00:06, 259MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  65%|███▎ | 3.24G/4.98G [00:11<00:06, 251MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  66%|███▎ | 3.27G/4.98G [00:11<00:06, 257MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  66%|██▋ | 3.30G/4.98G [00:13<00:30, 54.2MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  67%|██▋ | 3.32G/4.98G [00:13<00:25, 63.9MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  67%|██▋ | 3.36G/4.98G [00:13<00:19, 82.9MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  68%|███▍ | 3.39G/4.98G [00:13<00:15, 105MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  69%|███▍ | 3.42G/4.98G [00:13<00:12, 129MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  69%|███▍ | 3.45G/4.98G [00:13<00:09, 155MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  70%|███▍ | 3.48G/4.98G [00:13<00:08, 177MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  71%|███▌ | 3.51G/4.98G [00:14<00:07, 198MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  71%|███▌ | 3.54G/4.98G [00:14<00:06, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  72%|███▌ | 3.58G/4.98G [00:14<00:06, 229MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  72%|███▌ | 3.61G/4.98G [00:14<00:05, 237MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  73%|███▋ | 3.64G/4.98G [00:14<00:05, 246MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  74%|███▋ | 3.67G/4.98G [00:14<00:05, 257MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  74%|███▋ | 3.70G/4.98G [00:14<00:04, 270MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  75%|███▊ | 3.73G/4.98G [00:14<00:04, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  76%|███▊ | 3.76G/4.98G [00:14<00:04, 282MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  76%|███▊ | 3.80G/4.98G [00:15<00:04, 287MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  77%|███▊ | 3.83G/4.98G [00:15<00:03, 292MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  78%|███▉ | 3.86G/4.98G [00:15<00:03, 292MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  78%|███▉ | 3.90G/4.98G [00:15<00:03, 302MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  79%|███▉ | 3.93G/4.98G [00:15<00:03, 299MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  80%|███▉ | 3.96G/4.98G [00:15<00:03, 303MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  80%|████ | 4.01G/4.98G [00:15<00:03, 308MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  81%|████ | 4.04G/4.98G [00:15<00:03, 305MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  82%|████ | 4.07G/4.98G [00:15<00:02, 306MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  82%|████ | 4.10G/4.98G [00:16<00:02, 298MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  83%|████▏| 4.13G/4.98G [00:16<00:02, 295MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  84%|████▏| 4.16G/4.98G [00:16<00:02, 288MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  84%|████▏| 4.19G/4.98G [00:16<00:02, 281MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  85%|████▏| 4.23G/4.98G [00:16<00:02, 276MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  86%|████▎| 4.26G/4.98G [00:16<00:02, 272MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  86%|████▎| 4.29G/4.98G [00:16<00:02, 269MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  87%|████▎| 4.32G/4.98G [00:16<00:02, 273MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  87%|████▎| 4.35G/4.98G [00:17<00:02, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  88%|████▍| 4.38G/4.98G [00:17<00:02, 283MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  89%|████▍| 4.41G/4.98G [00:17<00:01, 284MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  89%|████▍| 4.45G/4.98G [00:17<00:01, 276MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  90%|████▍| 4.48G/4.98G [00:17<00:01, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  91%|████▌| 4.51G/4.98G [00:17<00:01, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  91%|████▌| 4.54G/4.98G [00:17<00:01, 278MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  92%|████▌| 4.57G/4.98G [00:17<00:01, 271MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  92%|████▌| 4.60G/4.98G [00:17<00:01, 270MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  93%|████▋| 4.63G/4.98G [00:18<00:01, 269MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  94%|████▋| 4.67G/4.98G [00:18<00:01, 270MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  94%|████▋| 4.70G/4.98G [00:18<00:01, 269MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  95%|████▊| 4.73G/4.98G [00:18<00:00, 266MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  96%|████▊| 4.76G/4.98G [00:18<00:00, 266MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  96%|████▊| 4.79G/4.98G [00:18<00:00, 265MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  97%|████▊| 4.82G/4.98G [00:18<00:00, 269MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  98%|████▉| 4.85G/4.98G [00:18<00:00, 271MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  98%|████▉| 4.89G/4.98G [00:18<00:00, 274MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  99%|████▉| 4.92G/4.98G [00:19<00:00, 272MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  99%|████▉| 4.95G/4.98G [00:19<00:00, 271MB/s]\u001b[A\nmodel-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:19<00:00, 257MB/s]\u001b[A\nDownloading shards:  25%|██████▎                  | 1/4 [00:19<00:58, 19.35s/it]\nmodel-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00004.safetensors:   1%|     | 31.5M/5.00G [00:00<00:16, 299MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   1%|     | 62.9M/5.00G [00:00<00:17, 278MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   2%|     | 94.4M/5.00G [00:00<00:17, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   3%|▏     | 126M/5.00G [00:00<00:16, 288MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   3%|▏     | 157M/5.00G [00:00<00:16, 295MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   4%|▏     | 189M/5.00G [00:00<00:16, 298MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   4%|▎     | 220M/5.00G [00:00<00:15, 299MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   5%|▎     | 252M/5.00G [00:00<00:16, 295MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   6%|▎     | 283M/5.00G [00:00<00:15, 299MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   7%|▍     | 325M/5.00G [00:01<00:15, 301MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   7%|▍     | 357M/5.00G [00:01<00:24, 188MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   8%|▍     | 388M/5.00G [00:01<00:22, 207MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   8%|▌     | 419M/5.00G [00:01<00:20, 227MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   9%|▌     | 451M/5.00G [00:01<00:19, 238MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  10%|▌     | 482M/5.00G [00:01<00:18, 249MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  10%|▌     | 514M/5.00G [00:01<00:17, 258MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  11%|▋     | 545M/5.00G [00:02<00:17, 261MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  12%|▋     | 577M/5.00G [00:02<00:16, 267MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  12%|▋     | 608M/5.00G [00:02<00:16, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  13%|▊     | 640M/5.00G [00:02<00:15, 277MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  13%|▊     | 671M/5.00G [00:02<00:15, 276MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  14%|▊     | 703M/5.00G [00:02<00:15, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  15%|▉     | 734M/5.00G [00:02<00:15, 272MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  15%|▉     | 765M/5.00G [00:02<00:15, 273MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  16%|▉     | 797M/5.00G [00:03<00:15, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  17%|▉     | 828M/5.00G [00:03<00:15, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  17%|█     | 860M/5.00G [00:03<00:15, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  18%|█     | 891M/5.00G [00:03<00:15, 264MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  18%|█     | 923M/5.00G [00:03<00:15, 265MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  19%|█▏    | 954M/5.00G [00:03<00:15, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  20%|█▏    | 986M/5.00G [00:03<00:15, 266MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  20%|█    | 1.02G/5.00G [00:03<00:14, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  21%|█    | 1.05G/5.00G [00:03<00:14, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  22%|█    | 1.08G/5.00G [00:04<00:14, 267MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  22%|▉   | 1.11G/5.00G [00:06<01:43, 37.7MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  23%|▉   | 1.14G/5.00G [00:06<01:16, 50.4MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  23%|▉   | 1.17G/5.00G [00:06<00:57, 66.3MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  24%|▉   | 1.21G/5.00G [00:06<00:44, 85.6MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  25%|█▏   | 1.24G/5.00G [00:07<00:34, 108MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  25%|█▎   | 1.27G/5.00G [00:07<00:28, 132MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  26%|█▎   | 1.30G/5.00G [00:07<00:23, 156MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  27%|█▎   | 1.33G/5.00G [00:07<00:20, 178MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  27%|█▎   | 1.36G/5.00G [00:07<00:18, 198MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  28%|█▍   | 1.39G/5.00G [00:07<00:16, 213MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  29%|█▍   | 1.43G/5.00G [00:07<00:15, 228MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  29%|█▍   | 1.46G/5.00G [00:07<00:14, 240MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  30%|█▍   | 1.49G/5.00G [00:08<00:14, 250MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  30%|█▌   | 1.52G/5.00G [00:08<00:13, 256MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  31%|█▌   | 1.55G/5.00G [00:08<00:13, 260MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  32%|█▌   | 1.58G/5.00G [00:08<00:12, 267MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  32%|█▌   | 1.61G/5.00G [00:08<00:12, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  33%|█▋   | 1.65G/5.00G [00:08<00:12, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  34%|█▋   | 1.68G/5.00G [00:08<00:12, 259MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  34%|█▋   | 1.71G/5.00G [00:08<00:12, 258MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  35%|█▋   | 1.74G/5.00G [00:08<00:12, 261MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  35%|█▊   | 1.77G/5.00G [00:09<00:12, 265MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  36%|█▊   | 1.80G/5.00G [00:09<00:12, 262MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  37%|█▊   | 1.84G/5.00G [00:09<00:11, 267MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  37%|█▊   | 1.87G/5.00G [00:09<00:11, 266MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  38%|█▉   | 1.90G/5.00G [00:09<00:11, 265MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  39%|█▉   | 1.93G/5.00G [00:09<00:11, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  39%|█▉   | 1.96G/5.00G [00:09<00:11, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  40%|█▉   | 1.99G/5.00G [00:09<00:11, 271MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  40%|██   | 2.02G/5.00G [00:09<00:10, 276MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  41%|██   | 2.06G/5.00G [00:10<00:10, 281MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  42%|██   | 2.09G/5.00G [00:10<00:10, 284MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  42%|██   | 2.12G/5.00G [00:10<00:10, 286MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  43%|██▏  | 2.15G/5.00G [00:10<00:10, 283MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  44%|██▏  | 2.18G/5.00G [00:10<00:10, 277MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  44%|██▏  | 2.21G/5.00G [00:10<00:10, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  45%|██▏  | 2.24G/5.00G [00:10<00:10, 272MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  46%|██▎  | 2.28G/5.00G [00:10<00:10, 272MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  46%|██▎  | 2.31G/5.00G [00:11<00:09, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  47%|██▎  | 2.34G/5.00G [00:11<00:09, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  47%|██▎  | 2.37G/5.00G [00:11<00:09, 277MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  48%|██▍  | 2.41G/5.00G [00:11<00:08, 289MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  49%|██▍  | 2.44G/5.00G [00:11<00:08, 290MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  49%|██▍  | 2.47G/5.00G [00:11<00:08, 288MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  50%|██▌  | 2.51G/5.00G [00:11<00:08, 285MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  51%|██▌  | 2.54G/5.00G [00:11<00:08, 281MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  51%|██▌  | 2.57G/5.00G [00:11<00:08, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  52%|██▌  | 2.60G/5.00G [00:12<00:08, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  53%|██▋  | 2.63G/5.00G [00:12<00:08, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  53%|██▋  | 2.66G/5.00G [00:12<00:08, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  54%|██▋  | 2.69G/5.00G [00:12<00:08, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  55%|██▋  | 2.73G/5.00G [00:12<00:08, 282MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  55%|██▊  | 2.76G/5.00G [00:12<00:09, 243MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  56%|██▊  | 2.79G/5.00G [00:12<00:08, 257MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  56%|██▊  | 2.82G/5.00G [00:12<00:08, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  57%|██▊  | 2.85G/5.00G [00:13<00:07, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  58%|██▉  | 2.88G/5.00G [00:13<00:07, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  58%|██▉  | 2.92G/5.00G [00:13<00:07, 279MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  59%|██▉  | 2.95G/5.00G [00:13<00:07, 280MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  60%|██▉  | 2.98G/5.00G [00:13<00:07, 287MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  60%|███  | 3.01G/5.00G [00:13<00:06, 286MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  61%|███  | 3.04G/5.00G [00:13<00:06, 289MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  61%|███  | 3.07G/5.00G [00:13<00:06, 293MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  62%|███  | 3.10G/5.00G [00:13<00:06, 291MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  63%|███▏ | 3.14G/5.00G [00:13<00:06, 285MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  63%|███▏ | 3.17G/5.00G [00:14<00:06, 282MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  64%|███▏ | 3.20G/5.00G [00:14<00:06, 285MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  65%|███▏ | 3.23G/5.00G [00:14<00:06, 285MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  65%|███▎ | 3.26G/5.00G [00:14<00:05, 290MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  66%|███▎ | 3.30G/5.00G [00:14<00:05, 296MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  67%|███▎ | 3.33G/5.00G [00:14<00:05, 301MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  67%|███▎ | 3.37G/5.00G [00:14<00:05, 295MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  68%|███▍ | 3.40G/5.00G [00:14<00:05, 288MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  69%|███▍ | 3.43G/5.00G [00:15<00:05, 278MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  69%|███▍ | 3.46G/5.00G [00:15<00:05, 278MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  70%|███▍ | 3.49G/5.00G [00:15<00:05, 273MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  70%|███▌ | 3.52G/5.00G [00:15<00:05, 272MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  71%|███▌ | 3.55G/5.00G [00:15<00:05, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  72%|███▌ | 3.59G/5.00G [00:15<00:05, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  72%|███▌ | 3.62G/5.00G [00:15<00:05, 275MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  73%|███▋ | 3.65G/5.00G [00:15<00:04, 276MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  74%|███▋ | 3.68G/5.00G [00:15<00:04, 277MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  74%|███▋ | 3.71G/5.00G [00:16<00:04, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  75%|███▋ | 3.74G/5.00G [00:16<00:04, 273MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  76%|███▊ | 3.77G/5.00G [00:16<00:04, 272MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  76%|███▊ | 3.81G/5.00G [00:16<00:04, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  77%|███▊ | 3.84G/5.00G [00:16<00:04, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  77%|███▊ | 3.87G/5.00G [00:16<00:04, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  78%|███▉ | 3.90G/5.00G [00:16<00:04, 266MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  79%|███▉ | 3.93G/5.00G [00:16<00:03, 269MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  79%|███▉ | 3.96G/5.00G [00:16<00:03, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  80%|███▉ | 4.00G/5.00G [00:17<00:03, 268MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  81%|████ | 4.03G/5.00G [00:17<00:03, 270MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  81%|████ | 4.06G/5.00G [00:17<00:03, 271MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  82%|████ | 4.09G/5.00G [00:17<00:03, 276MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  82%|████ | 4.12G/5.00G [00:17<00:03, 280MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  83%|████▏| 4.15G/5.00G [00:17<00:02, 284MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  84%|████▏| 4.19G/5.00G [00:17<00:02, 295MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  85%|████▏| 4.23G/5.00G [00:17<00:02, 296MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  85%|████▎| 4.26G/5.00G [00:18<00:02, 293MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  86%|████▎| 4.29G/5.00G [00:18<00:02, 288MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  86%|████▎| 4.32G/5.00G [00:18<00:02, 281MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  87%|████▎| 4.35G/5.00G [00:18<00:02, 278MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  88%|████▍| 4.38G/5.00G [00:18<00:02, 274MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  88%|████▍| 4.41G/5.00G [00:18<00:02, 276MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  89%|████▍| 4.45G/5.00G [00:18<00:03, 176MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  90%|████▍| 4.48G/5.00G [00:19<00:02, 199MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  90%|████▌| 4.51G/5.00G [00:19<00:02, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  91%|████▌| 4.54G/5.00G [00:19<00:01, 236MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  91%|████▌| 4.57G/5.00G [00:19<00:01, 245MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  92%|████▌| 4.60G/5.00G [00:19<00:01, 252MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  93%|████▋| 4.63G/5.00G [00:19<00:02, 162MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  93%|████▋| 4.67G/5.00G [00:20<00:02, 139MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  94%|████▋| 4.69G/5.00G [00:20<00:02, 139MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  94%|████▋| 4.71G/5.00G [00:20<00:02, 140MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  95%|████▋| 4.73G/5.00G [00:20<00:01, 146MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  95%|████▊| 4.75G/5.00G [00:20<00:01, 152MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  95%|████▊| 4.77G/5.00G [00:20<00:01, 156MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  96%|████▊| 4.79G/5.00G [00:20<00:01, 163MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  96%|████▊| 4.81G/5.00G [00:21<00:01, 167MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  97%|████▊| 4.83G/5.00G [00:21<00:00, 174MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  97%|████▊| 4.85G/5.00G [00:21<00:00, 178MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  98%|████▉| 4.88G/5.00G [00:21<00:00, 183MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  98%|████▉| 4.91G/5.00G [00:21<00:00, 196MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  99%|████▉| 4.94G/5.00G [00:21<00:00, 209MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  99%|████▉| 4.97G/5.00G [00:21<00:00, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:21<00:00, 228MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 2/4 [00:41<00:41, 20.92s/it]\nmodel-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\nmodel-00003-of-00004.safetensors:   0%|     | 21.0M/4.92G [00:00<00:32, 153MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   1%|     | 41.9M/4.92G [00:00<00:27, 177MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   1%|     | 73.4M/4.92G [00:00<00:21, 225MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   2%|▏     | 105M/4.92G [00:00<00:20, 239MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   3%|▏     | 136M/4.92G [00:00<00:19, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   3%|▏     | 168M/4.92G [00:00<00:19, 246MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   4%|▏     | 199M/4.92G [00:00<00:18, 250MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   5%|▎     | 231M/4.92G [00:00<00:18, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   5%|▎     | 262M/4.92G [00:01<00:18, 251MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   6%|▎     | 294M/4.92G [00:01<00:18, 256MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   7%|▍     | 325M/4.92G [00:01<00:17, 256MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   7%|▍     | 357M/4.92G [00:01<00:17, 262MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   8%|▍     | 388M/4.92G [00:01<00:17, 260MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   9%|▌     | 419M/4.92G [00:01<00:17, 260MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   9%|▌     | 451M/4.92G [00:01<00:17, 263MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  10%|▌     | 482M/4.92G [00:01<00:16, 263MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  10%|▋     | 514M/4.92G [00:02<00:16, 263MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  11%|▋     | 545M/4.92G [00:02<00:16, 264MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  12%|▋     | 577M/4.92G [00:02<00:16, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  12%|▋     | 608M/4.92G [00:02<00:16, 263MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  13%|▊     | 640M/4.92G [00:02<00:16, 267MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  14%|▊     | 671M/4.92G [00:02<00:15, 266MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  14%|▊     | 703M/4.92G [00:02<00:15, 272MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  15%|▉     | 734M/4.92G [00:02<00:15, 275MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  16%|▉     | 765M/4.92G [00:02<00:14, 278MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  16%|▉     | 797M/4.92G [00:03<00:15, 274MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  17%|█     | 828M/4.92G [00:03<00:14, 275MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  17%|█     | 860M/4.92G [00:03<00:14, 274MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  18%|█     | 891M/4.92G [00:03<00:14, 273MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  19%|█▏    | 923M/4.92G [00:03<00:14, 272MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  19%|█▏    | 954M/4.92G [00:03<00:14, 267MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  20%|█▏    | 986M/4.92G [00:03<00:14, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  21%|█    | 1.02G/4.92G [00:03<00:14, 266MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  21%|█    | 1.05G/4.92G [00:04<00:14, 274MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  22%|█    | 1.08G/4.92G [00:04<00:13, 278MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  23%|█▏   | 1.11G/4.92G [00:04<00:13, 281MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  23%|█▏   | 1.14G/4.92G [00:04<00:13, 288MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  24%|█▏   | 1.18G/4.92G [00:04<00:12, 298MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  25%|█▏   | 1.22G/4.92G [00:04<00:12, 299MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  25%|█▎   | 1.25G/4.92G [00:04<00:12, 296MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  26%|█▎   | 1.28G/4.92G [00:04<00:12, 298MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  27%|█▎   | 1.31G/4.92G [00:04<00:12, 296MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  27%|█▎   | 1.34G/4.92G [00:05<00:12, 294MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  28%|█▍   | 1.37G/4.92G [00:05<00:12, 295MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  29%|█▍   | 1.41G/4.92G [00:05<00:11, 294MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  29%|█▍   | 1.44G/4.92G [00:05<00:12, 286MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  30%|█▍   | 1.47G/4.92G [00:05<00:12, 281MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  31%|█▌   | 1.50G/4.92G [00:05<00:12, 273MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  31%|█▌   | 1.53G/4.92G [00:05<00:12, 271MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  32%|█▌   | 1.56G/4.92G [00:05<00:13, 240MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  32%|█▌   | 1.59G/4.92G [00:06<00:20, 163MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  33%|█▋   | 1.61G/4.92G [00:06<00:22, 146MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  33%|█▋   | 1.64G/4.92G [00:06<00:24, 136MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  34%|█▋   | 1.66G/4.92G [00:06<00:24, 131MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  34%|█▋   | 1.68G/4.92G [00:06<00:25, 126MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  35%|█▋   | 1.70G/4.92G [00:07<00:25, 127MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  35%|█▋   | 1.72G/4.92G [00:07<00:23, 139MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  35%|█▊   | 1.74G/4.92G [00:07<00:21, 151MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  36%|█▊   | 1.77G/4.92G [00:07<00:17, 180MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  37%|█▊   | 1.80G/4.92G [00:07<00:15, 197MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  37%|█▊   | 1.84G/4.92G [00:07<00:14, 207MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  38%|█▉   | 1.87G/4.92G [00:07<00:13, 225MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  39%|█▉   | 1.90G/4.92G [00:07<00:12, 239MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  39%|█▉   | 1.93G/4.92G [00:08<00:12, 240MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  40%|█▉   | 1.96G/4.92G [00:08<00:12, 240MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  41%|██   | 1.99G/4.92G [00:08<00:11, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  41%|██   | 2.02G/4.92G [00:08<00:11, 257MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  42%|██   | 2.06G/4.92G [00:08<00:12, 236MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  42%|██   | 2.09G/4.92G [00:08<00:11, 236MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  43%|██▏  | 2.12G/4.92G [00:08<00:11, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  44%|██▏  | 2.15G/4.92G [00:08<00:10, 256MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  44%|██▏  | 2.18G/4.92G [00:09<00:10, 250MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  45%|██▎  | 2.21G/4.92G [00:09<00:10, 251MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  46%|██▎  | 2.24G/4.92G [00:09<00:10, 256MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  46%|██▎  | 2.28G/4.92G [00:09<00:10, 251MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  47%|██▎  | 2.31G/4.92G [00:09<00:10, 255MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  48%|██▍  | 2.34G/4.92G [00:09<00:10, 257MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  48%|██▍  | 2.37G/4.92G [00:09<00:09, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  49%|██▍  | 2.40G/4.92G [00:09<00:09, 259MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  49%|██▍  | 2.43G/4.92G [00:10<00:10, 244MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  50%|██▌  | 2.46G/4.92G [00:10<00:12, 194MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  51%|██▌  | 2.50G/4.92G [00:10<00:14, 167MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  51%|██  | 2.52G/4.92G [00:12<00:59, 40.6MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  52%|██  | 2.54G/4.92G [00:12<00:49, 47.8MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  52%|██  | 2.57G/4.92G [00:12<00:35, 65.6MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  53%|██  | 2.60G/4.92G [00:12<00:26, 86.7MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  54%|██▋  | 2.63G/4.92G [00:13<00:20, 110MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  54%|██▋  | 2.66G/4.92G [00:13<00:16, 133MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  55%|██▋  | 2.69G/4.92G [00:13<00:14, 155MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  55%|██▊  | 2.73G/4.92G [00:13<00:12, 177MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  56%|██▊  | 2.76G/4.92G [00:13<00:10, 197MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  57%|██▊  | 2.79G/4.92G [00:13<00:10, 209MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  57%|██▊  | 2.82G/4.92G [00:13<00:09, 223MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  58%|██▉  | 2.85G/4.92G [00:13<00:08, 231MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  59%|██▉  | 2.88G/4.92G [00:14<00:08, 238MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  59%|██▉  | 2.92G/4.92G [00:14<00:08, 244MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  60%|██▉  | 2.95G/4.92G [00:14<00:07, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  61%|███  | 2.98G/4.92G [00:14<00:07, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  61%|███  | 3.01G/4.92G [00:14<00:07, 249MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  62%|███  | 3.04G/4.92G [00:14<00:07, 247MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  62%|██▍ | 3.07G/4.92G [00:16<00:38, 47.9MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  63%|██▌ | 3.10G/4.92G [00:16<00:28, 63.0MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  64%|██▌ | 3.14G/4.92G [00:16<00:21, 82.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  64%|███▏ | 3.17G/4.92G [00:16<00:16, 105MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  65%|███▎ | 3.20G/4.92G [00:16<00:13, 131MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  66%|███▎ | 3.24G/4.92G [00:17<00:10, 165MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  67%|███▎ | 3.28G/4.92G [00:17<00:08, 198MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  67%|███▎ | 3.31G/4.92G [00:17<00:07, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  68%|███▍ | 3.34G/4.92G [00:17<00:06, 236MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  69%|███▍ | 3.38G/4.92G [00:17<00:06, 248MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  69%|███▍ | 3.41G/4.92G [00:17<00:05, 256MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  70%|███▍ | 3.44G/4.92G [00:17<00:05, 258MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  71%|███▌ | 3.47G/4.92G [00:17<00:05, 264MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  71%|███▌ | 3.50G/4.92G [00:18<00:05, 272MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  72%|███▌ | 3.53G/4.92G [00:18<00:04, 282MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  73%|███▋ | 3.57G/4.92G [00:18<00:04, 284MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  73%|███▋ | 3.60G/4.92G [00:18<00:04, 290MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  74%|███▋ | 3.63G/4.92G [00:18<00:04, 292MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  74%|███▋ | 3.66G/4.92G [00:18<00:04, 286MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  75%|███▊ | 3.69G/4.92G [00:18<00:04, 276MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  76%|███▊ | 3.72G/4.92G [00:18<00:04, 274MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  76%|███▊ | 3.75G/4.92G [00:18<00:04, 268MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  77%|███▊ | 3.79G/4.92G [00:19<00:04, 266MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  78%|███▉ | 3.82G/4.92G [00:19<00:04, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  78%|███▉ | 3.85G/4.92G [00:19<00:04, 263MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  79%|███▉ | 3.88G/4.92G [00:19<00:03, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  80%|███▉ | 3.91G/4.92G [00:19<00:03, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  80%|████ | 3.94G/4.92G [00:19<00:03, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  81%|████ | 3.97G/4.92G [00:19<00:03, 260MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  81%|████ | 4.01G/4.92G [00:19<00:03, 261MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  82%|████ | 4.04G/4.92G [00:20<00:03, 260MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  83%|████▏| 4.07G/4.92G [00:20<00:03, 261MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  83%|████▏| 4.10G/4.92G [00:20<00:03, 262MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  84%|████▏| 4.13G/4.92G [00:20<00:02, 269MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  85%|████▏| 4.16G/4.92G [00:20<00:02, 274MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  85%|████▎| 4.19G/4.92G [00:20<00:02, 241MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  86%|████▎| 4.23G/4.92G [00:20<00:02, 252MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  87%|████▎| 4.26G/4.92G [00:20<00:02, 261MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  87%|████▎| 4.29G/4.92G [00:20<00:02, 266MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  88%|████▍| 4.32G/4.92G [00:21<00:02, 267MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  89%|████▍| 4.35G/4.92G [00:21<00:02, 267MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  89%|████▍| 4.38G/4.92G [00:21<00:02, 265MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  90%|████▍| 4.41G/4.92G [00:21<00:01, 272MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  90%|████▌| 4.45G/4.92G [00:21<00:01, 282MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  91%|████▌| 4.48G/4.92G [00:21<00:01, 283MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  92%|████▌| 4.51G/4.92G [00:21<00:01, 290MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  92%|████▌| 4.54G/4.92G [00:21<00:01, 286MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  93%|████▋| 4.57G/4.92G [00:21<00:01, 280MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  94%|████▋| 4.60G/4.92G [00:22<00:01, 276MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  94%|████▋| 4.63G/4.92G [00:22<00:01, 279MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  95%|████▋| 4.67G/4.92G [00:22<00:00, 284MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  96%|████▊| 4.70G/4.92G [00:22<00:00, 289MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  96%|████▊| 4.73G/4.92G [00:22<00:00, 285MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  97%|████▊| 4.76G/4.92G [00:22<00:00, 286MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  97%|████▊| 4.79G/4.92G [00:22<00:00, 280MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  98%|████▉| 4.82G/4.92G [00:22<00:00, 280MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  99%|████▉| 4.85G/4.92G [00:22<00:00, 277MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  99%|████▉| 4.89G/4.92G [00:23<00:00, 277MB/s]\u001b[A\nmodel-00003-of-00004.safetensors: 100%|█████| 4.92G/4.92G [00:23<00:00, 212MB/s]\u001b[A\nDownloading shards:  75%|██████████████████▊      | 3/4 [01:04<00:22, 22.00s/it]\nmodel-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\nmodel-00004-of-00004.safetensors:   2%|     | 21.0M/1.17G [00:00<00:07, 145MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:   4%|▏    | 52.4M/1.17G [00:00<00:05, 201MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:   7%|▎    | 83.9M/1.17G [00:00<00:04, 236MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  11%|▋     | 126M/1.17G [00:00<00:03, 269MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  13%|▊     | 157M/1.17G [00:00<00:03, 277MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  16%|▉     | 189M/1.17G [00:00<00:03, 281MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  19%|█▏    | 220M/1.17G [00:00<00:03, 283MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  22%|█▎    | 252M/1.17G [00:00<00:03, 283MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  24%|█▍    | 283M/1.17G [00:01<00:03, 280MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  27%|█▌    | 315M/1.17G [00:01<00:03, 280MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  30%|█▊    | 346M/1.17G [00:01<00:02, 279MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  32%|█▉    | 377M/1.17G [00:01<00:02, 277MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  35%|██    | 409M/1.17G [00:01<00:02, 277MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  38%|██▎   | 440M/1.17G [00:01<00:02, 275MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  40%|██▍   | 472M/1.17G [00:01<00:02, 271MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  43%|██▌   | 503M/1.17G [00:01<00:02, 241MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  46%|██▋   | 535M/1.17G [00:02<00:03, 192MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  48%|██▉   | 566M/1.17G [00:02<00:03, 163MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  50%|███   | 587M/1.17G [00:02<00:04, 145MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  52%|███   | 608M/1.17G [00:02<00:03, 148MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  54%|███▏  | 629M/1.17G [00:02<00:03, 158MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  57%|███▍  | 661M/1.17G [00:02<00:02, 182MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  59%|███▌  | 692M/1.17G [00:03<00:02, 203MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  62%|███▋  | 724M/1.17G [00:03<00:02, 219MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  65%|███▉  | 755M/1.17G [00:03<00:01, 232MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  67%|████  | 786M/1.17G [00:03<00:02, 158MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  70%|████▏ | 818M/1.17G [00:03<00:01, 181MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  73%|████▎ | 849M/1.17G [00:03<00:01, 198MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  75%|████▌ | 881M/1.17G [00:04<00:01, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  78%|████▋ | 912M/1.17G [00:04<00:01, 229MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  81%|████▊ | 944M/1.17G [00:04<00:00, 238MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  83%|█████ | 975M/1.17G [00:04<00:00, 247MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  86%|████▎| 1.01G/1.17G [00:04<00:00, 254MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  89%|████▍| 1.04G/1.17G [00:04<00:00, 253MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  92%|████▌| 1.07G/1.17G [00:04<00:00, 254MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  94%|████▋| 1.10G/1.17G [00:04<00:00, 259MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  97%|████▊| 1.13G/1.17G [00:05<00:00, 258MB/s]\u001b[A\nmodel-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:05<00:00, 227MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 4/4 [01:10<00:00, 17.50s/it]\nDownloading shards: 100%|█████████████████████████| 4/4 [01:09<00:00, 17.48s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:20<00:00, 20.15s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:20<00:00, 20.16s/it]\ngeneration_config.json: 100%|███████████████████| 187/187 [00:00<00:00, 159kB/s]\nGenerating train split: 113 examples [00:00, 590.28 examples/s]\nGenerating train split: 13 examples [00:00, 698.51 examples/s]\ntrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n{'loss': 2.3247, 'grad_norm': 0.59765625, 'learning_rate': 0.0005, 'epoch': 0.35}\n{'loss': 1.8965, 'grad_norm': 0.67578125, 'learning_rate': 0.0005, 'epoch': 0.7}\n 33%|██████████████▎                            | 28/84 [11:50<23:50, 25.54s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:04<00:10,  2.05s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:08<00:11,  2.90s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:12<00:10,  3.34s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:16<00:07,  3.60s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:20<00:03,  3.76s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:24<00:00,  3.87s/it]\u001b[A\n{'eval_loss': 1.8174043893814087, 'eval_runtime': 28.7459, 'eval_samples_per_second': 0.452, 'eval_steps_per_second': 0.244, 'epoch': 0.98}\n\n 33%|██████████████▎                            | 28/84 [12:31<23:50, 25.54s/it]\u001b[A\n                                                                                \u001b[A[rank0]:[2024-05-09 20:56:51,520] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.5212345970001024, 'preprocessing_with_comm': 0.015572352999924988, 'state_converting': 0.27797494099991127, <Type.ALL: 'all'>: 0.8391994240000713})\n{'loss': 1.8752, 'grad_norm': 0.318359375, 'learning_rate': 0.0005, 'epoch': 1.05}\n{'loss': 1.6495, 'grad_norm': 0.451171875, 'learning_rate': 0.0005, 'epoch': 1.4}\n 52%|██████████████████████▌                    | 44/84 [19:32<17:03, 25.58s/it]","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n \npeft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"\n \n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  torch_dtype=torch.float16,\n  quantization_config= {\"load_in_4bit\": True},\n  device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randint\n \n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = 2\nmessages = eval_dataset[rand_idx][\"messages\"][:2]\n \n# Test on sample\ninput_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id= tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\n\nprint(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\nprint(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\nprint(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n\n# **Query:**\n# How long was the Revolutionary War?\n# **Original Answer:**\n# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783.\n# **Generated Answer:**\n# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"markdown","source":"##### Zip the lora file\nDownload manually from the output section in the sidebar","metadata":{}},{"cell_type":"code","source":"# !ls\n# !zip -0 -r llama-3-8b-FinGPT.zip /home/jupyter/llama-3-8b-FinGPT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Merge PEFT and base model","metadata":{}},{"cell_type":"code","source":"# !ls /home/jupyter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import AutoPeftModelForCausalLM\n# import torch\n \n# # Load PEFT model on CPU\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     \"/home/jupyter/llama-3-8b-FinGPT\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n\n# # Save locally\n# # merged_model.save_pretrained(\"/home/jupyter/llama-3-8b-FinGPT-Merged\",safe_serialization=True, max_shard_size=\"2GB\")\n# # !zip -0 -r llama-3-8b-FinGPT-Merged.zip /home/jupyter/llama-3-8b-FinGPT-Merged\n\n# # # Publish to Huggingface\n# merged_model.push_to_hub(\"my-awesome-model\", safe_serialization=True, max_shard_size=\"2GB\")\n# peft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from peft import AutoPeftModelForCausalLM\n# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n# tokenizer.push_to_hub(\"my-awesome-model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful sources\n- https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n- https://www.philschmid.de/fsdp-qlora-llama3\n\n- https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa\n- https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset","metadata":{}}]}