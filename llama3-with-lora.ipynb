{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train llama3 with LoRA\n[Source of this notebook](https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa)\n\nThis notebook is designed for Kaggle notebook with 2 Nvidia T4 GPUs","metadata":{}},{"cell_type":"markdown","source":"### Enviornment setup\n- Set your `HF_TOKEN` at `Add-ons -> Secrets`","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-04T20:25:01.816265Z","iopub.execute_input":"2024-05-04T20:25:01.816886Z","iopub.status.idle":"2024-05-04T20:25:02.038892Z","shell.execute_reply.started":"2024-05-04T20:25:01.816822Z","shell.execute_reply":"2024-05-04T20:25:02.037661Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install Pytorch for FSDP and FA/SDPA\n%pip install \"torch==2.2.2\" tensorboard\n \n# Install Hugging Face libraries\n%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T20:25:04.274530Z","iopub.execute_input":"2024-05-04T20:25:04.275134Z","iopub.status.idle":"2024-05-04T20:28:56.144262Z","shell.execute_reply.started":"2024-05-04T20:25:04.275085Z","shell.execute_reply":"2024-05-04T20:28:56.142361Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch==2.2.2\n  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.2)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.60.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2+cpu\n    Uninstalling torch-2.1.2+cpu:\n      Successfully uninstalled torch-2.1.2+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\ntorchvision 0.16.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: accelerate==0.29.3 in /opt/conda/lib/python3.10/site-packages (0.29.3)\nCollecting evaluate==0.4.1\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting bitsandbytes==0.43.1\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: huggingface_hub==0.22.2 in /opt/conda/lib/python3.10/site-packages (0.22.2)\nCollecting trl==0.8.6\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting peft==0.10.0\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (2.2.2)\nCollecting responses<0.19 (from evaluate==0.4.1)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2) (4.9.0)\nCollecting tyro>=0.5.11 (from trl==0.8.6)\n  Downloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.4.127)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, responses, tyro, tokenizers, transformers, bitsandbytes, trl, peft, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed bitsandbytes-0.43.1 evaluate-0.4.1 peft-0.10.0 responses-0.18.0 shtab-1.7.1 tokenizers-0.19.1 transformers-4.40.0 trl-0.8.6 tyro-0.8.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login --token $HF_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:53:52.006371Z","iopub.execute_input":"2024-05-04T20:53:52.006877Z","iopub.status.idle":"2024-05-04T20:53:53.680336Z","shell.execute_reply.started":"2024-05-04T20:53:52.006838Z","shell.execute_reply":"2024-05-04T20:53:53.678793Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load and prepare the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n \n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are a seasoned stock market analyst. Please follow the following INSTRUCTION. \nINSTRUCTION:\n{instruction}\"\"\"\n\ndef create_conversation(sample):\n#     return {\n#         \"messages\": [\n#             {\"role\": \"system\", \"content\": system_message.format(instruction=sample[\"instruction\"])},\n#             {\"role\": \"user\", \"content\": sample[\"input\"]},\n#             {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n#         ]\n#     }\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"instruction\"]},\n            {\"role\": \"user\", \"content\": sample[\"input\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n        ]\n    }\n \n# Load dataset from the hub\ndataset = load_dataset(\"FinGPT/fingpt-finred\", split=\"train\")\ndataset = dataset.select(range(0, 1250))\n \n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=250/1250, seed=42)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk\ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:47:39.714542Z","iopub.execute_input":"2024-05-04T20:47:39.715024Z","iopub.status.idle":"2024-05-04T20:47:41.107363Z","shell.execute_reply.started":"2024-05-04T20:47:39.714980Z","shell.execute_reply":"2024-05-04T20:47:41.105960Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[{'content': 'What is the relationship between debt and lifetime in the context of the input sentence. Choose an answer from: product/material produced; manufacturer; distributed by; industry; position held; original broadcaster; owned by; founded by; distribution format; headquarters location; stock exchange; currency; parent organization; chief executive officer; director/manager; owner of; operator; member of; employer; chairperson; platform; subsidiary; legal form; publisher; developer; brand; business division; location of formation; creator.', 'role': 'system'}, {'content': 'I remember at one point you were kind of targeting net debt of 10 billion but that feels like a lifetime ago and obviously a lot have changed since then.', 'role': 'user'}, {'content': 'original_broadcaster', 'role': 'assistant'}]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f218ac66d61849f5816bf6ed2e400a8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1fbf508aeb41bbb7d8fe3d52828767"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"236103"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load base model and setup training parameters","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.591194Z","iopub.execute_input":"2024-05-04T19:21:51.591717Z","iopub.status.idle":"2024-05-04T19:21:51.769964Z","shell.execute_reply.started":"2024-05-04T19:21:51.591691Z","shell.execute_reply":"2024-05-04T19:21:51.768976Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0823bbd819c54dda9e77d53b2463b82f"}},"metadata":{}}]},{"cell_type":"code","source":"%%writefile llama_3_8b_fsdp_qlora.yaml\n# script parameters\nmodel_id: \"meta-llama/Meta-Llama-3-8B-Instruct\" # Hugging Face model id\ndataset_path: \".\"                      # path to dataset\nmax_seq_len:  2048 # 2048              # max sequence length for model and packing of the dataset\n# training parameters\noutput_dir: \"/home/jupyter/llama-3-8b-FinGPT\" # Temporary output directory for model checkpoints\nreport_to: \"tensorboard\"               # report metrics to tensorboard\nlearning_rate: 0.0002                  # learning rate 2e-4\nlr_scheduler_type: \"constant\"          # learning rate scheduler\nnum_train_epochs: 1                    # number of training epochs\nper_device_train_batch_size: 1         # batch size per device during training\nper_device_eval_batch_size: 1          # batch size for evaluation\ngradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\noptim: adamw_torch                     # use torch adamw optimizer\nlogging_steps: 10                      # log every 10 steps\nsave_strategy: epoch                   # save checkpoint every epoch\nevaluation_strategy: epoch             # evaluate every epoch\nmax_grad_norm: 0.3                     # max gradient norm\nwarmup_ratio: 0.03                     # warmup ratio\nbf16: false                             # use bfloat16 precision\ntf32: false                             # use tf32 precision\ngradient_checkpointing: true           # use gradient checkpointing to save memory\nhub_private_repo: true\n# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\nfsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\nfsdp_config:\n  backward_prefetch: \"backward_pre\"\n  forward_prefetch: \"true\"\n  use_orig_params: \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.771329Z","iopub.execute_input":"2024-05-04T19:21:51.772284Z","iopub.status.idle":"2024-05-04T19:21:51.780646Z","shell.execute_reply.started":"2024-05-04T19:21:51.772246Z","shell.execute_reply":"2024-05-04T19:21:51.779589Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing llama_3_8b_fsdp_qlora.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_fsdp_qlora.py\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom trl.commands.cli_utils import  TrlParser\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n        set_seed,\n\n)\nfrom trl import setup_chat_format\nfrom peft import LoraConfig\n\n\nfrom trl import (\n   SFTTrainer)\n\n# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n\n# Anthropic/Vicuna like template without the need for special tokens\nLLAMA_3_CHAT_TEMPLATE = (\n    \"{% for message in messages %}\"\n        \"{% if message['role'] == 'system' %}\"\n            \"{{ message['content'] }}\"\n        \"{% elif message['role'] == 'user' %}\"\n            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n        \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n        \"{% endif %}\"\n    \"{% endfor %}\"\n    \"{% if add_generation_prompt %}\"\n    \"{{ '\\n\\nAssistant: ' }}\"\n    \"{% endif %}\"\n)\n\n\n# ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n\n@dataclass\nclass ScriptArguments:\n    dataset_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to the dataset\"\n        },\n    )\n    model_id: str = field(\n        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n    )\n    max_seq_length: int = field(\n        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n    )\n\n\ndef training_function(script_args, training_args):\n    ################\n    # Dataset\n    ################\n    \n    train_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n        split=\"train\",\n    )\n    test_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n        split=\"train\",\n    )\n\n    ################\n    # Model & Tokenizer\n    ################\n\n    # Tokenizer        \n    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n    \n    # template dataset\n    def template_dataset(examples):\n        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n    \n    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    \n    # print random sample\n    with training_args.main_process_first(\n        desc=\"Log a few random samples from the processed training set\"\n    ):\n        for index in random.sample(range(len(train_dataset)), 2):\n            print(train_dataset[index][\"text\"])\n\n    # Model    \n    torch_dtype = torch.bfloat16\n    quant_storage_dtype = torch.bfloat16\n\n    quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n            bnb_4bit_quant_storage=quant_storage_dtype,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.model_id,\n        quantization_config=quantization_config,\n        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n        torch_dtype=quant_storage_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n    )\n    \n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    ################\n    # PEFT\n    ################\n\n    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n    peft_config = LoraConfig(\n        lora_alpha=8,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n#         modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n    )\n\n    ################\n    # Training\n    ################\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        dataset_text_field=\"text\",\n        eval_dataset=test_dataset,\n        peft_config=peft_config,\n        max_seq_length=script_args.max_seq_length,\n        tokenizer=tokenizer,\n        hub_private_repo=True,\n        packing=True,\n        dataset_kwargs={\n            \"add_special_tokens\": False,  # We template with special tokens\n            \"append_concat_token\": False,  # No need to add additional separator token\n        },\n    )\n    if trainer.accelerator.is_main_process:\n        trainer.model.print_trainable_parameters()\n\n    ##########################\n    # Train model\n    ##########################\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    ##########################\n    # SAVE MODEL FOR SAGEMAKER\n    ##########################\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n    \nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, TrainingArguments))\n    script_args, training_args = parser.parse_args_and_config()    \n    \n    # set use reentrant to False\n    if training_args.gradient_checkpointing:\n        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n    # set seed\n    set_seed(training_args.seed)\n  \n    # launch training\n    training_function(script_args, training_args)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.782157Z","iopub.execute_input":"2024-05-04T19:21:51.782484Z","iopub.status.idle":"2024-05-04T19:21:51.799266Z","shell.execute_reply.started":"2024-05-04T19:21:51.782453Z","shell.execute_reply":"2024-05-04T19:21:51.798366Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing run_fsdp_qlora.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"markdown","source":"##### Release unreferenced memory in Python","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.800223Z","iopub.execute_input":"2024-05-04T19:21:51.800484Z","iopub.status.idle":"2024-05-04T19:21:51.890728Z","shell.execute_reply.started":"2024-05-04T19:21:51.800461Z","shell.execute_reply":"2024-05-04T19:21:51.889639Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"129"},"metadata":{}}]},{"cell_type":"markdown","source":"##### Start training with torchrun","metadata":{}},{"cell_type":"code","source":"!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=2 ./run_fsdp_qlora.py --config llama_3_8b_fsdp_qlora.yaml","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T19:21:51.891967Z","iopub.execute_input":"2024-05-04T19:21:51.892315Z","iopub.status.idle":"2024-05-04T20:08:03.015934Z","shell.execute_reply.started":"2024-05-04T19:21:51.892287Z","shell.execute_reply":"2024-05-04T20:08:03.014742Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[2024-05-04 19:21:54,778] torch.distributed.run: [WARNING] \n[2024-05-04 19:21:54,778] torch.distributed.run: [WARNING] *****************************************\n[2024-05-04 19:21:54,778] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-05-04 19:21:54,778] torch.distributed.run: [WARNING] *****************************************\n2024-05-04 19:22:04.631548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 19:22:04.631549: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 19:22:04.631613: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 19:22:04.631677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 19:22:04.795483: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-04 19:22:04.795497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 1000 examples [00:00, 60292.44 examples/s]\nGenerating train split: 250 examples [00:00, 49818.32 examples/s]\ntokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 4.46MB/s]\ntokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 38.1MB/s]\nspecial_tokens_map.json: 100%|████████████████| 73.0/73.0 [00:00<00:00, 538kB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 1944.81 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 2167.46 examples/s]\nMap: 100%|███████████████████████████| 250/250 [00:00<00:00, 6840.52 examples/s]\nWhat is the relationship between Apple Inc and Tim Cook in the context of the input sentence. Choose an answer from: product/material produced; manufacturer; distributed by; industry; position held; original broadcaster; owned by; founded by; distribution format; headquarters location; stock exchange; currency; parent organization; chief executive officer; director/manager; owner of; operator; member of; employer; chairperson; platform; subsidiary; legal form; publisher; developer; brand; business division; location of formation; creator.\n\nHuman: Apple Inc Chief Executive Tim Cook unveiled a new version of its Apple TV product with an app store and a voice-controlled remote control, part of a trio of announcements aimed at revamping its product line.<|end_of_text|>\n\nAssistant: chief_executive_officer<|end_of_text|>\nGiven phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \"relation1: word1, word2; relation2: word3, word4\". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.\n\nHuman: \"By providing direct access to Azure via ExpressRoute inside Equinix data centers, we are enabling companies to bridge their cloud and data center strategies and fully realize the benefits of hybrid and multi-cloud.<|end_of_text|>\n\nAssistant: product_or_material_produced: Equinix, data center<|end_of_text|>\nWhat is the relationship between Apple Inc and Tim Cook in the context of the input sentence. Choose an answer from: product/material produced; manufacturer; distributed by; industry; position held; original broadcaster; owned by; founded by; distribution format; headquarters location; stock exchange; currency; parent organization; chief executive officer; director/manager; owner of; operator; member of; employer; chairperson; platform; subsidiary; legal form; publisher; developer; brand; business division; location of formation; creator.\n\nHuman: Apple Inc Chief Executive Tim Cook unveiled a new version of its Apple TV product with an app store and a voice-controlled remote control, part of a trio of announcements aimed at revamping its product line.<|end_of_text|>\n\nAssistant: chief_executive_officer<|end_of_text|>\nGiven phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be \"relation1: word1, word2; relation2: word3, word4\". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.\n\nHuman: \"By providing direct access to Azure via ExpressRoute inside Equinix data centers, we are enabling companies to bridge their cloud and data center strategies and fully realize the benefits of hybrid and multi-cloud.<|end_of_text|>\n\nAssistant: product_or_material_produced: Equinix, data center<|end_of_text|>\nconfig.json: 100%|█████████████████████████████| 654/654 [00:00<00:00, 3.04MB/s]\nmodel.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 25.4MB/s]\nDownloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\nmodel-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00004.safetensors:   0%|    | 10.5M/4.98G [00:00<00:59, 83.6MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   1%|     | 31.5M/4.98G [00:00<00:37, 133MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   1%|     | 62.9M/4.98G [00:00<00:28, 174MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   2%|     | 94.4M/4.98G [00:00<00:25, 195MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   3%|▏     | 126M/4.98G [00:00<00:23, 206MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   3%|▏     | 157M/4.98G [00:00<00:22, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   4%|▏     | 189M/4.98G [00:00<00:22, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   4%|▎     | 220M/4.98G [00:01<00:21, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   5%|▎     | 252M/4.98G [00:01<00:21, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   6%|▎     | 283M/4.98G [00:01<00:21, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   6%|▍     | 315M/4.98G [00:01<00:21, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   7%|▍     | 346M/4.98G [00:01<00:21, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   8%|▍     | 377M/4.98G [00:01<00:20, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   8%|▍     | 409M/4.98G [00:01<00:21, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   9%|▌     | 440M/4.98G [00:02<00:20, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:   9%|▌     | 472M/4.98G [00:02<00:21, 205MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  10%|▌     | 503M/4.98G [00:02<00:21, 208MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  11%|▋     | 535M/4.98G [00:02<00:20, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  11%|▋     | 566M/4.98G [00:02<00:20, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  12%|▋     | 598M/4.98G [00:02<00:20, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  13%|▊     | 629M/4.98G [00:03<00:20, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  13%|▊     | 661M/4.98G [00:03<00:20, 211MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  14%|▊     | 692M/4.98G [00:03<00:20, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  15%|▊     | 724M/4.98G [00:03<00:20, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  15%|▉     | 755M/4.98G [00:03<00:19, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  16%|▉     | 786M/4.98G [00:03<00:19, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  16%|▉     | 818M/4.98G [00:03<00:19, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  17%|█     | 849M/4.98G [00:04<00:18, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  18%|█     | 881M/4.98G [00:04<00:18, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  18%|█     | 912M/4.98G [00:04<00:18, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  19%|█▏    | 944M/4.98G [00:04<00:18, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  20%|█▏    | 975M/4.98G [00:04<00:18, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  20%|█    | 1.01G/4.98G [00:04<00:18, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  21%|█    | 1.04G/4.98G [00:04<00:18, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  21%|█    | 1.07G/4.98G [00:05<00:18, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  22%|█    | 1.10G/4.98G [00:05<00:18, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  23%|█▏   | 1.13G/4.98G [00:05<00:17, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  23%|█▏   | 1.16G/4.98G [00:05<00:17, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  24%|█▏   | 1.20G/4.98G [00:05<00:17, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  25%|█▏   | 1.23G/4.98G [00:05<00:17, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  25%|█▎   | 1.26G/4.98G [00:05<00:17, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  26%|█▎   | 1.29G/4.98G [00:06<00:17, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  27%|█▎   | 1.32G/4.98G [00:06<00:16, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  27%|█▎   | 1.35G/4.98G [00:06<00:16, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  28%|█▍   | 1.38G/4.98G [00:06<00:16, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  28%|█▍   | 1.42G/4.98G [00:06<00:16, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  29%|█▍   | 1.45G/4.98G [00:06<00:16, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  30%|█▍   | 1.48G/4.98G [00:06<00:16, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  30%|█▌   | 1.51G/4.98G [00:07<00:16, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  31%|█▌   | 1.54G/4.98G [00:07<00:16, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  32%|█▌   | 1.57G/4.98G [00:07<00:15, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  32%|█▌   | 1.60G/4.98G [00:07<00:15, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  33%|█▋   | 1.64G/4.98G [00:07<00:15, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  34%|█▋   | 1.67G/4.98G [00:07<00:15, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  34%|█▋   | 1.70G/4.98G [00:07<00:15, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  35%|█▋   | 1.73G/4.98G [00:08<00:14, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  35%|█▊   | 1.76G/4.98G [00:08<00:14, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  36%|█▊   | 1.79G/4.98G [00:08<00:14, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  37%|█▊   | 1.82G/4.98G [00:08<00:14, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  37%|█▊   | 1.86G/4.98G [00:08<00:14, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  38%|█▉   | 1.89G/4.98G [00:08<00:14, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  39%|█▉   | 1.92G/4.98G [00:08<00:13, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  39%|█▉   | 1.95G/4.98G [00:09<00:13, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  40%|█▉   | 1.98G/4.98G [00:09<00:13, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  40%|██   | 2.01G/4.98G [00:09<00:13, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  41%|██   | 2.04G/4.98G [00:09<00:13, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  42%|██   | 2.08G/4.98G [00:09<00:13, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  42%|██   | 2.11G/4.98G [00:09<00:13, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  43%|██▏  | 2.14G/4.98G [00:09<00:12, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  44%|██▏  | 2.17G/4.98G [00:10<00:12, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  44%|██▏  | 2.20G/4.98G [00:10<00:12, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  45%|██▏  | 2.23G/4.98G [00:10<00:12, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  46%|██▎  | 2.26G/4.98G [00:10<00:12, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  46%|██▎  | 2.30G/4.98G [00:10<00:12, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  47%|██▎  | 2.33G/4.98G [00:10<00:12, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  47%|██▎  | 2.36G/4.98G [00:10<00:11, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  48%|██▍  | 2.39G/4.98G [00:11<00:11, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  49%|██▍  | 2.42G/4.98G [00:11<00:11, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  49%|██▍  | 2.45G/4.98G [00:11<00:11, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  50%|██▍  | 2.49G/4.98G [00:11<00:11, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  51%|██▌  | 2.52G/4.98G [00:11<00:11, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  51%|██▌  | 2.55G/4.98G [00:11<00:11, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  52%|██▌  | 2.58G/4.98G [00:11<00:10, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  52%|██▌  | 2.61G/4.98G [00:12<00:10, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  53%|██▋  | 2.64G/4.98G [00:12<00:10, 222MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  54%|██▋  | 2.67G/4.98G [00:12<00:10, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  54%|██▋  | 2.71G/4.98G [00:12<00:10, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  55%|██▋  | 2.74G/4.98G [00:12<00:10, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  56%|██▊  | 2.77G/4.98G [00:12<00:10, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  56%|██▊  | 2.80G/4.98G [00:13<00:10, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  57%|██▊  | 2.83G/4.98G [00:13<00:09, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  58%|██▉  | 2.86G/4.98G [00:13<00:09, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  58%|██▉  | 2.89G/4.98G [00:13<00:09, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  59%|██▉  | 2.93G/4.98G [00:13<00:09, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  59%|██▉  | 2.96G/4.98G [00:13<00:09, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  60%|███  | 2.99G/4.98G [00:13<00:09, 212MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  61%|███  | 3.02G/4.98G [00:14<00:09, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  61%|███  | 3.05G/4.98G [00:14<00:09, 211MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  62%|███  | 3.08G/4.98G [00:14<00:08, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  63%|███▏ | 3.11G/4.98G [00:14<00:08, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  63%|███▏ | 3.15G/4.98G [00:14<00:08, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  64%|███▏ | 3.18G/4.98G [00:14<00:08, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  64%|███▏ | 3.21G/4.98G [00:14<00:08, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  65%|███▎ | 3.24G/4.98G [00:15<00:07, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  66%|███▎ | 3.27G/4.98G [00:15<00:07, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  66%|███▎ | 3.30G/4.98G [00:15<00:07, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  67%|███▎ | 3.33G/4.98G [00:15<00:07, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  68%|███▍ | 3.37G/4.98G [00:15<00:07, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  68%|███▍ | 3.40G/4.98G [00:15<00:07, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  69%|███▍ | 3.43G/4.98G [00:15<00:07, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  70%|███▍ | 3.46G/4.98G [00:16<00:06, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  70%|███▌ | 3.49G/4.98G [00:16<00:06, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  71%|███▌ | 3.52G/4.98G [00:16<00:06, 221MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  71%|███▌ | 3.55G/4.98G [00:16<00:06, 222MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  72%|███▌ | 3.59G/4.98G [00:16<00:06, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  73%|███▋ | 3.62G/4.98G [00:16<00:06, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  73%|███▋ | 3.65G/4.98G [00:16<00:06, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  74%|███▋ | 3.68G/4.98G [00:17<00:05, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  75%|███▋ | 3.71G/4.98G [00:17<00:05, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  75%|███▊ | 3.74G/4.98G [00:17<00:05, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  76%|███▊ | 3.77G/4.98G [00:17<00:05, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  76%|███▊ | 3.81G/4.98G [00:17<00:05, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  77%|███▊ | 3.84G/4.98G [00:17<00:05, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  78%|███▉ | 3.87G/4.98G [00:17<00:05, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  78%|███▉ | 3.90G/4.98G [00:18<00:04, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  79%|███▉ | 3.93G/4.98G [00:18<00:04, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  80%|███▉ | 3.96G/4.98G [00:18<00:04, 220MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  80%|████ | 4.00G/4.98G [00:18<00:04, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  81%|████ | 4.03G/4.98G [00:18<00:04, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  82%|████ | 4.06G/4.98G [00:18<00:04, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  82%|████ | 4.09G/4.98G [00:18<00:04, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  83%|████▏| 4.12G/4.98G [00:19<00:03, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  83%|████▏| 4.15G/4.98G [00:19<00:03, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  84%|████▏| 4.18G/4.98G [00:19<00:03, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  85%|████▏| 4.22G/4.98G [00:19<00:03, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  85%|████▎| 4.25G/4.98G [00:19<00:03, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  86%|████▎| 4.28G/4.98G [00:19<00:03, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  87%|████▎| 4.31G/4.98G [00:19<00:03, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  87%|████▎| 4.34G/4.98G [00:20<00:02, 217MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  88%|████▍| 4.37G/4.98G [00:20<00:02, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  88%|████▍| 4.40G/4.98G [00:20<00:02, 218MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  89%|████▍| 4.44G/4.98G [00:20<00:02, 219MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  90%|████▍| 4.47G/4.98G [00:20<00:02, 211MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  90%|████▌| 4.50G/4.98G [00:20<00:02, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  91%|████▌| 4.53G/4.98G [00:20<00:02, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  92%|████▌| 4.56G/4.98G [00:21<00:01, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  92%|████▌| 4.59G/4.98G [00:21<00:01, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  93%|████▋| 4.62G/4.98G [00:21<00:01, 213MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  94%|████▋| 4.66G/4.98G [00:21<00:01, 216MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  94%|████▋| 4.69G/4.98G [00:21<00:01, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  95%|████▋| 4.72G/4.98G [00:21<00:01, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  95%|████▊| 4.75G/4.98G [00:22<00:01, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  96%|████▊| 4.78G/4.98G [00:22<00:00, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  97%|████▊| 4.81G/4.98G [00:22<00:00, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  97%|████▊| 4.84G/4.98G [00:22<00:00, 214MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  98%|████▉| 4.88G/4.98G [00:22<00:00, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  99%|████▉| 4.91G/4.98G [00:22<00:00, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors:  99%|████▉| 4.94G/4.98G [00:22<00:00, 215MB/s]\u001b[A\nmodel-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:23<00:00, 216MB/s]\u001b[A\nDownloading shards:  25%|██████▎                  | 1/4 [00:23<01:09, 23.23s/it]\nmodel-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00004.safetensors:   1%|     | 31.5M/5.00G [00:00<00:21, 233MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   1%|     | 62.9M/5.00G [00:00<00:22, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   2%|     | 94.4M/5.00G [00:00<00:22, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   3%|▏     | 126M/5.00G [00:00<00:22, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   3%|▏     | 157M/5.00G [00:00<00:22, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   4%|▏     | 189M/5.00G [00:00<00:22, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   4%|▎     | 220M/5.00G [00:01<00:22, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   5%|▎     | 252M/5.00G [00:01<00:22, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   6%|▎     | 283M/5.00G [00:01<00:21, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   6%|▍     | 315M/5.00G [00:01<00:21, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   7%|▍     | 346M/5.00G [00:01<00:21, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   8%|▍     | 377M/5.00G [00:01<00:21, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   8%|▍     | 409M/5.00G [00:01<00:21, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   9%|▌     | 440M/5.00G [00:02<00:20, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:   9%|▌     | 472M/5.00G [00:02<00:20, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  10%|▌     | 503M/5.00G [00:02<00:20, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  11%|▋     | 535M/5.00G [00:02<00:20, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  11%|▋     | 566M/5.00G [00:02<00:20, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  12%|▋     | 598M/5.00G [00:02<00:20, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  13%|▊     | 629M/5.00G [00:02<00:19, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  13%|▊     | 661M/5.00G [00:03<00:19, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  14%|▋    | 692M/5.00G [00:04<01:19, 54.4MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  14%|▋    | 713M/5.00G [00:04<01:06, 64.1MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  15%|▋    | 734M/5.00G [00:04<00:55, 76.8MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  15%|▊    | 765M/5.00G [00:04<00:42, 98.5MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  16%|▉     | 797M/5.00G [00:05<00:35, 120MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  16%|▉     | 818M/5.00G [00:05<00:31, 133MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  17%|█     | 849M/5.00G [00:05<00:26, 154MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  18%|█     | 881M/5.00G [00:05<00:24, 171MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  18%|█     | 912M/5.00G [00:05<00:22, 184MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  19%|█▏    | 944M/5.00G [00:05<00:20, 195MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  20%|█▏    | 975M/5.00G [00:05<00:19, 201MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  20%|█    | 1.01G/5.00G [00:06<00:19, 205MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  21%|█    | 1.04G/5.00G [00:06<00:18, 210MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  21%|█    | 1.07G/5.00G [00:06<00:18, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  22%|█    | 1.10G/5.00G [00:06<00:18, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  23%|█▏   | 1.13G/5.00G [00:06<00:17, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  23%|█▏   | 1.16G/5.00G [00:06<00:17, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  24%|█▏   | 1.20G/5.00G [00:06<00:17, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  25%|█▏   | 1.23G/5.00G [00:07<00:17, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  25%|█▎   | 1.26G/5.00G [00:07<00:17, 213MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  26%|█   | 1.29G/5.00G [00:08<01:07, 54.7MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  26%|█   | 1.31G/5.00G [00:08<00:57, 64.4MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  27%|█   | 1.34G/5.00G [00:09<00:43, 83.2MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  27%|█▎   | 1.37G/5.00G [00:09<00:35, 103MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  28%|█▍   | 1.39G/5.00G [00:09<00:30, 117MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  29%|█▍   | 1.43G/5.00G [00:09<00:25, 138MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  29%|█▍   | 1.46G/5.00G [00:09<00:22, 156MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  30%|█▍   | 1.48G/5.00G [00:09<00:21, 165MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  30%|█▌   | 1.51G/5.00G [00:09<00:19, 179MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  31%|█▌   | 1.54G/5.00G [00:10<00:18, 188MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  31%|█▌   | 1.57G/5.00G [00:10<00:17, 196MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  32%|█▌   | 1.60G/5.00G [00:10<00:16, 201MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  33%|█▋   | 1.64G/5.00G [00:10<00:16, 207MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  33%|█▋   | 1.67G/5.00G [00:10<00:15, 211MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  34%|█▋   | 1.70G/5.00G [00:10<00:15, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  35%|█▋   | 1.73G/5.00G [00:10<00:15, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  35%|█▊   | 1.76G/5.00G [00:11<00:14, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  36%|█▊   | 1.79G/5.00G [00:11<00:14, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  36%|█▊   | 1.82G/5.00G [00:11<00:14, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  37%|█▊   | 1.86G/5.00G [00:11<00:14, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  38%|█▉   | 1.89G/5.00G [00:11<00:14, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  38%|█▉   | 1.92G/5.00G [00:11<00:13, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  39%|█▉   | 1.95G/5.00G [00:11<00:13, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  40%|█▉   | 1.98G/5.00G [00:12<00:13, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  40%|██   | 2.01G/5.00G [00:12<00:13, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  41%|██   | 2.04G/5.00G [00:12<00:13, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  42%|██   | 2.08G/5.00G [00:12<00:13, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  42%|██   | 2.11G/5.00G [00:12<00:13, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  43%|██▏  | 2.14G/5.00G [00:12<00:13, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  43%|██▏  | 2.17G/5.00G [00:12<00:13, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  44%|██▏  | 2.20G/5.00G [00:13<00:13, 211MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  45%|██▏  | 2.23G/5.00G [00:13<00:13, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  45%|██▎  | 2.26G/5.00G [00:13<00:13, 209MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  46%|██▎  | 2.29G/5.00G [00:13<00:13, 206MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  46%|██▎  | 2.31G/5.00G [00:13<00:13, 204MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  47%|██▎  | 2.34G/5.00G [00:13<00:12, 207MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  47%|██▎  | 2.36G/5.00G [00:13<00:12, 207MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  48%|██▍  | 2.38G/5.00G [00:13<00:12, 207MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  48%|██▍  | 2.40G/5.00G [00:14<00:12, 205MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  48%|██▍  | 2.42G/5.00G [00:14<00:12, 206MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  49%|██▍  | 2.44G/5.00G [00:14<00:12, 205MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  49%|██▍  | 2.47G/5.00G [00:14<00:12, 208MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  50%|██▌  | 2.51G/5.00G [00:14<00:11, 209MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  51%|██▌  | 2.54G/5.00G [00:14<00:11, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  51%|██▌  | 2.57G/5.00G [00:14<00:11, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  52%|██▌  | 2.60G/5.00G [00:14<00:11, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  53%|██▋  | 2.63G/5.00G [00:15<00:11, 208MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  53%|██▋  | 2.65G/5.00G [00:15<00:11, 208MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  54%|██▋  | 2.68G/5.00G [00:15<00:10, 211MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  54%|██▋  | 2.72G/5.00G [00:15<00:10, 210MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  55%|██▋  | 2.75G/5.00G [00:15<00:10, 213MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  56%|██▊  | 2.78G/5.00G [00:15<00:10, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  56%|██▊  | 2.81G/5.00G [00:15<00:10, 216MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  57%|██▊  | 2.84G/5.00G [00:16<00:10, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  57%|██▊  | 2.87G/5.00G [00:16<00:10, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  58%|██▉  | 2.90G/5.00G [00:16<00:09, 213MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  59%|██▉  | 2.94G/5.00G [00:16<00:09, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  59%|██▉  | 2.97G/5.00G [00:16<00:09, 213MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  60%|██▉  | 3.00G/5.00G [00:16<00:09, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  61%|███  | 3.03G/5.00G [00:16<00:09, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  61%|███  | 3.06G/5.00G [00:17<00:08, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  62%|███  | 3.09G/5.00G [00:17<00:08, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  62%|███  | 3.12G/5.00G [00:17<00:08, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  63%|███▏ | 3.16G/5.00G [00:17<00:08, 222MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  64%|███▏ | 3.19G/5.00G [00:17<00:08, 223MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  64%|███▏ | 3.22G/5.00G [00:17<00:08, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  65%|███▎ | 3.25G/5.00G [00:17<00:07, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  66%|███▎ | 3.28G/5.00G [00:18<00:07, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  66%|███▎ | 3.31G/5.00G [00:18<00:07, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  67%|███▎ | 3.34G/5.00G [00:18<00:07, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  68%|███▍ | 3.38G/5.00G [00:18<00:07, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  68%|███▍ | 3.41G/5.00G [00:18<00:07, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  69%|███▍ | 3.44G/5.00G [00:18<00:07, 212MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  69%|███▍ | 3.47G/5.00G [00:19<00:07, 214MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  70%|███▌ | 3.50G/5.00G [00:19<00:06, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  71%|███▌ | 3.53G/5.00G [00:19<00:06, 215MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  71%|███▌ | 3.57G/5.00G [00:19<00:06, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  72%|███▌ | 3.60G/5.00G [00:19<00:06, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  73%|███▋ | 3.63G/5.00G [00:19<00:06, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  73%|███▋ | 3.66G/5.00G [00:19<00:06, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  74%|███▋ | 3.69G/5.00G [00:20<00:05, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  74%|███▋ | 3.72G/5.00G [00:20<00:05, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  75%|███▊ | 3.75G/5.00G [00:20<00:05, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  76%|███▊ | 3.79G/5.00G [00:20<00:05, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  76%|███▊ | 3.82G/5.00G [00:20<00:05, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  77%|███▊ | 3.85G/5.00G [00:20<00:05, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  78%|███▉ | 3.88G/5.00G [00:20<00:05, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  78%|███▉ | 3.91G/5.00G [00:21<00:04, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  79%|███▉ | 3.94G/5.00G [00:21<00:04, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  79%|███▉ | 3.97G/5.00G [00:21<00:04, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  80%|████ | 4.01G/5.00G [00:21<00:04, 221MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  81%|████ | 4.04G/5.00G [00:21<00:04, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  81%|████ | 4.07G/5.00G [00:21<00:04, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  82%|████ | 4.10G/5.00G [00:21<00:04, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  83%|████▏| 4.13G/5.00G [00:22<00:03, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  83%|████▏| 4.16G/5.00G [00:22<00:03, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  84%|████▏| 4.19G/5.00G [00:22<00:03, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  85%|████▏| 4.23G/5.00G [00:22<00:03, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  85%|████▎| 4.26G/5.00G [00:22<00:03, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  86%|████▎| 4.29G/5.00G [00:22<00:03, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  86%|████▎| 4.32G/5.00G [00:22<00:03, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  87%|████▎| 4.35G/5.00G [00:23<00:02, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  88%|████▍| 4.38G/5.00G [00:23<00:02, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  88%|████▍| 4.41G/5.00G [00:23<00:02, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  89%|████▍| 4.45G/5.00G [00:23<00:02, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  90%|████▍| 4.48G/5.00G [00:23<00:02, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  90%|████▌| 4.51G/5.00G [00:23<00:02, 220MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  91%|████▌| 4.54G/5.00G [00:23<00:02, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  91%|████▌| 4.57G/5.00G [00:24<00:01, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  92%|████▌| 4.60G/5.00G [00:24<00:01, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  93%|████▋| 4.63G/5.00G [00:24<00:01, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  93%|████▋| 4.67G/5.00G [00:24<00:01, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  94%|████▋| 4.70G/5.00G [00:24<00:01, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  95%|████▋| 4.73G/5.00G [00:24<00:01, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  95%|████▊| 4.76G/5.00G [00:24<00:01, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  96%|████▊| 4.79G/5.00G [00:25<00:00, 217MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  96%|████▊| 4.82G/5.00G [00:25<00:00, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  97%|████▊| 4.85G/5.00G [00:25<00:00, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  98%|████▉| 4.89G/5.00G [00:25<00:00, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  98%|████▉| 4.92G/5.00G [00:25<00:00, 219MB/s]\u001b[A\nmodel-00002-of-00004.safetensors:  99%|████▉| 4.95G/5.00G [00:25<00:00, 218MB/s]\u001b[A\nmodel-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:26<00:00, 192MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 2/4 [00:49<00:49, 24.93s/it]\nmodel-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\nmodel-00003-of-00004.safetensors:   1%|     | 31.5M/4.92G [00:00<00:21, 229MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   1%|     | 62.9M/4.92G [00:00<00:22, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   2%|     | 94.4M/4.92G [00:00<00:22, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   3%|▏     | 126M/4.92G [00:00<00:22, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   3%|▏     | 157M/4.92G [00:00<00:22, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   4%|▏     | 189M/4.92G [00:00<00:22, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   4%|▎     | 220M/4.92G [00:01<00:22, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   5%|▎     | 252M/4.92G [00:01<00:21, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   6%|▎     | 283M/4.92G [00:01<00:21, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   6%|▍     | 315M/4.92G [00:01<00:21, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   7%|▍     | 346M/4.92G [00:01<00:21, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   8%|▍     | 377M/4.92G [00:01<00:21, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   8%|▍     | 409M/4.92G [00:01<00:21, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:   9%|▌     | 440M/4.92G [00:02<00:20, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  10%|▌     | 472M/4.92G [00:02<00:20, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  10%|▌     | 503M/4.92G [00:02<00:20, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  11%|▋     | 535M/4.92G [00:02<00:20, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  12%|▋     | 566M/4.92G [00:02<00:20, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  12%|▋     | 598M/4.92G [00:02<00:20, 212MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  13%|▊     | 629M/4.92G [00:02<00:20, 212MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  13%|▊     | 661M/4.92G [00:03<00:20, 212MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  14%|▊     | 692M/4.92G [00:03<00:20, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  15%|▉     | 724M/4.92G [00:03<00:19, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  15%|▉     | 755M/4.92G [00:03<00:19, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  16%|▉     | 786M/4.92G [00:03<00:19, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  17%|▉     | 818M/4.92G [00:03<00:19, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  17%|▊    | 849M/4.92G [00:05<01:12, 56.0MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  18%|▉    | 870M/4.92G [00:05<01:03, 63.4MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  18%|▉    | 891M/4.92G [00:05<00:53, 75.9MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  19%|▉    | 923M/4.92G [00:05<00:40, 97.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  19%|█▏    | 954M/4.92G [00:05<00:33, 119MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  20%|█▏    | 986M/4.92G [00:06<00:28, 140MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  21%|█    | 1.02G/4.92G [00:06<00:24, 157MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  21%|█    | 1.05G/4.92G [00:06<00:22, 172MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  22%|█    | 1.08G/4.92G [00:06<00:20, 183MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  23%|█▏   | 1.11G/4.92G [00:06<00:19, 193MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  23%|█▏   | 1.14G/4.92G [00:06<00:18, 202MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  24%|█▏   | 1.17G/4.92G [00:06<00:18, 205MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  25%|█▏   | 1.21G/4.92G [00:07<00:17, 208MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  25%|█▎   | 1.24G/4.92G [00:07<00:17, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  26%|█▎   | 1.27G/4.92G [00:07<00:17, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  26%|█▎   | 1.30G/4.92G [00:07<00:16, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  27%|█▎   | 1.33G/4.92G [00:07<00:16, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  28%|█   | 1.36G/4.92G [00:09<01:06, 53.4MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  28%|█▏  | 1.38G/4.92G [00:09<00:55, 63.8MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  29%|█▏  | 1.41G/4.92G [00:10<01:41, 34.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  29%|█▏  | 1.43G/4.92G [00:11<01:22, 42.2MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  29%|█▏  | 1.45G/4.92G [00:11<01:04, 53.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  30%|█▏  | 1.48G/4.92G [00:11<00:46, 73.9MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  31%|█▏  | 1.51G/4.92G [00:11<00:35, 95.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  31%|█▌   | 1.54G/4.92G [00:11<00:28, 117MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  32%|█▌   | 1.57G/4.92G [00:11<00:24, 138MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  33%|█▋   | 1.60G/4.92G [00:11<00:21, 154MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  33%|█▋   | 1.64G/4.92G [00:12<00:19, 169MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  34%|█▋   | 1.67G/4.92G [00:12<00:17, 182MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  35%|█▋   | 1.70G/4.92G [00:12<00:16, 191MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  35%|█▊   | 1.73G/4.92G [00:12<00:16, 199MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  36%|█▊   | 1.76G/4.92G [00:12<00:15, 203MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  36%|█▊   | 1.79G/4.92G [00:12<00:15, 207MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  37%|█▊   | 1.82G/4.92G [00:12<00:14, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  38%|█▉   | 1.86G/4.92G [00:13<00:14, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  38%|█▉   | 1.89G/4.92G [00:13<00:14, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  39%|█▉   | 1.92G/4.92G [00:13<00:13, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  40%|█▉   | 1.95G/4.92G [00:13<00:13, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  40%|██   | 1.98G/4.92G [00:13<00:13, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  41%|██   | 2.01G/4.92G [00:13<00:13, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  42%|██   | 2.04G/4.92G [00:13<00:13, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  42%|██   | 2.08G/4.92G [00:14<00:13, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  43%|██▏  | 2.11G/4.92G [00:14<00:12, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  44%|██▏  | 2.14G/4.92G [00:14<00:12, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  44%|██▏  | 2.17G/4.92G [00:14<00:12, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  45%|██▏  | 2.20G/4.92G [00:14<00:12, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  45%|██▎  | 2.23G/4.92G [00:14<00:12, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  46%|██▎  | 2.26G/4.92G [00:14<00:11, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  47%|██▎  | 2.30G/4.92G [00:15<00:11, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  47%|██▎  | 2.33G/4.92G [00:15<00:11, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  48%|██▍  | 2.36G/4.92G [00:15<00:11, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  49%|██▍  | 2.39G/4.92G [00:15<00:11, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  49%|██▍  | 2.42G/4.92G [00:15<00:11, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  50%|██▍  | 2.45G/4.92G [00:15<00:11, 223MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  51%|██▌  | 2.49G/4.92G [00:15<00:10, 223MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  51%|██▌  | 2.52G/4.92G [00:16<00:10, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  52%|██▌  | 2.55G/4.92G [00:16<00:10, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  52%|██▌  | 2.58G/4.92G [00:16<00:10, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  53%|██▋  | 2.61G/4.92G [00:16<00:10, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  54%|██▋  | 2.64G/4.92G [00:16<00:10, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  54%|██▋  | 2.67G/4.92G [00:16<00:10, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  55%|██▊  | 2.71G/4.92G [00:16<00:10, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  56%|██▊  | 2.74G/4.92G [00:17<00:10, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  56%|██▊  | 2.77G/4.92G [00:17<00:10, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  57%|██▊  | 2.80G/4.92G [00:17<00:09, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  58%|██▉  | 2.83G/4.92G [00:17<00:09, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  58%|██▉  | 2.86G/4.92G [00:17<00:09, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  59%|██▉  | 2.89G/4.92G [00:17<00:09, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  60%|██▉  | 2.93G/4.92G [00:17<00:09, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  60%|███  | 2.96G/4.92G [00:18<00:09, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  61%|███  | 2.99G/4.92G [00:18<00:08, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  61%|███  | 3.02G/4.92G [00:18<00:08, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  62%|███  | 3.05G/4.92G [00:18<00:08, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  63%|███▏ | 3.08G/4.92G [00:18<00:08, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  63%|███▏ | 3.11G/4.92G [00:18<00:08, 210MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  64%|███▏ | 3.15G/4.92G [00:18<00:08, 210MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  65%|███▏ | 3.18G/4.92G [00:19<00:08, 210MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  65%|███▎ | 3.21G/4.92G [00:19<00:07, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  66%|███▎ | 3.24G/4.92G [00:19<00:07, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  67%|███▎ | 3.27G/4.92G [00:19<00:07, 212MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  67%|██▋ | 3.30G/4.92G [00:21<00:31, 50.5MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  68%|██▋ | 3.32G/4.92G [00:21<00:26, 59.8MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  68%|██▋ | 3.36G/4.92G [00:21<00:19, 78.1MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  69%|██▊ | 3.39G/4.92G [00:21<00:15, 98.2MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  70%|███▍ | 3.42G/4.92G [00:21<00:12, 119MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  70%|███▌ | 3.45G/4.92G [00:21<00:10, 139MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  71%|███▌ | 3.47G/4.92G [00:22<00:09, 150MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  71%|███▌ | 3.49G/4.92G [00:22<00:08, 161MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  72%|███▌ | 3.52G/4.92G [00:22<00:07, 176MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  72%|███▌ | 3.55G/4.92G [00:22<00:07, 189MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  73%|███▋ | 3.59G/4.92G [00:22<00:06, 198MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  74%|███▋ | 3.62G/4.92G [00:22<00:06, 205MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  74%|███▋ | 3.65G/4.92G [00:22<00:06, 211MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  75%|███▋ | 3.68G/4.92G [00:23<00:05, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  76%|███▊ | 3.71G/4.92G [00:23<00:05, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  76%|███▊ | 3.74G/4.92G [00:23<00:05, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  77%|███▊ | 3.77G/4.92G [00:23<00:05, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  77%|███▊ | 3.81G/4.92G [00:23<00:05, 210MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  78%|███▉ | 3.84G/4.92G [00:23<00:07, 153MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  79%|███▉ | 3.87G/4.92G [00:24<00:06, 169MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  79%|███▉ | 3.90G/4.92G [00:24<00:05, 182MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  80%|███▉ | 3.93G/4.92G [00:24<00:05, 191MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  80%|████ | 3.95G/4.92G [00:24<00:05, 192MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  81%|████ | 3.98G/4.92G [00:24<00:04, 201MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  82%|████ | 4.02G/4.92G [00:24<00:04, 207MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  82%|████ | 4.05G/4.92G [00:24<00:04, 213MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  83%|████▏| 4.08G/4.92G [00:25<00:03, 214MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  84%|████▏| 4.11G/4.92G [00:25<00:03, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  84%|████▏| 4.14G/4.92G [00:25<00:03, 215MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  85%|████▏| 4.17G/4.92G [00:25<00:03, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  86%|████▎| 4.20G/4.92G [00:25<00:03, 218MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  86%|████▎| 4.24G/4.92G [00:25<00:03, 216MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  87%|████▎| 4.27G/4.92G [00:25<00:02, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  87%|████▎| 4.30G/4.92G [00:26<00:02, 217MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  88%|████▍| 4.33G/4.92G [00:26<00:02, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  89%|████▍| 4.36G/4.92G [00:26<00:02, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  89%|████▍| 4.39G/4.92G [00:26<00:02, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  90%|████▌| 4.42G/4.92G [00:26<00:02, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  91%|████▌| 4.46G/4.92G [00:26<00:02, 219MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  91%|████▌| 4.49G/4.92G [00:26<00:01, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  92%|████▌| 4.52G/4.92G [00:27<00:01, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  93%|████▋| 4.55G/4.92G [00:27<00:01, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  93%|████▋| 4.58G/4.92G [00:27<00:01, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  94%|████▋| 4.61G/4.92G [00:27<00:01, 223MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  94%|████▋| 4.65G/4.92G [00:27<00:01, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  95%|████▊| 4.68G/4.92G [00:27<00:01, 222MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  96%|████▊| 4.71G/4.92G [00:27<00:00, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  96%|████▊| 4.74G/4.92G [00:28<00:00, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  97%|████▊| 4.77G/4.92G [00:28<00:00, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  98%|████▉| 4.80G/4.92G [00:28<00:00, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  98%|████▉| 4.83G/4.92G [00:28<00:00, 221MB/s]\u001b[A\nmodel-00003-of-00004.safetensors:  99%|████▉| 4.87G/4.92G [00:28<00:00, 220MB/s]\u001b[A\nmodel-00003-of-00004.safetensors: 100%|█████| 4.92G/4.92G [00:28<00:00, 170MB/s]\u001b[A\nDownloading shards:  75%|██████████████████▊      | 3/4 [01:18<00:26, 26.77s/it]\nmodel-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\nmodel-00004-of-00004.safetensors:   3%|▏    | 31.5M/1.17G [00:00<00:04, 233MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:   5%|▎    | 62.9M/1.17G [00:00<00:04, 223MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:   8%|▍    | 94.4M/1.17G [00:00<00:05, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  11%|▋     | 126M/1.17G [00:00<00:04, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  13%|▊     | 157M/1.17G [00:00<00:04, 217MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  16%|▉     | 189M/1.17G [00:00<00:04, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  19%|█▏    | 220M/1.17G [00:01<00:04, 217MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  22%|█▎    | 252M/1.17G [00:01<00:04, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  24%|█▍    | 283M/1.17G [00:01<00:04, 217MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  27%|█▌    | 315M/1.17G [00:01<00:03, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  30%|█▊    | 346M/1.17G [00:01<00:03, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  32%|█▉    | 377M/1.17G [00:01<00:03, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  35%|██    | 409M/1.17G [00:01<00:03, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  38%|██▎   | 440M/1.17G [00:02<00:03, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  40%|██▍   | 472M/1.17G [00:02<00:03, 217MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  43%|██▌   | 503M/1.17G [00:02<00:03, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  46%|██▋   | 535M/1.17G [00:02<00:02, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  48%|██▉   | 566M/1.17G [00:02<00:02, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  51%|███   | 598M/1.17G [00:02<00:02, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  54%|███▏  | 629M/1.17G [00:02<00:02, 213MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  57%|███▍  | 661M/1.17G [00:03<00:02, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  59%|███▌  | 692M/1.17G [00:03<00:02, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  62%|███▋  | 724M/1.17G [00:03<00:02, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  65%|███▉  | 755M/1.17G [00:03<00:01, 212MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  67%|████  | 786M/1.17G [00:03<00:01, 209MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  70%|████▏ | 818M/1.17G [00:03<00:01, 210MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  73%|████▎ | 849M/1.17G [00:03<00:01, 210MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  75%|████▌ | 881M/1.17G [00:04<00:01, 212MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  78%|████▋ | 912M/1.17G [00:04<00:01, 212MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  81%|████▊ | 944M/1.17G [00:04<00:01, 212MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  83%|█████ | 975M/1.17G [00:04<00:00, 212MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  86%|████▎| 1.01G/1.17G [00:04<00:00, 214MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  89%|████▍| 1.04G/1.17G [00:04<00:00, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  92%|████▌| 1.07G/1.17G [00:04<00:00, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  94%|████▋| 1.10G/1.17G [00:05<00:00, 216MB/s]\u001b[A\nmodel-00004-of-00004.safetensors:  97%|████▊| 1.13G/1.17G [00:05<00:00, 215MB/s]\u001b[A\nmodel-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:05<00:00, 214MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 4/4 [01:23<00:00, 20.96s/it]\nDownloading shards: 100%|█████████████████████████| 4/4 [01:23<00:00, 20.96s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:27<00:00, 21.95s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:27<00:00, 21.98s/it]\ngeneration_config.json: 100%|███████████████████| 187/187 [00:00<00:00, 983kB/s]\nGenerating train split: 374 examples [00:00, 704.04 examples/s]\nGenerating train split: 88 examples [00:00, 864.00 examples/s]\ntrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n{'loss': 2.0405, 'grad_norm': 0.52734375, 'learning_rate': 0.0002, 'epoch': 0.11}\n{'loss': 1.2429, 'grad_norm': 0.447265625, 'learning_rate': 0.0002, 'epoch': 0.21}\n{'loss': 0.9364, 'grad_norm': 0.341796875, 'learning_rate': 0.0002, 'epoch': 0.32}\n{'loss': 0.8379, 'grad_norm': 0.435546875, 'learning_rate': 0.0002, 'epoch': 0.43}\n{'loss': 0.9, 'grad_norm': 0.369140625, 'learning_rate': 0.0002, 'epoch': 0.53} \n{'loss': 0.8563, 'grad_norm': 0.375, 'learning_rate': 0.0002, 'epoch': 0.64}    \n{'loss': 0.8616, 'grad_norm': 0.2734375, 'learning_rate': 0.0002, 'epoch': 0.75}\n{'loss': 0.8209, 'grad_norm': 0.279296875, 'learning_rate': 0.0002, 'epoch': 0.86}\n{'loss': 0.8125, 'grad_norm': 0.30859375, 'learning_rate': 0.0002, 'epoch': 0.96}\n100%|███████████████████████████████████████████| 93/93 [39:15<00:00, 25.36s/it]\n  0%|                                                    | 0/44 [00:00<?, ?it/s]\u001b[A\n  5%|██                                          | 2/44 [00:03<01:20,  1.92s/it]\u001b[A\n  7%|███                                         | 3/44 [00:07<01:51,  2.72s/it]\u001b[A\n  9%|████                                        | 4/44 [00:11<02:05,  3.13s/it]\u001b[A\n 11%|█████                                       | 5/44 [00:15<02:11,  3.38s/it]\u001b[A\n 14%|██████                                      | 6/44 [00:19<02:14,  3.53s/it]\u001b[A\n 16%|███████                                     | 7/44 [00:23<02:14,  3.63s/it]\u001b[A\n 18%|████████                                    | 8/44 [00:26<02:12,  3.69s/it]\u001b[A\n 20%|█████████                                   | 9/44 [00:30<02:10,  3.74s/it]\u001b[A\n 23%|█████████▊                                 | 10/44 [00:34<02:08,  3.77s/it]\u001b[A\n 25%|██████████▊                                | 11/44 [00:38<02:05,  3.79s/it]\u001b[A\n 27%|███████████▋                               | 12/44 [00:42<02:01,  3.80s/it]\u001b[A\n 30%|████████████▋                              | 13/44 [00:46<01:58,  3.81s/it]\u001b[A\n 32%|█████████████▋                             | 14/44 [00:49<01:54,  3.81s/it]\u001b[A\n 34%|██████████████▋                            | 15/44 [00:53<01:50,  3.81s/it]\u001b[A\n 36%|███████████████▋                           | 16/44 [00:57<01:46,  3.81s/it]\u001b[A\n 39%|████████████████▌                          | 17/44 [01:01<01:42,  3.81s/it]\u001b[A\n 41%|█████████████████▌                         | 18/44 [01:05<01:39,  3.81s/it]\u001b[A\n 43%|██████████████████▌                        | 19/44 [01:08<01:35,  3.81s/it]\u001b[A\n 45%|███████████████████▌                       | 20/44 [01:12<01:31,  3.81s/it]\u001b[A\n 48%|████████████████████▌                      | 21/44 [01:16<01:27,  3.81s/it]\u001b[A\n 50%|█████████████████████▌                     | 22/44 [01:20<01:23,  3.81s/it]\u001b[A\n 52%|██████████████████████▍                    | 23/44 [01:24<01:20,  3.81s/it]\u001b[A\n 55%|███████████████████████▍                   | 24/44 [01:27<01:16,  3.82s/it]\u001b[A\n 57%|████████████████████████▍                  | 25/44 [01:31<01:12,  3.82s/it]\u001b[A\n 59%|█████████████████████████▍                 | 26/44 [01:35<01:08,  3.82s/it]\u001b[A\n 61%|██████████████████████████▍                | 27/44 [01:39<01:04,  3.81s/it]\u001b[A\n 64%|███████████████████████████▎               | 28/44 [01:43<01:01,  3.82s/it]\u001b[A\n 66%|████████████████████████████▎              | 29/44 [01:47<00:57,  3.82s/it]\u001b[A\n 68%|█████████████████████████████▎             | 30/44 [01:50<00:53,  3.82s/it]\u001b[A\n 70%|██████████████████████████████▎            | 31/44 [01:54<00:49,  3.82s/it]\u001b[A\n 73%|███████████████████████████████▎           | 32/44 [01:58<00:45,  3.82s/it]\u001b[A\n 75%|████████████████████████████████▎          | 33/44 [02:02<00:42,  3.82s/it]\u001b[A\n 77%|█████████████████████████████████▏         | 34/44 [02:06<00:38,  3.82s/it]\u001b[A\n 80%|██████████████████████████████████▏        | 35/44 [02:10<00:34,  3.83s/it]\u001b[A\n 82%|███████████████████████████████████▏       | 36/44 [02:13<00:30,  3.83s/it]\u001b[A\n 84%|████████████████████████████████████▏      | 37/44 [02:17<00:26,  3.83s/it]\u001b[A\n 86%|█████████████████████████████████████▏     | 38/44 [02:21<00:22,  3.83s/it]\u001b[A\n 89%|██████████████████████████████████████     | 39/44 [02:25<00:19,  3.83s/it]\u001b[A\n 91%|███████████████████████████████████████    | 40/44 [02:29<00:15,  3.82s/it]\u001b[A\n 93%|████████████████████████████████████████   | 41/44 [02:32<00:11,  3.82s/it]\u001b[A\n 95%|█████████████████████████████████████████  | 42/44 [02:36<00:07,  3.82s/it]\u001b[A\n 98%|██████████████████████████████████████████ | 43/44 [02:40<00:03,  3.83s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.7839272022247314, 'eval_runtime': 168.593, 'eval_samples_per_second': 0.522, 'eval_steps_per_second': 0.261, 'epoch': 0.99}\n100%|███████████████████████████████████████████| 93/93 [42:04<00:00, 25.36s/it]\n100%|███████████████████████████████████████████| 44/44 [02:44<00:00,  3.83s/it]\u001b[A\n                                                                                \u001b[A[rank0]:[2024-05-04 20:07:47,585] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 1.5964253010001812, 'preprocessing_with_comm': 0.014866056999835564, 'state_converting': 0.2845569119999709, <Type.ALL: 'all'>: 1.953739153999777})\n{'train_runtime': 2551.1576, 'train_samples_per_second': 0.147, 'train_steps_per_second': 0.036, 'train_loss': 1.0216076207417313, 'epoch': 0.99}\n100%|███████████████████████████████████████████| 93/93 [42:31<00:00, 27.43s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n \npeft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"\n \n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  torch_dtype=torch.float16,\n  quantization_config= {\"load_in_4bit\": True},\n  device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:08:06.899019Z","iopub.execute_input":"2024-05-04T20:08:06.899661Z","iopub.status.idle":"2024-05-04T20:08:51.127302Z","shell.execute_reply.started":"2024-05-04T20:08:06.899614Z","shell.execute_reply":"2024-05-04T20:08:51.126310Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20cd4eded0fb40eea04172f7833d6234"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randint\n \n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = 2\nmessages = eval_dataset[rand_idx][\"messages\"][:2]\n \n# Test on sample\ninput_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id= tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\n\nprint(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\nprint(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\nprint(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n\n# **Query:**\n# How long was the Revolutionary War?\n# **Original Answer:**\n# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783.\n# **Generated Answer:**\n# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783.","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:12:38.449378Z","iopub.execute_input":"2024-05-04T20:12:38.450119Z","iopub.status.idle":"2024-05-04T20:12:42.365284Z","shell.execute_reply.started":"2024-05-04T20:12:38.450077Z","shell.execute_reply":"2024-05-04T20:12:42.364370Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"**Query:**\nDeliveries slumped 8.2 per cent for the Toyota and Lexus brands and 13 per cent for Daihatsu.\n\n**Original Answer:**\nparent_organization: Lexus, Toyota; owned_by: Lexus, Toyota\n\n**Generated Answer:**\n1: Toyota, Daihatsu; product_or_material_produced: Toyota, vehicle\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"markdown","source":"##### Zip the lora file\nDownload manually from the output section in the sidebar","metadata":{}},{"cell_type":"code","source":"# !ls\n# !zip -0 -r llama-3-8b-FinGPT.zip /home/jupyter/llama-3-8b-FinGPT","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Merge PEFT and base model","metadata":{}},{"cell_type":"code","source":"# # #### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import AutoPeftModelForCausalLM\n# import torch\n \n# # Load PEFT model on CPU\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     \"/home/jupyter/llama-3-8b-FinGPT\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n\n# # Save locally\n# merged_model.save_pretrained(\"/home/jupyter/llama-3-8b-FinGPT-Merged\",safe_serialization=True, max_shard_size=\"2GB\")\n\n# # Publish to Huggingface\n# merged_model.push_to_hub(\"my-awesome-model\", safe_serialization=True, max_shard_size=\"2GB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:12:36.992766Z","iopub.execute_input":"2024-05-04T21:12:36.994111Z","iopub.status.idle":"2024-05-04T21:21:41.707975Z","shell.execute_reply.started":"2024-05-04T21:12:36.994064Z","shell.execute_reply":"2024-05-04T21:21:41.704732Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d8d2b40e4a439e80e932db294815dc"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0edb436f30428b9870adf6c0942057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00009.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ac19f3f4d7487c847188a5acd9eee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00009.safetensors:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae8e363cc3b409a81cf27471f41dc62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00009.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71be539e5d645909704e1abb6b0aa75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 9 LFS files:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c54ad517cb4c4ba782706cdbe7148f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00009.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7a311f1782486a8fa19f459a65bf72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00009.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b67783bc72d4141a85b34e429982f7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00009.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667c93422b8942c28a3f8a35e26ee80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00009.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835a59da88694cad9b0e0d403c351c56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00009.safetensors:   0%|          | 0.00/1.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1fa2a7269b34a698a7a59fed6ea3ae7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00009.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edda783127ab471a9372ad943e52edff"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tobyyu007/my-awesome-model/commit/215f0ff9eb2d4aecb452a3ca10208c57e2cb7389', commit_message='Upload LlamaForCausalLM', commit_description='', oid='215f0ff9eb2d4aecb452a3ca10208c57e2cb7389', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Useful sources\n- https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n- https://www.philschmid.de/fsdp-qlora-llama3\n\n- https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa\n- https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset","metadata":{}}]}