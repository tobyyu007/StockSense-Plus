{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train llama3 with LoRA\n[Source of this notebook](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)\n\nThis notebook is designed for Kaggle notebook with 2 Nvidia T4 GPUs","metadata":{}},{"cell_type":"markdown","source":"### Enviornment setup\n- Set your `HF_TOKEN` at `Add-ons -> Secrets`","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:32:13.672722Z","iopub.status.busy":"2024-05-02T01:32:13.672341Z","iopub.status.idle":"2024-05-02T01:32:13.811496Z","shell.execute_reply":"2024-05-02T01:32:13.810606Z","shell.execute_reply.started":"2024-05-02T01:32:13.672690Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub\n\n# from huggingface_hub import notebook_login\n# notebook_login()\n\n!huggingface-cli login --token $HF_TOKEN\n!pip install -U transformers[torch] datasets\n!pip install -q bitsandbytes trl peft accelerate\n!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:32:13.813515Z","iopub.status.busy":"2024-05-02T01:32:13.813210Z","iopub.status.idle":"2024-05-02T01:33:43.369146Z","shell.execute_reply":"2024-05-02T01:33:43.368212Z","shell.execute_reply.started":"2024-05-02T01:32:13.813462Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\n\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n\nToken has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n\nToken is valid (permission: write).\n\nYour token has been saved to /root/.cache/huggingface/token\n\nLogin successful\n\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n\nCollecting datasets\n\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.39.3)\n\nCollecting transformers[torch]\n\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\n\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n\nCollecting tokenizers<0.20,>=0.19 (from transformers[torch])\n\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\n\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\n\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.29.3)\n\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\n\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\n\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\n\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\n\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n\nDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\n\u001b[?25hInstalling collected packages: tokenizers, transformers, datasets\n\n  Attempting uninstall: tokenizers\n\n    Found existing installation: tokenizers 0.15.2\n\n    Uninstalling tokenizers-0.15.2:\n\n      Successfully uninstalled tokenizers-0.15.2\n\n  Attempting uninstall: transformers\n\n    Found existing installation: transformers 4.39.3\n\n    Uninstalling transformers-4.39.3:\n\n      Successfully uninstalled transformers-4.39.3\n\n  Attempting uninstall: datasets\n\n    Found existing installation: datasets 2.18.0\n\n    Uninstalling datasets-2.18.0:\n\n      Successfully uninstalled datasets-2.18.0\n\nSuccessfully installed datasets-2.19.0 tokenizers-0.19.1 transformers-4.40.1\n\nCollecting flash-attn\n\n  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.1.2)\n\nCollecting einops (from flash-attn)\n\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (21.3)\n\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash-attn) (3.1.1)\n\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.9.0)\n\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.2.1)\n\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\n\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.2.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120607435 sha256=b962f0eb38a2e54a3ece20b4b43f59f5a638ce53fe6e269992c58b119425d1f0\n\n  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n\nSuccessfully built flash-attn\n\nInstalling collected packages: einops, flash-attn\n\nSuccessfully installed einops-0.8.0 flash-attn-2.5.8\n"}]},{"cell_type":"markdown","source":"### Load dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\n# based on config\nraw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n\n# remove this when done debugging to include whole dataset\nindices = range(0,500)\n\ndataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n                \"test\": raw_datasets[\"test_sft\"].select(indices)}\n\nraw_datasets = DatasetDict(dataset_dict)\nraw_datasets","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:34:39.222816Z","iopub.status.busy":"2024-05-02T01:34:39.222504Z","iopub.status.idle":"2024-05-02T01:35:11.511148Z","shell.execute_reply":"2024-05-02T01:35:11.510360Z","shell.execute_reply.started":"2024-05-02T01:34:39.222787Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"072a5b2367764b52aab51fe2d246c558","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.44k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4d59deb351f471dab499609c9efd915","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e758d48727e4e83b3c393db87407ad4","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af88d1243b414a69b535584ec96de7ca","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b64b1003c624be1a8c5f2794f729fa9","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/81.2M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd8476a9db3d42f7a1eca8b4945f34b3","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/244M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e95ce8dc05f444f0b5319ad62685d3ce","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/243M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d96b679219fb45d6a028c0f626440e59","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/243M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e73a0a1be314c488d9891c6ada6f78c","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/80.4M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"603f44abb11b4ac2ad011631a3d046a0","version_major":2,"version_minor":0},"text/plain":["Generating train_sft split:   0%|          | 0/207865 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55c1617950b74081a61b0e1e2598f6ed","version_major":2,"version_minor":0},"text/plain":["Generating test_sft split:   0%|          | 0/23110 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7b24f4b18bb4d0b933d4a5c560616e4","version_major":2,"version_minor":0},"text/plain":["Generating train_gen split:   0%|          | 0/256032 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a8495f191e342aebebff713357f19f1","version_major":2,"version_minor":0},"text/plain":["Generating test_gen split:   0%|          | 0/28304 [00:00<?, ? examples/s]"]},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets[\"train\"][:2]['messages']","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:35:11.527344Z","iopub.status.busy":"2024-05-02T01:35:11.526970Z","iopub.status.idle":"2024-05-02T01:35:11.540543Z","shell.execute_reply":"2024-05-02T01:35:11.539691Z","shell.execute_reply.started":"2024-05-02T01:35:11.527317Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":["[[{'content': \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\",\n","   'role': 'user'},\n","  {'content': 'This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.',\n","   'role': 'assistant'},\n","  {'content': 'Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?',\n","   'role': 'user'},\n","  {'content': \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\",\n","   'role': 'assistant'},\n","  {'content': 'Can you provide me with a link to the documentation for my theme?',\n","   'role': 'user'},\n","  {'content': \"I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\",\n","   'role': 'assistant'},\n","  {'content': 'Can you confirm if this feature also works for the Quick Shop section of my theme?',\n","   'role': 'user'},\n","  {'content': \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\",\n","   'role': 'assistant'}],\n"," [{'content': 'Which famous landmarks should I visit in London, beyond the usual ones?',\n","   'role': 'user'},\n","  {'content': \"1. Leadenhall Market - a beautiful indoor market with stunning Victorian architecture, also used as a filming location in the Harry Potter films.\\n\\n2. St. Dunstan in the East - a ruined church in the middle of the city that has been turned into a beautiful public garden.\\n\\n3. The Monument - a 202-foot-tall column commemorating the Great Fire of London, with a staircase leading to a viewing platform offering great views of the city.\\n\\n4. The Camden Town Markets - an eclectic collection of markets offering food, fashion, and vintage items, plus live music and street performers.\\n\\n5. Novelist's House - the former home of Charles Dickens, now a museum dedicated to his life and works.\\n\\n6. The Old Operating Theatre - a museum housed in the oldest surviving operating theatre in Europe, with exhibits on the history of surgery and medical practices.\\n\\n7. The Churchill War Rooms - an underground complex used as a British government command centre during World War II, now a museum dedicated to Winston Churchill and his leadership during the war.\\n\\n8. The Brunel Museum - a museum dedicated to the life and works of engineer Isambard Kingdom Brunel, with exhibitions on his revolutionary designs for bridges, ships, and tunnels.\\n\\n9. The Clink Prison Museum - a museum in the former site of the notorious medieval prison, with exhibits on the history of crime and punishment in London.\\n\\n10. The Royal Observatory Greenwich - an observatory and museum dedicated to astronomy and navigation, with exhibits on the history of timekeeping and space exploration.\",\n","   'role': 'assistant'},\n","  {'content': \"Hmm, those are interesting suggestions, but I've already seen so many touristy landmarks in London. Is there something more offbeat that you can recommend? Something that locals might know about?\",\n","   'role': 'user'},\n","  {'content': \"Absolutely! Here are some offbeat and lesser-known things to see in London that locals might recommend:\\n\\n1. God's Own Junkyard - a neon wonderland filled with vintage and new neon signs.\\n\\n2. Hornsey Town Hall Arts Centre - a grade II listed building with a rich history and home to many art exhibitions and events.\\n\\n3. Clapham Grand - a historic Victorian theatre that has been transformed into a music venue, cinema and a comedy club.\\n\\n4. Gordon Museum of Pathology - the pathology museum of King's College in London that holds a collection of over 8,000 medical specimens.\\n\\n5. John Soane's Museum - the former home of wealthy architect John Soane is a museum with eclectic collections of paintings, sculptures, and curiosities.\\n\\n6. The Viktor Wynd Museum of Curiosities - a small museum filled with strange and unusual objects, taxidermy, and rare books.\\n\\n7. The Cross Bones Graveyard - a medieval graveyard that became known as an outcast cemetery with a history connected to the sex industry, now a memorial garden.\\n\\n8. Highgate Cemetery - a beautiful Victorian graveyard known for its Gothic architecture and where many famous people, including Karl Marx, are buried.\\n\\n9. Dalston Eastern Curve Garden - a beautiful community garden near the Dalston Junction station that is run by volunteers and has regular events.\\n\\n10. The Geffrye Museum of the Home - a museum that explores the history of domestic interiors in England, with a beautiful garden and regular market events.\\n\\nThere are many other hidden gems in London, and a quick Google search for ‘offbeat things in London’ will bring up many blogs and resources with more options.\",\n","   'role': 'assistant'},\n","  {'content': \"Wow, these offbeat suggestions are really interesting. I had no idea there were so many unique things to see in London. Thanks for the recommendations, you're much more helpful than I expected. Can you tell me more about the Viktor Wynd Museum of Curiosities? It sounds like my kind of place.\",\n","   'role': 'user'},\n","  {'content': \"Sure! The Viktor Wynd Museum of Curiosities, Fine Art & Natural History is a small museum located in Hackney, East London. The museum, founded by collector Viktor Wynd is a place where you can find taxidermy, books, anatomical models, and other curiosities. The museum is famous for its collection of shrunken heads, dodo bones, and the famous mermaid. It is also notable for its dimly lit, atmospheric decor, and its bar which serves cocktails and allows visitors to take drinks into the museum. The museum is open daily except for Mondays and Tuesdays, and it also frequently hosts events and exhibitions, so be sure to check the museum's website to see what's on.\",\n","   'role': 'assistant'},\n","  {'content': \"Woah, the Viktor Wynd Museum sounds like a wacky and fun place to visit. I don't think I've ever seen a shrunken head before. I wonder what kind of cocktails they serve there... Maybe I can convince some friends to check it out with me!\",\n","   'role': 'user'},\n","  {'content': 'Yes, the museum is definitely one of the more unusual places to visit in London! The cocktail menu at Viktor Wynd\\'s bar is just as eclectic as the museum\\'s collection, featuring brightly colored cocktails inspired by the exhibits, such as the \"Shrunken Head\" or \"Mermaid Tail\" cocktails. The museum and bar are on the smaller side, so it would make for a unique and intimate outing with friends. If you\\'re looking for something different and memorable to do in London, the Viktor Wynd Museum is definitely worth checking out!',\n","   'role': 'assistant'},\n","  {'content': \"Whoa, a bar in a museum?! That's wild. I'm definitely going to have to try one of those cocktails. Do you think they have anything with a shrunken head as a garnish? That would be a trip.\",\n","   'role': 'user'},\n","  {'content': \"I am not sure if the Viktor Wynd Museum of Curiosities serves cocktails with an actual shrunken head as a garnish. However, they might have some creative cocktails that incorporate a shrunken head replica or something similar as an element in the drink. Nevertheless, the bar offers a unique setting and a cocktail menu that includes some unusual ingredients so it's definitely worth checking out! It's likely to be an eccentric and memorable experience, and I'm sure you'll have a great time there with your friends.\",\n","   'role': 'assistant'},\n","  {'content': 'I can\\'t wait to see that mermaid at the Viktor Wynd Museum. I wonder if it\\'s real or just a replica. Either way, it\\'s bound to be a fascinating item to behold. And I\\'ll make sure to order that \"Mermaid Tail\" cocktail while I\\'m there!',\n","   'role': 'user'},\n","  {'content': 'Yes, the mermaid at the Viktor Wynd Museum of Curiosities is definitely one of its most famous exhibits. It\\'s actually a recreation made from the skeleton of a monkey and the tail of a fish, so it\\'s not a real mermaid of course, but it\\'s still an intriguing piece. The museum has many other curiosities to discover as well, and the atmosphere there is unlike anything you\\'ll find in other museums. And don\\'t forget to try that \"Mermaid Tail\" cocktail - it\\'s the perfect accompaniment to the museum\\'s quirky vibe!',\n","   'role': 'assistant'}]]"]},"metadata":{}}]},{"cell_type":"code","source":"example = raw_datasets[\"train\"][0]\nmessages = example[\"messages\"]\nfor message in messages:\n  role = message[\"role\"]\n  content = message[\"content\"]\n  print('{0:20}:  {1}'.format(role, content))","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:35:11.542087Z","iopub.status.busy":"2024-05-02T01:35:11.541777Z","iopub.status.idle":"2024-05-02T01:35:11.550869Z","shell.execute_reply":"2024-05-02T01:35:11.550039Z","shell.execute_reply.started":"2024-05-02T01:35:11.542063Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\n\nassistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n\nuser                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n\nassistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n\n\n\n1. Log in to your Shopify account and go to your Online Store.\n\n2. Click on Customize theme for the section-based theme you are using.\n\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n\n6. If available, select 'Show secondary image on hover'.\n\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n\n\n\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n\nuser                :  Can you provide me with a link to the documentation for my theme?\n\nassistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n\nuser                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n\nassistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n\n\n\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"}]},{"cell_type":"markdown","source":"### Load and setup base model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom accelerate import PartialState\n\n# model_id = \"mistralai/Mistral-7B-v0.2\"\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, device_map=\"auto\")\ntokenizer.eos_token_id\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:36:48.309087Z","iopub.status.busy":"2024-05-02T01:36:48.308202Z","iopub.status.idle":"2024-05-02T01:36:49.371419Z","shell.execute_reply":"2024-05-02T01:36:49.370530Z","shell.execute_reply.started":"2024-05-02T01:36:48.309053Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9ea62199c8440d1a62d38467eefbbee","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f3a682135204dc5b5bcfe35f49e873a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"563f036be35c47d0a60b9702e5cfcea7","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"}]},{"cell_type":"code","source":"import re\nimport random\nfrom multiprocessing import cpu_count\n\ndef apply_chat_template(example, tokenizer):\n    messages = example[\"messages\"]\n    # We add an empty system message if there is none\n    if messages[0][\"role\"] != \"system\":\n        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n\n    return example","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:36:56.234105Z","iopub.status.busy":"2024-05-02T01:36:56.233752Z","iopub.status.idle":"2024-05-02T01:36:56.239665Z","shell.execute_reply":"2024-05-02T01:36:56.238742Z","shell.execute_reply.started":"2024-05-02T01:36:56.234077Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"column_names = list(raw_datasets[\"train\"].features)\nraw_datasets = raw_datasets.map(apply_chat_template,\n                                num_proc=cpu_count(),\n                                fn_kwargs={\"tokenizer\": tokenizer},\n                                remove_columns=column_names,\n                                desc=\"Applying chat template\",)\n\nraw_datasets","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:36:58.270633Z","iopub.status.busy":"2024-05-02T01:36:58.270252Z","iopub.status.idle":"2024-05-02T01:37:02.307287Z","shell.execute_reply":"2024-05-02T01:37:02.306332Z","shell.execute_reply.started":"2024-05-02T01:36:58.270603Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"323af59e0027455e9e8fd5fe547274fa","version_major":2,"version_minor":0},"text/plain":["Applying chat template (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43b64c43c0844e959fc8a702e9e19c4e","version_major":2,"version_minor":0},"text/plain":["Applying chat template (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{}}]},{"cell_type":"code","source":"# set pad_token_id equal to the eos_token_id if not set\nif tokenizer.pad_token_id is None:\n  tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Set reasonable default for models without max length\nif tokenizer.model_max_length > 100_000:\n  tokenizer.model_max_length = 2048","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:05.722989Z","iopub.status.busy":"2024-05-02T01:37:05.722638Z","iopub.status.idle":"2024-05-02T01:37:05.728576Z","shell.execute_reply":"2024-05-02T01:37:05.727522Z","shell.execute_reply.started":"2024-05-02T01:37:05.722960Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# create the splits\ntrain_dataset = raw_datasets[\"train\"]\neval_dataset = raw_datasets[\"test\"]\n\nfor index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")\n  print(\"#####################################\")","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:07.490365Z","iopub.status.busy":"2024-05-02T01:37:07.489777Z","iopub.status.idle":"2024-05-02T01:37:07.497467Z","shell.execute_reply":"2024-05-02T01:37:07.496606Z","shell.execute_reply.started":"2024-05-02T01:37:07.490332Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Sample 306 of the processed training set:\n\n\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nGiven the text: Applications are invited for an NIH funded postdoctoral position combining eye tracking and intracranial EEG recordings in humans to study the neural basis of face recognition, object recognition, and social and affective perception, at the University of Pittsburgh, USA.\n\nThe research will focus on understanding the dynamic neural code that underlies the recognition of faces, bodies, objects, facial expressions, and other social and affective stimuli and how this information guides, and is guided by, eye movements. Of particular interest is how this information is coded in interactive neural circuits at the level of large-scale brain networks. The neural data will primarily be local field potentials/ event related potential from intracranial surface electrodes (electrocorticography, ECoG), cortical depth electrodes, and subcortical depth electrodes in humans in conjunction with eye tracking. There is also the potential for studies involving direct cortical stimulation to assess how neural modulation alters visual perception.\n\nThe ideal applicant would hold a Ph.D. In Neuroscience, Psychology, Computer Science, Mathematics, Statistics, Engineering or a related field. Strong technical, computational, and statistical skills are required. Experience with combining eye tracking and electrophysiological data is required (for example, scalp EEG or MEG and eye tracking). Applicants should have a strong track record of publication.\n\nPittsburgh is consistently ranked the most livable city in America and the neuroscientific community here, particularly at the University of Pittsburgh and Carnegie Mellon University, is both very strong and collaborative.\n\nInterested applicants should send a letter of interest, a CV, and the names and contact information for 3 researchers who can provide a recommendation. For further information or to submit an application, please contact Avniel Ghuman, Ph.D. At ghumana@upmc.edu and see our lab webpage atwww.lcnd.pitt.edu<http://www.lcnd.pitt.edu> .\n\nCan you provide a brief overview of the postdoctoral position combining eye tracking and intracranial EEG recordings in humans at University of Pittsburgh funded by NIH?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nThe University of Pittsburgh is inviting applications for a postdoctoral position funded by NIH that focuses on the neural basis of face recognition, object recognition, and social and affective perception. This involves understanding the dynamic neural code that underlies the recognition of faces, bodies, objects, and other social and affective stimuli and how this is guided by eye movements. The research will use intracranial EEG recordings in conjunction with eye tracking and potentially direct cortical stimulation to assess how neural modulation alters visual perception. The ideal applicant would have a Ph.D. In Neuroscience, Psychology, Computer Science, Mathematics, Statistics, Engineering, or a related field and strong technical, computational, and statistical skills. Pittsburgh is consistently ranked the most livable city in America, and the neuroscientific community at the University of Pittsburgh and Carnegie Mellon University is considered strong and collaborative. Interested applicants can send a letter of interest, a CV, and contact information for 3 researchers who can provide a recommendation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nCan you provide more details on the specific techniques used for combining eye tracking and intracranial EEG recordings in this postdoctoral position at the University of Pittsburgh?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nThe postdoctoral position at the University of Pittsburgh combining eye tracking and intracranial EEG recordings will focus on understanding the neural basis of face recognition, object recognition, and social and affective perception. The aim is to study the dynamic neural code that underlies the recognition of faces, bodies, objects, and social and affective stimuli and how this information guides, and is guided by, eye movements. The techniques used in the research will involve intracranial EEG recordings from local field potentials, event-related potentials from surface electrodes (electrocorticography, ECoG), cortical depth electrodes, and subcortical depth electrodes in humans, in conjunction with eye tracking. \n\n\n\nThe research will mainly focus on using electrophysiological data, specifically intracranial EEG recordings, to measure neural activity, combined with eye-tracking data to measure the participants' eye movements. The neural data obtained will primarily comprise local field potentials (LFPs) and event-related potential (ERP) activity. The LFPs will be recorded using intracranial surface electrodes, while cortical depth electrodes and subcortical depth electrodes will be used to record the ERPs. Additionally, there is potential for using direct cortical stimulation to investigate how neural modulation alters visual perception. \n\n\n\nThe combination of eye tracking and electrophysiological data is a powerful technique for understanding the neural basis of perception and cognition as it allows for the measurement of how neural activity changes during eye movements related to visual processing. The successful applicant will thus have experience in combining eye tracking and electrophysiological data (e.g. Scalp EEG or MEG and eye tracking), as well as strong technical, computational, and statistical skills.<|eot_id|>\n\n#####################################\n\nSample 38 of the processed training set:\n\n\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nHow can smart building automation systems be used to improve the experience of building users?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nSmart building automation systems can be used to improve the experience of building users in several ways:\n\n\n\n1. Comfort: Smart building automation systems can help control indoor temperature, lighting, and air quality to ensure occupants are comfortable throughout the day. This can be achieved by using sensors that detect occupancy, temperature, humidity, and even sound.\n\n\n\n2. Energy Efficiency: Smart building automation systems can be used to monitor and control energy consumption, reducing costs and energy use. Occupants can adjust temperature, lighting, and other environmental factors, optimizing energy usage without compromising on comfort.\n\n\n\n3. Security: Smart building automation systems can enhance building security, providing a safe and secure environment for occupants. Access controls, surveillance systems, and intrusion detection systems can help prevent unauthorized access and monitor activity in and around the building.\n\n\n\n4. Productivity: Smart building automation systems can help improve productivity by optimizing lighting and temperature, minimizing distractions and creating an atmosphere that's conducive to work. Besides, automated systems can help book conference rooms, control AV systems, and even create customized workspace settings for individual users.\n\n\n\n5. Personalization: Smart building automation systems can personalize the building experience for occupants, tailoring the environment to their preferences. By using machine learning algorithms, systems can learn an occupant's habits and preferences and adjust environmental factors accordingly.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nCan smart building automation systems also help with maintenance and upkeep of the building?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nYes, smart building automation systems can help with maintenance and upkeep of the building in several ways:\n\n\n\n1. Predictive Maintenance: By using sensors and data analytics, smart building automation systems can predict when equipment or systems are likely to fail, allowing for proactive maintenance.\n\n\n\n2. Condition-Based Maintenance: Smart building automation systems can monitor the condition of equipment in real-time and trigger alerts when maintenance is necessary, reducing unexpected downtime and maintenance costs.\n\n\n\n3. Remote Monitoring: Smart building automation systems can be accessed remotely to monitor equipment and systems, allowing maintenance personnel to troubleshoot and fix issues without having to be onsite.\n\n\n\n4. Asset Tracking: By using sensors and tracking tags, smart building automation systems can keep track of the location and use of equipment and other assets, streamlining inventory management and reducing loss or theft.\n\n\n\n5. Energy Management: Smart building automation systems can monitor energy usage and identify opportunities to increase energy efficiency, reducing maintenance costs and prolonging the life of equipment and systems.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nWow, it sounds like smart building automation systems have a lot of benefits for both occupants and maintenance personnel. Can you give me an example of a building that successfully utilizes these systems?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nCertainly! One example of a building that successfully utilizes smart building automation systems is the Edge in Amsterdam, Netherlands. \n\n\n\nThe Edge is a high-rise office building that uses a range of innovative technologies to maximize sustainability, comfort, and efficiency. Some of the ways that The Edge utilizes smart building automation systems include:\n\n\n\n1. Sensor Networks: The Edge uses more than 30,000 sensors to measure temperature, motion, lighting, and other environmental factors throughout the building, ensuring optimal comfort for occupants.\n\n\n\n2. Automated Lighting: Each workspace in The Edge is equipped with LEDs that can be adjusted according to user preferences and natural lighting levels. The lighting is also connected to the building's sensor network, which allows the lights to automatically adjust based on occupancy and daylight levels.\n\n\n\n3. Smart HVAC: The Edge uses a smart HVAC system that not only heats and cools the building but also generates electricity. The system uses thermal energy storage to regulate temperature and reduce energy consumption during peak hours.\n\n\n\n4. Predictive Analytics: The Edge's smart building automation system uses data analytics to predict when maintenance is needed, reducing unexpected downtime and maintenance costs.\n\n\n\nThanks to these and other smart building automation systems, The Edge has achieved a \"Building Research Establishment Environmental Assessment Method\" rating of \"Outstanding.\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nThat's impressive! Can smart building automation systems also make coffee for users? I mean, that would really take the building experience to the next level.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nIt is possible for smart building automation systems to make coffee for users, but it would require additional equipment, such as a coffee machine, that is also connected to the building's automation system. \n\n\n\nWith these additional components, it would be possible to automate the preparation of coffee in a way that is personalized to each individual user. For example, the automation system could recognize the user's presence in the building, and based on their past coffee preferences, it could start the coffee machine and prepare the coffee, with the desired amount of sugar and milk.\n\n\n\nWhile the ability to make coffee may seem like a small detail, it is an example of how smart building automation systems can improve the user experience and create a more personalized environment. In the future, we may see even more intelligent and advanced technologies integrated into our buildings, enhancing the way we live and work.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nHey, can the automated system also play my favorite music in the office? That would be really cool, don't you think?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nYes, the automated system in a smart building can also play your favorite music in the office, making your working environment more enjoyable and personalized. \n\n\n\nBy integrating your music preferences with the building's automation system, the system can automatically start playing your favorite playlist when you arrive at your workspace, or it could play relaxing tunes during break times to help you recharge. This is possible by using streaming services, wireless speakers, and voice controls that allow you to control the music playlist from your phone or computer.\n\n\n\nOverall, by integrating music preferences with a smart building automation system, users can enjoy a more engaging, personalized and enjoyable experience at their workplace.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nThis is all so fascinating! Can smart building automation systems also order me lunch from my favorite restaurant? That would be a real time-saver!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nYes, smart building automation systems can also help users order food from their favorite restaurants, usually via food delivery apps or online ordering systems. \n\n\n\nBy integrating these services with the building's automation system, users can place lunch orders using smart assistants or other voice-controlled devices. The system can then send the order to the restaurant, provide an estimated delivery time, and process payment automatically using built-in payment processing systems.\n\n\n\nBy automating food ordering, employees can save time, minimize distractions, and enjoy a more convenient and personalized experience at work.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nCan the automation system also order me a massage during my break time? A quick massage would really help me relax and recharge for the rest of the workday.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nIt's possible for a smart building automation system to facilitate booking a massage during break time, but this type of service would likely require additional software and hardware integration, as well as partnerships with service providers.\n\n\n\nFor example, a smart building could partner with a massage provider and enable users to book massages via a mobile app or through voice-controlled devices. The automation system could then communicate with the provider's booking system to confirm appointment details, ensure that massage therapists have access to appropriate parts of the building, and enable seamless payment processing.\n\n\n\nWhile the ability to book a massage using a smart building automation system may seem like a luxury or convenience, it can help improve employee well-being, promote relaxation and reduce stress, ultimately improving productivity and job satisfaction.<|eot_id|>\n\n#####################################\n\nSample 242 of the processed training set:\n\n\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nWrite a social media post in the style of an infographic encouraging people to reduce their meat consumption for the environment. Use statistics and eye-catching visuals to illustrate the impact of meat consumption on greenhouse gas emissions and deforestation. Include actionable tips and resources to help people transition to a plant-based diet. Use hashtags such as #MeatlessMonday and #PlantBased to reach a wider audience.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n🌍🐄 Did you know that livestock production is responsible for 18% of greenhouse gas emissions globally? It's time to take action and reduce our meat consumption for the environment! 🌱\n\n\n\n🌳 Deforestation is also a major issue due to the expansion of livestock production. More than 70% of the Amazon rainforest has been destroyed for cattle ranching. 🌿\n\n\n\nBut there's hope! Here are some actionable tips and resources to help you transition to a plant-based diet:\n\n\n\n🌟 Try #MeatlessMonday or choose one day a week to go meat-free.\n\n🌟 Swap meat for plant-based alternatives like tofu or legumes.\n\n🌟 Incorporate more fruits and veggies into your meals.\n\n🌟 Check out websites like @veganuary for plant-based recipes and meal plans.\n\n\n\nTogether, we can make a difference and protect our planet. 🌎🌿 #PlantBased #SustainableLiving #EatMorePlants #LessMeat #ClimateChange #GreenLiving 🌿🌱🌍<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n\n\nThis is a great start, but can you also include some information on the impact of meat consumption on water usage? And maybe some tips on how to make plant-based meals taste delicious?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\nOf course! 🌿💦\n\n\n\n🌊 Did you know that it takes approximately 1,800 gallons of water to produce just one pound of beef? That's a lot of water! By reducing our meat consumption, we can help to conserve this precious resource. 💧\n\n\n\n🍴 But what about taste? We hear you! Here are some tips on how to make plant-based meals delicious:\n\n\n\n🌟 Experiment with different flavor profiles and spices to bring variety to your meals.\n\n🌟 Try new ingredients like nutritional yeast, tofu, or seitan to add texture and flavor.\n\n🌟 Embrace sauces and condiments to add depth to your meals.\n\n🌟 Look for vegan versions of your favorite meals or adapt your favorite recipes using plant-based alternatives.\n\n\n\nTogether, we can reduce our impact on the environment and still enjoy delicious meals! 🌍🍴 #MeatlessMonday #PlantBased #SustainableLiving #TastyMeals #WaterConservation 💦🌿🌱<|eot_id|>\n\n#####################################\n"}]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nimport torch\n\n# specify how to quantize the model\nquantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Set device_map to \"auto\" to allow multi-GPU training\ndevice_map = \"auto\"#{\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n\nmodel_kwargs = dict(\n#     attn_implementation=False,#\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n    torch_dtype=\"auto\",\n    use_cache=False, # set to False as we're going to use gradient checkpointing\n    device_map=device_map,\n    quantization_config=quantization_config,\n)","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:11.694291Z","iopub.status.busy":"2024-05-02T01:37:11.693940Z","iopub.status.idle":"2024-05-02T01:37:11.706610Z","shell.execute_reply":"2024-05-02T01:37:11.705705Z","shell.execute_reply.started":"2024-05-02T01:37:11.694263Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:15.420073Z","iopub.status.busy":"2024-05-02T01:37:15.419113Z","iopub.status.idle":"2024-05-02T01:37:27.784863Z","shell.execute_reply":"2024-05-02T01:37:27.783992Z","shell.execute_reply.started":"2024-05-02T01:37:15.420035Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","output_type":"stream","text":"2024-05-02 01:37:18.741914: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n\n2024-05-02 01:37:18.742013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n\n2024-05-02 01:37:18.911854: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"}]},{"cell_type":"code","source":"model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:31.958712Z","iopub.status.busy":"2024-05-02T01:37:31.958331Z","iopub.status.idle":"2024-05-02T01:37:31.963248Z","shell.execute_reply":"2024-05-02T01:37:31.962185Z","shell.execute_reply.started":"2024-05-02T01:37:31.958679Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# path where the Trainer will save its checkpoints and logs\ntrained_model_id = \"Llama-3-8B-sft-lora-ultrachat\"\noutput_dir = 'kaggle/working/' + trained_model_id\n\n# based on config\ntraining_args = TrainingArguments(\n    fp16=False, # specify bf16=True instead when training on GPUs that support bf16 else fp16\n    bf16=False,\n    do_eval=True,\n    evaluation_strategy=\"epoch\",\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=2.0e-05,\n    log_level=\"info\",\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=-1,\n    num_train_epochs=1,\n    output_dir=output_dir,\n    overwrite_output_dir=True,\n    per_device_eval_batch_size=1, # originally set to 8\n    per_device_train_batch_size=1, # originally set to 8\n    push_to_hub=False,\n    hub_model_id=trained_model_id,\n    # hub_strategy=\"every_save\",\n    # report_to=\"tensorboard\",\n    report_to=\"none\",\n    save_strategy=\"no\",\n    save_total_limit=None,\n    seed=42,\n)","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:33.639074Z","iopub.status.busy":"2024-05-02T01:37:33.638345Z","iopub.status.idle":"2024-05-02T01:37:33.672411Z","shell.execute_reply":"2024-05-02T01:37:33.671636Z","shell.execute_reply.started":"2024-05-02T01:37:33.639035Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# based on config\npeft_config = LoraConfig(\n        r=64,\n        lora_alpha=16,\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:36.018387Z","iopub.status.busy":"2024-05-02T01:37:36.017621Z","iopub.status.idle":"2024-05-02T01:37:36.023298Z","shell.execute_reply":"2024-05-02T01:37:36.022207Z","shell.execute_reply.started":"2024-05-02T01:37:36.018351Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# del trainer\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:38.313489Z","iopub.status.busy":"2024-05-02T01:37:38.312754Z","iopub.status.idle":"2024-05-02T01:37:38.317316Z","shell.execute_reply":"2024-05-02T01:37:38.316396Z","shell.execute_reply.started":"2024-05-02T01:37:38.313446Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n        model=model_id,\n        model_init_kwargs=model_kwargs,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        dataset_text_field=\"text\",\n        tokenizer=tokenizer,\n        packing=True,\n        peft_config=peft_config,\n    )","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:37:39.883548Z","iopub.status.busy":"2024-05-02T01:37:39.883170Z","iopub.status.idle":"2024-05-02T01:40:19.757449Z","shell.execute_reply":"2024-05-02T01:40:19.756626Z","shell.execute_reply.started":"2024-05-02T01:37:39.883516Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4446cec5dd19455ba24bcec95e005944","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a74a5309ef01425ea8390f64cef6e3ea","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a50006f8d3f42c28c96731892953dad","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58a17192295d442f923348d015f2ab65","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b33eb6584b748ec8d9fa708938c528d","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b36893d4c03443a4be795f9c4e9f0b59","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"691d04023b034a21ab84d2487ba6ba6c","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9fdc9a0bae4475cb8cc1285259b0381","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b025385abcc4a9082d2ab3e3ef4ca93","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcbe4f9e66f54d51a0d914acee1f4c55","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2991 > 2048). Running this sequence through the model will result in indexing errors\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b58e3afc42b74c26a799ef8120ad8dfd","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n"}]},{"cell_type":"code","source":"!export CUDA_VISIBLE_DEVICES=0,1","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:35:16.923485Z","iopub.status.idle":"2024-05-02T01:35:16.923814Z","shell.execute_reply":"2024-05-02T01:35:16.923670Z","shell.execute_reply.started":"2024-05-02T01:35:16.923656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_result = trainer.train()","metadata":{"execution":{"iopub.execute_input":"2024-05-02T01:46:27.271430Z","iopub.status.busy":"2024-05-02T01:46:27.270512Z","iopub.status.idle":"2024-05-02T04:25:12.153453Z","shell.execute_reply":"2024-05-02T04:25:12.152632Z","shell.execute_reply.started":"2024-05-02T01:46:27.271397Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","output_type":"stream","text":"***** Running training *****\n\n  Num examples = 556\n\n  Num Epochs = 1\n\n  Instantaneous batch size per device = 1\n\n  Total train batch size (w. parallel, distributed & accumulation) = 1\n\n  Gradient Accumulation steps = 1\n\n  Total optimization steps = 556\n\n  Number of trainable parameters = 54,525,952\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='556' max='556' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [556/556 2:38:31, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.222300</td>\n","      <td>1.362127</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"***** Running Evaluation *****\n\n  Num examples = 574\n\n  Batch size = 1\n\n\n\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\n"}]},{"cell_type":"markdown","source":"### Save the trained model","metadata":{}},{"cell_type":"code","source":"print('Finished')\ntrainer.save_model(training_args.output_dir)","metadata":{"execution":{"iopub.execute_input":"2024-05-02T04:38:17.980036Z","iopub.status.busy":"2024-05-02T04:38:17.979144Z","iopub.status.idle":"2024-05-02T04:38:28.727585Z","shell.execute_reply":"2024-05-02T04:38:28.726695Z","shell.execute_reply.started":"2024-05-02T04:38:17.979997Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","output_type":"stream","text":"Saving model checkpoint to kaggle/working/Llama-3-8B-sft-lora-ultrachat\n"},{"name":"stdout","output_type":"stream","text":"Finished\n"},{"name":"stderr","output_type":"stream","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/config.json\n\nModel config LlamaConfig {\n\n  \"architectures\": [\n\n    \"LlamaForCausalLM\"\n\n  ],\n\n  \"attention_bias\": false,\n\n  \"attention_dropout\": 0.0,\n\n  \"bos_token_id\": 128000,\n\n  \"eos_token_id\": 128001,\n\n  \"hidden_act\": \"silu\",\n\n  \"hidden_size\": 4096,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 14336,\n\n  \"max_position_embeddings\": 8192,\n\n  \"model_type\": \"llama\",\n\n  \"num_attention_heads\": 32,\n\n  \"num_hidden_layers\": 32,\n\n  \"num_key_value_heads\": 8,\n\n  \"pretraining_tp\": 1,\n\n  \"rms_norm_eps\": 1e-05,\n\n  \"rope_scaling\": null,\n\n  \"rope_theta\": 500000.0,\n\n  \"tie_word_embeddings\": false,\n\n  \"torch_dtype\": \"bfloat16\",\n\n  \"transformers_version\": \"4.40.1\",\n\n  \"use_cache\": true,\n\n  \"vocab_size\": 128256\n\n}\n\n\n\ntokenizer config file saved in kaggle/working/Llama-3-8B-sft-lora-ultrachat/tokenizer_config.json\n\nSpecial tokens file saved in kaggle/working/Llama-3-8B-sft-lora-ultrachat/special_tokens_map.json\n\nSaving model checkpoint to kaggle/working/Llama-3-8B-sft-lora-ultrachat\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/config.json\n\nModel config LlamaConfig {\n\n  \"architectures\": [\n\n    \"LlamaForCausalLM\"\n\n  ],\n\n  \"attention_bias\": false,\n\n  \"attention_dropout\": 0.0,\n\n  \"bos_token_id\": 128000,\n\n  \"eos_token_id\": 128001,\n\n  \"hidden_act\": \"silu\",\n\n  \"hidden_size\": 4096,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 14336,\n\n  \"max_position_embeddings\": 8192,\n\n  \"model_type\": \"llama\",\n\n  \"num_attention_heads\": 32,\n\n  \"num_hidden_layers\": 32,\n\n  \"num_key_value_heads\": 8,\n\n  \"pretraining_tp\": 1,\n\n  \"rms_norm_eps\": 1e-05,\n\n  \"rope_scaling\": null,\n\n  \"rope_theta\": 500000.0,\n\n  \"tie_word_embeddings\": false,\n\n  \"torch_dtype\": \"bfloat16\",\n\n  \"transformers_version\": \"4.40.1\",\n\n  \"use_cache\": true,\n\n  \"vocab_size\": 128256\n\n}\n\n\n\ntokenizer config file saved in kaggle/working/Llama-3-8B-sft-lora-ultrachat/tokenizer_config.json\n\nSpecial tokens file saved in kaggle/working/Llama-3-8B-sft-lora-ultrachat/special_tokens_map.json\n\nDropping the following result as it does not have all the necessary fields:\n\n{'dataset': {'name': 'generator', 'type': 'generator', 'config': 'default', 'split': 'train', 'args': 'default'}}\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b66e58d24de546cbaeec0e0351c4c30d","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b095ab619a7f44459dc57e666fc7fdb6","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"808f54ef9b484d4c8dc9b24d37c452b7","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"## Useful sources\n- https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms","metadata":{}}]}