{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train llama3 with LoRA\n[Source of this notebook](https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa)\n\nThis notebook is designed for Kaggle notebook with 2 Nvidia T4 GPUs","metadata":{}},{"cell_type":"markdown","source":"### Enviornment setup\n- Set your `HF_TOKEN` at `Add-ons -> Secrets`","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n\n!huggingface-cli login --token $HF_TOKEN","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install Pytorch for FSDP and FA/SDPA\n%pip install \"torch==2.2.2\" tensorboard\n \n# Install Hugging Face libraries\n%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-02T23:51:08.320329Z","iopub.execute_input":"2024-06-02T23:51:08.320656Z","iopub.status.idle":"2024-06-02T23:54:02.964507Z","shell.execute_reply.started":"2024-06-02T23:51:08.320624Z","shell.execute_reply":"2024-06-02T23:54:02.963373Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torch==2.2.2\n  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.15.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.2)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.5.2)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.31.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.0.3)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\nDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers==4.40.0\n  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m89.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets==2.18.0 in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: accelerate==0.29.3 in /opt/conda/lib/python3.10/site-packages (0.29.3)\nCollecting evaluate==0.4.1\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nCollecting bitsandbytes==0.43.1\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: huggingface_hub==0.22.2 in /opt/conda/lib/python3.10/site-packages (0.22.2)\nCollecting trl==0.8.6\n  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\nCollecting peft==0.10.0\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3) (2.2.2)\nCollecting responses<0.19 (from evaluate==0.4.1)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2) (4.9.0)\nCollecting tyro>=0.5.11 (from trl==0.8.6)\n  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.40.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.5.40)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\nDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nInstalling collected packages: shtab, responses, tyro, tokenizers, transformers, bitsandbytes, trl, peft, evaluate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed bitsandbytes-0.43.1 evaluate-0.4.1 peft-0.10.0 responses-0.18.0 shtab-1.7.1 tokenizers-0.19.1 transformers-4.40.0 trl-0.8.6 tyro-0.8.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load and prepare the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n \n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are a seasoned stock market analyst. What is the summary of this financial text\"\"\"\n\ndef create_conversation(sample):\n#     return {\n#         \"messages\": [\n#             {\"role\": \"system\", \"content\": system_message},\n#             {\"role\": \"user\", \"content\": sample[\"document\"]},\n#             {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n#         ]\n#     }\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": sample[\"Instruction\"]},\n            {\"role\": \"user\", \"content\": sample[\"Input\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"Output\"]}\n        ]\n    }\n \n# Load dataset from the hub\ndataset = load_dataset(\"ECS289L/Stocksense-Prediction-Current-Week-6stock-llama3\", split=\"train\")\n# print(dataset)\n# dataset = dataset.select(range(0, 50))\n \n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=100, seed=42)\n\nprint(dataset[\"train\"][123][\"messages\"])\n\n# save datasets to disk\ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-02T23:54:02.965926Z","iopub.execute_input":"2024-06-02T23:54:02.966238Z","iopub.status.idle":"2024-06-02T23:54:07.013946Z","shell.execute_reply.started":"2024-06-02T23:54:02.966204Z","shell.execute_reply":"2024-06-02T23:54:07.013090Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 1.97M/1.97M [00:00<00:00, 9.05MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824e59a95f2147bab653ef8c19e74526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48410a72ebb4aa29a1fef290349ecc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd3f9b868e44c9896f10ca38a3d628e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fa5ae2a68c45aa8a54b324f6527b28"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"105366"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load base model and setup training parameters","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T23:54:07.016554Z","iopub.execute_input":"2024-06-02T23:54:07.017278Z","iopub.status.idle":"2024-06-02T23:54:07.225333Z","shell.execute_reply.started":"2024-06-02T23:54:07.017242Z","shell.execute_reply":"2024-06-02T23:54:07.224392Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd156c7fd8e94312946ce4e80b25de50"}},"metadata":{}}]},{"cell_type":"code","source":"%%writefile llama_3_8b_fsdp_qlora.yaml\n# script parameters\nmodel_id: \"ECS289L/Stocksense-Plus-Full\" # Hugging Face model id\ndataset_path: \".\"                      # path to dataset\nmax_seq_len:  4096 # 2048              # max sequence length for model and packing of the dataset\n# training parameters\noutput_dir: \"/home/jupyter/llama-3-8b-FinGPT\" # Temporary output directory for model checkpoints\nreport_to: \"tensorboard\"               # report metrics to tensorboard\nlearning_rate: 0.0003                  # learning rate 2e-4\nlr_scheduler_type: \"constant\"          # learning rate scheduler\nnum_train_epochs: 2                    # number of training epochs\nper_device_train_batch_size: 1         # batch size per device during training\nper_device_eval_batch_size: 1          # batch size for evaluation\ngradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\noptim: adamw_torch                     # use torch adamw optimizer\nlogging_steps: 10                      # log every 10 steps\nsave_strategy: epoch                   # save checkpoint every epoch\nevaluation_strategy: epoch             # evaluate every epoch\nmax_grad_norm: 0.3                     # max gradient norm\nwarmup_ratio: 0.03                     # warmup ratio\nbf16: false                             # use bfloat16 precision\ntf32: false                             # use tf32 precision\ngradient_checkpointing: true           # use gradient checkpointing to save memory\nhub_private_repo: true\n# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\nfsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\nfsdp_config:\n  backward_prefetch: \"backward_pre\"\n  forward_prefetch: \"false\"\n  use_orig_params: \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-06-02T23:54:07.226897Z","iopub.execute_input":"2024-06-02T23:54:07.227195Z","iopub.status.idle":"2024-06-02T23:54:07.234155Z","shell.execute_reply.started":"2024-06-02T23:54:07.227170Z","shell.execute_reply":"2024-06-02T23:54:07.233304Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing llama_3_8b_fsdp_qlora.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_fsdp_qlora.py\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom trl.commands.cli_utils import  TrlParser\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n        set_seed,\n\n)\nfrom trl import setup_chat_format\nfrom peft import LoraConfig\n\n\nfrom trl import (\n   SFTTrainer)\n\n# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n\n# Anthropic/Vicuna like template without the need for special tokens\nLLAMA_3_CHAT_TEMPLATE = (\n    \"{% for message in messages %}\"\n        \"{% if message['role'] == 'system' %}\"\n            \"{{ message['content'] }}\"\n        \"{% elif message['role'] == 'user' %}\"\n            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n        \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n        \"{% endif %}\"\n    \"{% endfor %}\"\n    \"{% if add_generation_prompt %}\"\n    \"{{ '\\n\\nAssistant: ' }}\"\n    \"{% endif %}\"\n)\n\n# LLAMA_3_CHAT_TEMPLATE = (\n# \"{% set ns = namespace(found=false) %}\"\n#   \"{% for message in messages %}\"\n#       \"{% if message['role'] == 'system' %}\"\n#           \"{% set ns.found = true %}\"\n#       \"{% endif %}\"\n#   \"{% endfor %}\"\n#   \"{% if not ns.found %}\"\n#       \"{{ '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n' + 'You are a helpful assistant' + '<|eot_id|>' }}\"\n#   \"{% endif %}\"\n#   \"{% for message in messages %}\"\n#       \"{% if message['role'] == 'system' %}\"\n#           \"{{ '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\"\n#       \"{% else %}\"\n#           \"{% if message['role'] == 'user' %}\"\n#               \"{{ '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>'}}\"\n#           \"{% else %}\"\n#               \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}\"\n#           \"{% endif %}\"\n#       \"{% endif %}\"\n#   \"{% endfor %}\"\n#   \"{% if add_generation_prompt %}\"\n#       \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n#   \"{% endif %}\"\n# )\n\n# ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n\n@dataclass\nclass ScriptArguments:\n    dataset_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to the dataset\"\n        },\n    )\n    model_id: str = field(\n        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n    )\n    max_seq_length: int = field(\n        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n    )\n\n\ndef training_function(script_args, training_args):\n    ################\n    # Dataset\n    ################\n    \n    train_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n        split=\"train\",\n    )\n    test_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n        split=\"train\",\n    )\n\n    ################\n    # Model & Tokenizer\n    ################\n\n    # Tokenizer        \n    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n    \n    # template dataset\n    def template_dataset(examples):\n        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n    \n    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    \n    # print random sample\n    with training_args.main_process_first(\n        desc=\"Log a few random samples from the processed training set\"\n    ):\n        for index in random.sample(range(len(train_dataset)), 2):\n            print(train_dataset[index][\"text\"])\n\n    # Model    \n    torch_dtype = torch.bfloat16\n    quant_storage_dtype = torch.bfloat16\n\n    quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n            bnb_4bit_quant_storage=quant_storage_dtype,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.model_id,\n        quantization_config=quantization_config,\n        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n        torch_dtype=quant_storage_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n    )\n    \n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    ################\n    # PEFT\n    ################\n\n    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n    peft_config = LoraConfig(\n        lora_alpha=8,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n#         modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n    )\n\n    ################\n    # Training\n    ################\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        dataset_text_field=\"text\",\n        eval_dataset=test_dataset,\n        peft_config=peft_config,\n        max_seq_length=script_args.max_seq_length,\n        tokenizer=tokenizer,\n        packing=True,\n        dataset_kwargs={\n            \"add_special_tokens\": False,  # We template with special tokens\n            \"append_concat_token\": False,  # No need to add additional separator token\n        },\n    )\n    if trainer.accelerator.is_main_process:\n        trainer.model.print_trainable_parameters()\n\n    ##########################\n    # Train model\n    ##########################\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    ##########################\n    # SAVE MODEL FOR SAGEMAKER\n    ##########################\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n    \nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, TrainingArguments))\n    script_args, training_args = parser.parse_args_and_config()    \n    \n    # set use reentrant to False\n    if training_args.gradient_checkpointing:\n        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n    # set seed\n    set_seed(training_args.seed)\n  \n    # launch training\n    training_function(script_args, training_args)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T23:54:07.235533Z","iopub.execute_input":"2024-06-02T23:54:07.235820Z","iopub.status.idle":"2024-06-02T23:54:07.250780Z","shell.execute_reply.started":"2024-06-02T23:54:07.235795Z","shell.execute_reply":"2024-06-02T23:54:07.249905Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing run_fsdp_qlora.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"markdown","source":"##### Release unreferenced memory in Python","metadata":{}},{"cell_type":"code","source":"import gc\nimport torch\n\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T23:54:07.251988Z","iopub.execute_input":"2024-06-02T23:54:07.252266Z","iopub.status.idle":"2024-06-02T23:54:09.160489Z","shell.execute_reply.started":"2024-06-02T23:54:07.252240Z","shell.execute_reply":"2024-06-02T23:54:09.159527Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"##### Start training with torchrun","metadata":{}},{"cell_type":"code","source":"!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=2 ./run_fsdp_qlora.py --config llama_3_8b_fsdp_qlora.yaml","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-02T23:54:09.161733Z","iopub.execute_input":"2024-06-02T23:54:09.162013Z","iopub.status.idle":"2024-06-03T03:00:47.826005Z","shell.execute_reply.started":"2024-06-02T23:54:09.161988Z","shell.execute_reply":"2024-06-03T03:00:47.823655Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[2024-06-02 23:54:11,614] torch.distributed.run: [WARNING] \n[2024-06-02 23:54:11,614] torch.distributed.run: [WARNING] *****************************************\n[2024-06-02 23:54:11,614] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-06-02 23:54:11,614] torch.distributed.run: [WARNING] *****************************************\n2024-06-02 23:54:19.161143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-02 23:54:19.161146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-02 23:54:19.161209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-02 23:54:19.161269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-02 23:54:19.252121: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-06-02 23:54:19.252128: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nGenerating train split: 1983 examples [00:00, 72525.57 examples/s]\nGenerating train split: 100 examples [00:00, 42409.54 examples/s]\ntokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 4.41MB/s]\ntokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 37.9MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 459/459 [00:00<00:00, 3.12MB/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nMap: 100%|█████████████████████████| 1983/1983 [00:00<00:00, 3713.59 examples/s]\nMap: 100%|█████████████████████████| 1983/1983 [00:00<00:00, 3759.95 examples/s]\nMap: 100%|███████████████████████████| 100/100 [00:00<00:00, 5765.29 examples/s]\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Verb Technology partners with Meta for integrated shopping By Investing.com\n* Meta stock rebounding amid TikTok controversy\n* Meta Platforms Inc. stock outperforms competitors on strong trading day\n* Amazon's Expanding Ad Business Is Challenging Meta, Google\n* Here's Why Meta Platforms (META) Rose in 2023\n\nNegative Headlines:\n* The companies aren't investing in the metaverse anymore\n* Social media giant Meta under investigation for alleged drug sales on its platforms: WSJ\n* Federal government slams Meta over running scams\n* Leading adviser quits over Instagram’s failure to remove self-harm content\n* Meta's role in illegal drug sales on Facebook is under federal investigation, report says\n\nPredict META's stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%<|eot_id|>\n\nAssistant: increased in 0.11%<|eot_id|>\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Up 290% Since The Start Of 2023, Where Is Meta Platforms Stock Headed?\n* Here's Why Meta Platforms (META) is a Strong Growth Stock\n* Will Meta Platforms Be a $2 Trillion Stock by 2025?\n* Why Meta Platforms' New Dividend Is a Bigger Deal Than You Think\n* Meta's Stock Jumped 10% in January, For the 6th Time Since 2013. Here's How It's Historically Done Afterward.\n\nNegative Headlines:\n* Meta pushes to label all AI images on Instagram and Facebook in crackdown on deceptive content\n* Meta's $90 Million Tracking Deal Challenged Before Ninth Circuit\n* Meta removes Iran supreme leader’s accounts from Facebook and Instagram\n* Meta’s review of hate speech policy sparks concern of further censorship of pro-Palestinian content\n* Meta Platforms Inc. stock underperforms Friday when compared to competitors\n\nPredict META's stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%<|eot_id|>\n\nAssistant: increased in 1.89%<|eot_id|>\nMap: 100%|███████████████████████████| 100/100 [00:00<00:00, 1979.82 examples/s]\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Verb Technology partners with Meta for integrated shopping By Investing.com\n* Meta stock rebounding amid TikTok controversy\n* Meta Platforms Inc. stock outperforms competitors on strong trading day\n* Amazon's Expanding Ad Business Is Challenging Meta, Google\n* Here's Why Meta Platforms (META) Rose in 2023\n\nNegative Headlines:\n* The companies aren't investing in the metaverse anymore\n* Social media giant Meta under investigation for alleged drug sales on its platforms: WSJ\n* Federal government slams Meta over running scams\n* Leading adviser quits over Instagram’s failure to remove self-harm content\n* Meta's role in illegal drug sales on Facebook is under federal investigation, report says\n\nPredict META's stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%<|eot_id|>\n\nAssistant: increased in 0.11%<|eot_id|>\nYou are a seasoned stock market analyst. Your task is to predict the companies' stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%\n\nHuman: Company news during this period are listed below:\n\nPositive Headlines:\n* Up 290% Since The Start Of 2023, Where Is Meta Platforms Stock Headed?\n* Here's Why Meta Platforms (META) is a Strong Growth Stock\n* Will Meta Platforms Be a $2 Trillion Stock by 2025?\n* Why Meta Platforms' New Dividend Is a Bigger Deal Than You Think\n* Meta's Stock Jumped 10% in January, For the 6th Time Since 2013. Here's How It's Historically Done Afterward.\n\nNegative Headlines:\n* Meta pushes to label all AI images on Instagram and Facebook in crackdown on deceptive content\n* Meta's $90 Million Tracking Deal Challenged Before Ninth Circuit\n* Meta removes Iran supreme leader’s accounts from Facebook and Instagram\n* Meta’s review of hate speech policy sparks concern of further censorship of pro-Palestinian content\n* Meta Platforms Inc. stock underperforms Friday when compared to competitors\n\nPredict META's stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%<|eot_id|>\n\nAssistant: increased in 1.89%<|eot_id|>\nconfig.json: 100%|█████████████████████████████| 725/725 [00:00<00:00, 5.12MB/s]\npytorch_model.bin.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 93.3MB/s]\nDownloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\npytorch_model-00001-of-00004.bin:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\npytorch_model-00001-of-00004.bin:   0%|    | 10.5M/4.98G [00:00<02:12, 37.5MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   1%|    | 31.5M/4.98G [00:00<01:03, 77.8MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   1%|     | 52.4M/4.98G [00:00<00:49, 100MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   1%|     | 73.4M/4.98G [00:00<00:39, 124MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   2%|     | 94.4M/4.98G [00:00<00:35, 139MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   2%|▏     | 115M/4.98G [00:00<00:30, 157MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   3%|▏     | 147M/4.98G [00:01<00:27, 176MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   4%|▏     | 178M/4.98G [00:01<00:25, 186MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   4%|▏     | 199M/4.98G [00:01<00:24, 191MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   5%|▎     | 231M/4.98G [00:01<00:23, 201MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   5%|▎     | 262M/4.98G [00:01<00:22, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   6%|▎     | 294M/4.98G [00:01<00:22, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   7%|▍     | 325M/4.98G [00:01<00:21, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   7%|▍     | 357M/4.98G [00:02<00:21, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   8%|▍     | 388M/4.98G [00:02<00:21, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   8%|▍    | 419M/4.98G [00:03<01:23, 54.3MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   9%|▍    | 440M/4.98G [00:03<01:13, 61.8MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:   9%|▍    | 461M/4.98G [00:05<02:15, 33.4MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  10%|▍    | 482M/4.98G [00:05<01:49, 41.0MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  10%|▌    | 503M/4.98G [00:05<01:25, 52.1MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  11%|▌    | 524M/4.98G [00:05<01:07, 65.8MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  11%|▌    | 545M/4.98G [00:05<00:54, 81.7MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  12%|▋     | 577M/4.98G [00:06<00:41, 107MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  12%|▋     | 608M/4.98G [00:06<00:33, 130MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  13%|▊     | 640M/4.98G [00:06<00:29, 149MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  13%|▊     | 671M/4.98G [00:06<00:25, 166MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  14%|▊     | 703M/4.98G [00:06<00:23, 181MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  15%|▉     | 734M/4.98G [00:06<00:22, 191MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  15%|▉     | 765M/4.98G [00:06<00:21, 199MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  16%|▉     | 797M/4.98G [00:07<00:20, 202MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  17%|▉     | 828M/4.98G [00:07<00:20, 204MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  17%|█     | 860M/4.98G [00:07<00:19, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  18%|█     | 891M/4.98G [00:07<00:19, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  19%|█     | 923M/4.98G [00:07<00:18, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  19%|█▏    | 954M/4.98G [00:07<00:18, 217MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  20%|█▏    | 986M/4.98G [00:07<00:18, 217MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  20%|█    | 1.02G/4.98G [00:08<00:18, 217MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  21%|█    | 1.05G/4.98G [00:08<00:18, 216MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  22%|█    | 1.08G/4.98G [00:08<00:18, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  22%|█    | 1.11G/4.98G [00:08<00:17, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  23%|█▏   | 1.14G/4.98G [00:08<00:17, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  24%|█▏   | 1.17G/4.98G [00:08<00:18, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  24%|█▏   | 1.21G/4.98G [00:09<00:18, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  25%|█▏   | 1.23G/4.98G [00:09<00:18, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  25%|█▎   | 1.25G/4.98G [00:09<00:18, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  25%|█▎   | 1.27G/4.98G [00:09<00:17, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  26%|█▎   | 1.29G/4.98G [00:09<00:18, 203MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  26%|█▎   | 1.31G/4.98G [00:09<00:17, 204MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  27%|█▎   | 1.33G/4.98G [00:09<00:18, 202MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  27%|█▎   | 1.35G/4.98G [00:09<00:17, 202MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  28%|█▍   | 1.38G/4.98G [00:09<00:17, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  28%|█▍   | 1.42G/4.98G [00:10<00:17, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  29%|█▍   | 1.45G/4.98G [00:10<00:16, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  30%|█▍   | 1.48G/4.98G [00:10<00:16, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  30%|█▌   | 1.51G/4.98G [00:10<00:15, 217MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  31%|█▌   | 1.54G/4.98G [00:10<00:16, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  32%|█▌   | 1.57G/4.98G [00:10<00:15, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  32%|█▌   | 1.60G/4.98G [00:10<00:15, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  33%|█▋   | 1.64G/4.98G [00:11<00:15, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  34%|█▋   | 1.67G/4.98G [00:11<00:15, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  34%|█▋   | 1.70G/4.98G [00:11<00:15, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  35%|█▋   | 1.73G/4.98G [00:11<00:15, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  35%|█▊   | 1.76G/4.98G [00:11<00:15, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  36%|█▊   | 1.79G/4.98G [00:11<00:15, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  37%|█▊   | 1.82G/4.98G [00:11<00:14, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  37%|█▊   | 1.86G/4.98G [00:12<00:14, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  38%|█▉   | 1.89G/4.98G [00:12<00:14, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  39%|█▉   | 1.92G/4.98G [00:12<00:14, 216MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  39%|█▉   | 1.95G/4.98G [00:12<00:14, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  40%|█▉   | 1.98G/4.98G [00:12<00:13, 216MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  40%|██   | 2.01G/4.98G [00:12<00:13, 217MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  41%|██   | 2.04G/4.98G [00:12<00:13, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  42%|██   | 2.08G/4.98G [00:13<00:13, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  42%|██   | 2.11G/4.98G [00:13<00:13, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  43%|██▏  | 2.14G/4.98G [00:13<00:13, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  44%|██▏  | 2.17G/4.98G [00:13<00:13, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  44%|██▏  | 2.20G/4.98G [00:13<00:12, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  45%|██▏  | 2.23G/4.98G [00:13<00:12, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  46%|██▎  | 2.26G/4.98G [00:14<00:12, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  46%|██▎  | 2.30G/4.98G [00:14<00:12, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  47%|██▎  | 2.33G/4.98G [00:14<00:12, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  47%|██▎  | 2.36G/4.98G [00:14<00:12, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  48%|██▍  | 2.39G/4.98G [00:14<00:12, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  49%|██▍  | 2.42G/4.98G [00:14<00:11, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  49%|██▍  | 2.45G/4.98G [00:14<00:11, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  50%|██▍  | 2.49G/4.98G [00:15<00:11, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  51%|██▌  | 2.52G/4.98G [00:15<00:11, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  51%|██▌  | 2.55G/4.98G [00:15<00:11, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  52%|██▌  | 2.58G/4.98G [00:15<00:11, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  52%|██▌  | 2.61G/4.98G [00:15<00:11, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  53%|██▋  | 2.64G/4.98G [00:15<00:11, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  54%|██▋  | 2.67G/4.98G [00:15<00:10, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  54%|██▋  | 2.71G/4.98G [00:16<00:10, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  55%|██▋  | 2.74G/4.98G [00:16<00:15, 145MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  55%|██▊  | 2.76G/4.98G [00:16<00:14, 156MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  56%|██▊  | 2.78G/4.98G [00:16<00:13, 165MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  56%|██▊  | 2.81G/4.98G [00:16<00:12, 177MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  57%|██▊  | 2.84G/4.98G [00:16<00:11, 186MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  58%|██▉  | 2.87G/4.98G [00:17<00:10, 193MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  58%|██▉  | 2.89G/4.98G [00:17<00:10, 195MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  59%|██▉  | 2.93G/4.98G [00:17<00:10, 200MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  59%|██▉  | 2.95G/4.98G [00:17<00:10, 201MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  60%|██▉  | 2.97G/4.98G [00:17<00:09, 203MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  60%|███  | 3.00G/4.98G [00:17<00:09, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  61%|███  | 3.02G/4.98G [00:17<00:09, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  61%|███  | 3.04G/4.98G [00:17<00:09, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  62%|███  | 3.06G/4.98G [00:18<00:09, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  62%|███  | 3.08G/4.98G [00:18<00:09, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  62%|███  | 3.10G/4.98G [00:18<00:09, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  63%|███▏ | 3.14G/4.98G [00:18<00:08, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  63%|███▏ | 3.16G/4.98G [00:18<00:08, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  64%|███▏ | 3.19G/4.98G [00:18<00:08, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  65%|███▏ | 3.22G/4.98G [00:18<00:08, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  65%|███▎ | 3.24G/4.98G [00:18<00:08, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  66%|███▎ | 3.27G/4.98G [00:19<00:08, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  66%|███▎ | 3.30G/4.98G [00:19<00:07, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  67%|███▎ | 3.33G/4.98G [00:19<00:07, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  68%|███▍ | 3.37G/4.98G [00:19<00:07, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  68%|███▍ | 3.40G/4.98G [00:19<00:07, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  69%|███▍ | 3.43G/4.98G [00:19<00:07, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  70%|███▍ | 3.46G/4.98G [00:19<00:07, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  70%|███▌ | 3.49G/4.98G [00:20<00:06, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  71%|███▌ | 3.52G/4.98G [00:20<00:06, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  71%|███▌ | 3.55G/4.98G [00:20<00:06, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  72%|███▌ | 3.59G/4.98G [00:20<00:06, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  73%|███▋ | 3.62G/4.98G [00:20<00:06, 214MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  73%|███▋ | 3.65G/4.98G [00:20<00:06, 216MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  74%|███▋ | 3.68G/4.98G [00:20<00:06, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  75%|███▋ | 3.71G/4.98G [00:21<00:05, 215MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  75%|███▊ | 3.74G/4.98G [00:21<00:05, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  76%|███▊ | 3.77G/4.98G [00:21<00:05, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  76%|███▊ | 3.81G/4.98G [00:21<00:05, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  77%|███▊ | 3.84G/4.98G [00:21<00:05, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  78%|███▉ | 3.87G/4.98G [00:21<00:05, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  78%|███▉ | 3.90G/4.98G [00:21<00:05, 212MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  79%|███▉ | 3.93G/4.98G [00:22<00:04, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  80%|███▉ | 3.96G/4.98G [00:22<00:04, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  80%|████ | 4.00G/4.98G [00:22<00:05, 182MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  81%|████ | 4.02G/4.98G [00:22<00:05, 183MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  81%|████ | 4.05G/4.98G [00:22<00:04, 193MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  82%|████ | 4.08G/4.98G [00:22<00:04, 200MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  83%|████▏| 4.11G/4.98G [00:23<00:04, 204MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  83%|████▏| 4.14G/4.98G [00:23<00:04, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  84%|████▏| 4.17G/4.98G [00:23<00:03, 211MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  84%|████▏| 4.20G/4.98G [00:23<00:03, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  85%|████▎| 4.24G/4.98G [00:23<00:03, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  86%|████▎| 4.27G/4.98G [00:23<00:03, 213MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  86%|████▎| 4.30G/4.98G [00:23<00:03, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  87%|████▎| 4.33G/4.98G [00:24<00:03, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  88%|████▍| 4.36G/4.98G [00:24<00:02, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  88%|████▍| 4.38G/4.98G [00:24<00:02, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  88%|████▍| 4.40G/4.98G [00:24<00:02, 207MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  89%|████▍| 4.44G/4.98G [00:24<00:02, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  90%|████▍| 4.46G/4.98G [00:24<00:02, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  90%|████▍| 4.48G/4.98G [00:24<00:02, 195MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  90%|████▌| 4.50G/4.98G [00:24<00:02, 198MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  91%|████▌| 4.53G/4.98G [00:25<00:02, 202MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  92%|████▌| 4.56G/4.98G [00:25<00:02, 205MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  92%|████▌| 4.58G/4.98G [00:25<00:01, 204MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  92%|████▌| 4.60G/4.98G [00:25<00:01, 205MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  93%|████▋| 4.63G/4.98G [00:25<00:01, 205MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  94%|████▋| 4.66G/4.98G [00:25<00:01, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  94%|████▋| 4.69G/4.98G [00:25<00:01, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  95%|████▋| 4.71G/4.98G [00:25<00:01, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  95%|████▊| 4.74G/4.98G [00:26<00:01, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  96%|████▊| 4.76G/4.98G [00:26<00:01, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  96%|████▊| 4.78G/4.98G [00:26<00:00, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  96%|████▊| 4.80G/4.98G [00:26<00:00, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  97%|████▊| 4.82G/4.98G [00:26<00:00, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  97%|████▊| 4.84G/4.98G [00:26<00:00, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  98%|████▉| 4.87G/4.98G [00:26<00:00, 206MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  98%|████▉| 4.90G/4.98G [00:26<00:00, 209MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  99%|████▉| 4.93G/4.98G [00:27<00:00, 210MB/s]\u001b[A\npytorch_model-00001-of-00004.bin:  99%|████▉| 4.95G/4.98G [00:27<00:00, 208MB/s]\u001b[A\npytorch_model-00001-of-00004.bin: 100%|█████| 4.98G/4.98G [00:27<00:00, 183MB/s]\u001b[A\nDownloading shards:  25%|██████▎                  | 1/4 [00:27<01:22, 27.59s/it]\npytorch_model-00002-of-00004.bin:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\npytorch_model-00002-of-00004.bin:   0%|    | 10.5M/5.00G [00:00<01:27, 57.3MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   0%|    | 21.0M/5.00G [00:00<01:12, 68.8MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   1%|    | 31.5M/5.00G [00:00<01:02, 79.7MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   1%|     | 52.4M/5.00G [00:00<00:47, 103MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   1%|     | 73.4M/5.00G [00:00<00:39, 126MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   2%|    | 94.4M/5.00G [00:02<02:39, 30.7MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   2%|     | 105M/5.00G [00:02<02:15, 36.1MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   3%|▏    | 126M/5.00G [00:02<01:35, 51.3MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   3%|▏    | 147M/5.00G [00:02<01:09, 69.6MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   3%|▏    | 168M/5.00G [00:02<01:10, 68.3MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   4%|▏    | 199M/5.00G [00:02<00:50, 95.7MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   5%|▎     | 231M/5.00G [00:03<00:39, 120MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   5%|▎     | 252M/5.00G [00:03<00:35, 135MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   5%|▎     | 273M/5.00G [00:03<00:31, 148MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   6%|▎     | 294M/5.00G [00:03<00:29, 162MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   6%|▍     | 315M/5.00G [00:03<00:27, 171MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   7%|▍     | 346M/5.00G [00:03<00:25, 185MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   8%|▍     | 377M/5.00G [00:03<00:23, 193MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   8%|▍     | 409M/5.00G [00:04<00:22, 200MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   9%|▌     | 440M/5.00G [00:04<00:22, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:   9%|▌     | 472M/5.00G [00:04<00:21, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  10%|▌     | 503M/5.00G [00:04<00:21, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  11%|▋     | 535M/5.00G [00:04<00:20, 215MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  11%|▋     | 566M/5.00G [00:04<00:20, 215MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  12%|▋     | 598M/5.00G [00:04<00:20, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  13%|▊     | 629M/5.00G [00:05<00:20, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  13%|▊     | 661M/5.00G [00:05<00:20, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  14%|▊     | 692M/5.00G [00:05<00:19, 215MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  14%|▊     | 724M/5.00G [00:05<00:19, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  15%|▉     | 755M/5.00G [00:05<00:19, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  16%|▉     | 786M/5.00G [00:05<00:19, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  16%|▉     | 818M/5.00G [00:05<00:19, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  17%|█     | 849M/5.00G [00:06<00:19, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  18%|█     | 881M/5.00G [00:06<00:19, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  18%|█     | 912M/5.00G [00:06<00:18, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  19%|█▏    | 944M/5.00G [00:06<00:18, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  20%|█▏    | 975M/5.00G [00:06<00:18, 217MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  20%|█    | 1.01G/5.00G [00:06<00:19, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  21%|█    | 1.04G/5.00G [00:06<00:19, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  21%|█    | 1.07G/5.00G [00:07<00:18, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  22%|█    | 1.10G/5.00G [00:07<00:18, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  23%|▉   | 1.13G/5.00G [00:08<01:11, 54.3MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  23%|▉   | 1.15G/5.00G [00:08<00:59, 64.7MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  24%|▉   | 1.18G/5.00G [00:09<00:45, 83.8MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  24%|█▏   | 1.22G/5.00G [00:09<00:36, 104MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  25%|█▏   | 1.25G/5.00G [00:09<00:30, 125MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  25%|█▎   | 1.27G/5.00G [00:09<00:27, 137MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  26%|█▎   | 1.30G/5.00G [00:09<00:23, 156MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  27%|█▎   | 1.33G/5.00G [00:09<00:21, 172MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  27%|█▎   | 1.36G/5.00G [00:09<00:19, 182MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  28%|█▍   | 1.39G/5.00G [00:10<00:18, 193MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  29%|█▍   | 1.43G/5.00G [00:10<00:17, 200MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  29%|█▍   | 1.46G/5.00G [00:10<00:17, 205MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  30%|█▍   | 1.49G/5.00G [00:10<00:16, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  30%|█▌   | 1.52G/5.00G [00:10<00:16, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  31%|█▌   | 1.55G/5.00G [00:10<00:17, 192MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  31%|█▌   | 1.57G/5.00G [00:10<00:17, 195MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  32%|█▌   | 1.60G/5.00G [00:11<00:16, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  33%|█▋   | 1.64G/5.00G [00:11<00:16, 205MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  33%|█▋   | 1.67G/5.00G [00:11<00:16, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  34%|█▋   | 1.70G/5.00G [00:11<00:15, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  35%|█▋   | 1.73G/5.00G [00:11<00:15, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  35%|█▊   | 1.76G/5.00G [00:11<00:15, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  36%|█▊   | 1.79G/5.00G [00:11<00:15, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  36%|█▊   | 1.82G/5.00G [00:12<00:14, 215MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  37%|█▊   | 1.86G/5.00G [00:12<00:14, 218MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  38%|█▉   | 1.89G/5.00G [00:12<00:14, 219MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  38%|█▉   | 1.92G/5.00G [00:12<00:14, 217MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  39%|█▉   | 1.95G/5.00G [00:12<00:14, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  39%|█▉   | 1.97G/5.00G [00:12<00:14, 203MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  40%|█▉   | 1.99G/5.00G [00:12<00:14, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  40%|██   | 2.01G/5.00G [00:13<00:14, 202MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  41%|██   | 2.03G/5.00G [00:13<00:14, 198MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  41%|██   | 2.06G/5.00G [00:13<00:14, 200MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  42%|██   | 2.08G/5.00G [00:13<00:15, 191MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  42%|██   | 2.10G/5.00G [00:13<00:15, 183MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  42%|██   | 2.12G/5.00G [00:13<00:16, 170MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  43%|██▏  | 2.15G/5.00G [00:13<00:15, 183MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  43%|██▏  | 2.17G/5.00G [00:13<00:15, 188MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  44%|██▏  | 2.19G/5.00G [00:13<00:14, 193MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  44%|██▏  | 2.22G/5.00G [00:14<00:13, 199MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  45%|██▏  | 2.24G/5.00G [00:14<00:13, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  45%|██▎  | 2.26G/5.00G [00:14<00:13, 199MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  46%|██▎  | 2.29G/5.00G [00:14<00:13, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  46%|██▎  | 2.32G/5.00G [00:14<00:12, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  47%|██▎  | 2.34G/5.00G [00:14<00:12, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  47%|██▎  | 2.37G/5.00G [00:14<00:12, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  48%|██▍  | 2.39G/5.00G [00:14<00:13, 188MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  48%|██▍  | 2.41G/5.00G [00:15<00:13, 193MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  49%|██▍  | 2.44G/5.00G [00:15<00:13, 185MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  49%|██▍  | 2.46G/5.00G [00:15<00:13, 187MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  50%|██▍  | 2.50G/5.00G [00:15<00:13, 190MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  50%|██▌  | 2.52G/5.00G [00:15<00:14, 167MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  51%|██▌  | 2.54G/5.00G [00:15<00:14, 172MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  51%|██▌  | 2.57G/5.00G [00:15<00:13, 185MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  52%|██▌  | 2.59G/5.00G [00:16<00:12, 190MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  52%|██▌  | 2.61G/5.00G [00:16<00:12, 194MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  53%|██▋  | 2.64G/5.00G [00:16<00:11, 203MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  53%|██▋  | 2.67G/5.00G [00:16<00:11, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  54%|██▋  | 2.69G/5.00G [00:16<00:11, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  54%|██▋  | 2.72G/5.00G [00:16<00:10, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  55%|██▋  | 2.74G/5.00G [00:16<00:10, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  55%|██▊  | 2.77G/5.00G [00:16<00:10, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  56%|██▊  | 2.80G/5.00G [00:17<00:10, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  56%|██▊  | 2.82G/5.00G [00:17<00:10, 202MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  57%|██▊  | 2.85G/5.00G [00:17<00:10, 200MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  57%|██▊  | 2.87G/5.00G [00:17<00:10, 194MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  58%|██▉  | 2.89G/5.00G [00:17<00:11, 190MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  59%|██▉  | 2.93G/5.00G [00:17<00:10, 198MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  59%|██▉  | 2.96G/5.00G [00:17<00:10, 202MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  60%|██▉  | 2.98G/5.00G [00:17<00:10, 202MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  60%|███  | 3.01G/5.00G [00:18<00:09, 204MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  61%|███  | 3.03G/5.00G [00:18<00:09, 204MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  61%|███  | 3.06G/5.00G [00:18<00:09, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  62%|███  | 3.09G/5.00G [00:18<00:09, 211MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  62%|███  | 3.12G/5.00G [00:18<00:08, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  63%|███▏ | 3.16G/5.00G [00:18<00:08, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  64%|███▏ | 3.18G/5.00G [00:18<00:08, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  64%|███▏ | 3.20G/5.00G [00:19<00:08, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  65%|███▏ | 3.23G/5.00G [00:19<00:08, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  65%|███▎ | 3.26G/5.00G [00:19<00:08, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  66%|███▎ | 3.29G/5.00G [00:19<00:08, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  66%|███▎ | 3.32G/5.00G [00:19<00:08, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  67%|███▎ | 3.36G/5.00G [00:19<00:07, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  68%|███▍ | 3.39G/5.00G [00:19<00:07, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  68%|███▍ | 3.42G/5.00G [00:20<00:07, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  69%|███▍ | 3.45G/5.00G [00:20<00:07, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  70%|███▍ | 3.48G/5.00G [00:20<00:07, 216MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  70%|███▌ | 3.51G/5.00G [00:20<00:06, 217MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  71%|███▌ | 3.54G/5.00G [00:20<00:06, 217MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  72%|███▌ | 3.58G/5.00G [00:20<00:06, 219MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  72%|███▌ | 3.61G/5.00G [00:20<00:06, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  73%|███▋ | 3.64G/5.00G [00:21<00:06, 214MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  73%|███▋ | 3.67G/5.00G [00:21<00:06, 212MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  74%|███▋ | 3.70G/5.00G [00:21<00:06, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  75%|███▋ | 3.73G/5.00G [00:21<00:05, 213MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  75%|███▊ | 3.76G/5.00G [00:21<00:05, 211MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  76%|███▊ | 3.80G/5.00G [00:21<00:05, 211MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  77%|███▊ | 3.83G/5.00G [00:21<00:05, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  77%|███▊ | 3.85G/5.00G [00:22<00:05, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  77%|███▊ | 3.87G/5.00G [00:22<00:05, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  78%|███▉ | 3.90G/5.00G [00:22<00:05, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  78%|███▉ | 3.92G/5.00G [00:22<00:05, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  79%|███▉ | 3.95G/5.00G [00:22<00:05, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  79%|███▉ | 3.97G/5.00G [00:22<00:04, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  80%|████ | 4.01G/5.00G [00:22<00:04, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  81%|████ | 4.03G/5.00G [00:22<00:04, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  81%|████ | 4.05G/5.00G [00:23<00:04, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  82%|████ | 4.08G/5.00G [00:23<00:04, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  82%|████ | 4.11G/5.00G [00:23<00:04, 211MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  83%|████▏| 4.14G/5.00G [00:23<00:04, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  83%|████▏| 4.16G/5.00G [00:23<00:04, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  84%|████▏| 4.19G/5.00G [00:23<00:03, 210MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  85%|████▏| 4.23G/5.00G [00:23<00:03, 211MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  85%|████▎| 4.26G/5.00G [00:24<00:03, 203MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  86%|████▎| 4.28G/5.00G [00:24<00:03, 203MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  86%|████▎| 4.31G/5.00G [00:24<00:03, 198MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  87%|████▎| 4.33G/5.00G [00:24<00:03, 200MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  87%|████▎| 4.36G/5.00G [00:24<00:03, 204MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  88%|████▍| 4.38G/5.00G [00:24<00:03, 197MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  88%|████▍| 4.41G/5.00G [00:25<00:04, 137MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  89%|████▍| 4.45G/5.00G [00:25<00:03, 155MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  90%|████▍| 4.48G/5.00G [00:25<00:03, 169MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  90%|████▌| 4.51G/5.00G [00:25<00:02, 181MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  91%|████▌| 4.54G/5.00G [00:25<00:02, 190MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  91%|████▌| 4.57G/5.00G [00:25<00:02, 196MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  92%|████▌| 4.59G/5.00G [00:25<00:02, 199MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  92%|████▌| 4.61G/5.00G [00:25<00:01, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  93%|████▋| 4.63G/5.00G [00:26<00:01, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  93%|████▋| 4.66G/5.00G [00:26<00:01, 203MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  94%|████▋| 4.68G/5.00G [00:26<00:01, 201MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  94%|████▋| 4.70G/5.00G [00:26<00:01, 202MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  95%|████▋| 4.73G/5.00G [00:26<00:01, 205MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  95%|████▊| 4.75G/5.00G [00:26<00:01, 205MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  95%|████▊| 4.77G/5.00G [00:26<00:01, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  96%|████▊| 4.79G/5.00G [00:26<00:01, 204MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  96%|████▊| 4.82G/5.00G [00:26<00:00, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  97%|████▊| 4.84G/5.00G [00:27<00:00, 206MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  98%|████▉| 4.88G/5.00G [00:27<00:00, 208MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  98%|████▉| 4.91G/5.00G [00:27<00:00, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  99%|████▉| 4.94G/5.00G [00:27<00:00, 209MB/s]\u001b[A\npytorch_model-00002-of-00004.bin:  99%|████▉| 4.96G/5.00G [00:27<00:00, 207MB/s]\u001b[A\npytorch_model-00002-of-00004.bin: 100%|█████| 5.00G/5.00G [00:27<00:00, 180MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 2/4 [00:55<00:55, 27.88s/it]\npytorch_model-00003-of-00004.bin:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\npytorch_model-00003-of-00004.bin:   0%|    | 10.5M/4.92G [00:00<01:02, 78.4MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   1%|     | 31.5M/4.92G [00:00<00:47, 102MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   1%|     | 52.4M/4.92G [00:00<00:39, 125MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   1%|     | 73.4M/4.92G [00:00<00:34, 142MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   2%|▏     | 105M/4.92G [00:00<00:28, 171MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   3%|▏     | 126M/4.92G [00:00<00:27, 173MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   3%|▏     | 147M/4.92G [00:00<00:26, 183MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   3%|▏     | 168M/4.92G [00:01<00:25, 184MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   4%|▏     | 189M/4.92G [00:01<00:25, 185MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   4%|▎     | 220M/4.92G [00:01<00:23, 198MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   5%|▎     | 241M/4.92G [00:01<00:24, 190MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   6%|▎     | 273M/4.92G [00:01<00:22, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   6%|▎     | 304M/4.92G [00:01<00:22, 208MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   7%|▍     | 325M/4.92G [00:01<00:22, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   7%|▍     | 357M/4.92G [00:01<00:21, 210MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   8%|▍     | 388M/4.92G [00:02<00:22, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   9%|▌     | 419M/4.92G [00:02<00:21, 207MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:   9%|▌     | 451M/4.92G [00:02<00:21, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  10%|▌     | 482M/4.92G [00:02<00:20, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  10%|▋     | 514M/4.92G [00:02<00:21, 208MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  11%|▋     | 535M/4.92G [00:02<00:21, 206MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  12%|▋     | 566M/4.92G [00:02<00:22, 195MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  12%|▋     | 598M/4.92G [00:03<00:21, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  13%|▊     | 619M/4.92G [00:03<00:21, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  13%|▊     | 640M/4.92G [00:03<00:21, 196MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  14%|▊     | 671M/4.92G [00:03<00:21, 196MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  14%|▊     | 703M/4.92G [00:03<00:22, 191MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  15%|▋    | 724M/4.92G [00:05<01:29, 46.8MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  15%|▊    | 755M/4.92G [00:05<01:04, 64.9MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  16%|▊    | 786M/4.92G [00:05<00:47, 86.3MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  17%|▉     | 818M/4.92G [00:05<00:37, 108MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  17%|█     | 849M/4.92G [00:05<00:31, 129MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  18%|█     | 881M/4.92G [00:05<00:27, 149MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  19%|█     | 912M/4.92G [00:05<00:24, 166MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  19%|█▏    | 944M/4.92G [00:06<00:24, 161MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  20%|█▏    | 975M/4.92G [00:06<00:22, 175MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  20%|█    | 1.01G/4.92G [00:06<00:21, 178MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  21%|█    | 1.04G/4.92G [00:06<00:20, 190MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  22%|█    | 1.06G/4.92G [00:06<00:20, 188MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  22%|█    | 1.09G/4.92G [00:06<00:19, 195MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  23%|█▏   | 1.12G/4.92G [00:07<00:18, 204MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  23%|█▏   | 1.15G/4.92G [00:07<00:18, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  24%|█▏   | 1.17G/4.92G [00:07<00:18, 200MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  25%|█▏   | 1.21G/4.92G [00:07<00:17, 210MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  25%|█▎   | 1.24G/4.92G [00:07<00:17, 216MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  26%|█▎   | 1.27G/4.92G [00:07<00:16, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  26%|█▎   | 1.30G/4.92G [00:07<00:16, 222MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  27%|█▎   | 1.33G/4.92G [00:08<00:16, 221MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  28%|█▍   | 1.36G/4.92G [00:08<00:15, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  28%|█▍   | 1.39G/4.92G [00:08<00:15, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  29%|█▍   | 1.43G/4.92G [00:08<00:15, 220MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  30%|█▍   | 1.46G/4.92G [00:08<00:15, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  30%|█▌   | 1.49G/4.92G [00:08<00:15, 224MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  31%|█▌   | 1.52G/4.92G [00:08<00:15, 221MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  32%|█▌   | 1.55G/4.92G [00:09<00:14, 224MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  32%|█▌   | 1.58G/4.92G [00:09<00:14, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  33%|█▋   | 1.61G/4.92G [00:09<00:15, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  33%|█▋   | 1.65G/4.92G [00:09<00:14, 218MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  34%|█▋   | 1.68G/4.92G [00:09<00:15, 215MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  35%|█▋   | 1.71G/4.92G [00:09<00:15, 206MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  35%|█▊   | 1.74G/4.92G [00:09<00:15, 211MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  36%|█▊   | 1.77G/4.92G [00:10<00:16, 191MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  36%|█▊   | 1.79G/4.92G [00:10<00:16, 192MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  37%|█▊   | 1.82G/4.92G [00:10<00:15, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  38%|█▉   | 1.86G/4.92G [00:10<00:15, 201MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  38%|█▉   | 1.89G/4.92G [00:10<00:14, 207MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  39%|█▉   | 1.91G/4.92G [00:10<00:14, 207MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  39%|█▉   | 1.94G/4.92G [00:10<00:13, 213MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  40%|██   | 1.97G/4.92G [00:11<00:13, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  41%|██   | 2.00G/4.92G [00:11<00:13, 220MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  41%|██   | 2.03G/4.92G [00:11<00:13, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  42%|██   | 2.07G/4.92G [00:11<00:12, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  43%|██▏  | 2.10G/4.92G [00:11<00:12, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  43%|██▏  | 2.13G/4.92G [00:11<00:12, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  44%|██▏  | 2.16G/4.92G [00:11<00:13, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  45%|██▏  | 2.19G/4.92G [00:12<00:13, 210MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  45%|█▊  | 2.22G/4.92G [00:13<00:49, 54.9MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  46%|█▊  | 2.25G/4.92G [00:13<00:37, 71.3MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  46%|█▊  | 2.29G/4.92G [00:13<00:28, 91.1MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  47%|██▎  | 2.32G/4.92G [00:13<00:23, 113MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  48%|██▍  | 2.35G/4.92G [00:14<00:19, 134MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  48%|██▍  | 2.38G/4.92G [00:14<00:16, 151MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  49%|██▍  | 2.41G/4.92G [00:14<00:14, 169MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  50%|██▍  | 2.44G/4.92G [00:14<00:13, 179MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  50%|██▌  | 2.47G/4.92G [00:14<00:13, 185MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  51%|██▌  | 2.51G/4.92G [00:14<00:13, 179MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  51%|██▌  | 2.53G/4.92G [00:15<00:12, 184MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  52%|██▌  | 2.56G/4.92G [00:15<00:12, 195MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  52%|██▌  | 2.58G/4.92G [00:15<00:12, 194MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  53%|██▋  | 2.60G/4.92G [00:15<00:11, 194MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  54%|██▋  | 2.63G/4.92G [00:15<00:11, 205MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  54%|██▋  | 2.65G/4.92G [00:15<00:11, 206MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  55%|██▋  | 2.68G/4.92G [00:15<00:10, 209MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  55%|██▊  | 2.72G/4.92G [00:15<00:10, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  56%|██▊  | 2.75G/4.92G [00:16<00:09, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  57%|██▊  | 2.78G/4.92G [00:16<00:10, 201MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  57%|██▊  | 2.80G/4.92G [00:16<00:10, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  58%|██▉  | 2.83G/4.92G [00:16<00:09, 209MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  58%|██▉  | 2.86G/4.92G [00:16<00:09, 211MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  59%|██▉  | 2.89G/4.92G [00:16<00:09, 213MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  60%|██▉  | 2.93G/4.92G [00:16<00:09, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  60%|███  | 2.96G/4.92G [00:17<00:08, 218MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  61%|███  | 2.99G/4.92G [00:17<00:08, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  61%|███  | 3.02G/4.92G [00:17<00:08, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  62%|███  | 3.05G/4.92G [00:17<00:08, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  63%|███▏ | 3.08G/4.92G [00:17<00:08, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  63%|███▏ | 3.11G/4.92G [00:17<00:09, 181MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  64%|███▏ | 3.14G/4.92G [00:17<00:10, 170MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  64%|███▏ | 3.17G/4.92G [00:18<00:09, 185MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  65%|███▎ | 3.20G/4.92G [00:18<00:08, 197MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  66%|███▎ | 3.23G/4.92G [00:18<00:08, 203MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  66%|███▎ | 3.25G/4.92G [00:18<00:08, 204MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  67%|███▎ | 3.28G/4.92G [00:18<00:07, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  67%|███▎ | 3.31G/4.92G [00:18<00:07, 212MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  68%|███▍ | 3.34G/4.92G [00:18<00:07, 218MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  69%|███▍ | 3.38G/4.92G [00:19<00:06, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  69%|███▍ | 3.41G/4.92G [00:19<00:06, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  70%|███▍ | 3.44G/4.92G [00:19<00:06, 215MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  71%|███▌ | 3.47G/4.92G [00:19<00:06, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  71%|███▌ | 3.50G/4.92G [00:19<00:06, 222MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  72%|███▌ | 3.53G/4.92G [00:19<00:06, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  73%|███▋ | 3.57G/4.92G [00:19<00:06, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  73%|███▋ | 3.60G/4.92G [00:20<00:05, 221MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  74%|███▋ | 3.63G/4.92G [00:20<00:05, 223MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  74%|███▋ | 3.66G/4.92G [00:20<00:05, 227MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  75%|███▊ | 3.69G/4.92G [00:20<00:05, 226MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  76%|███▊ | 3.72G/4.92G [00:20<00:05, 228MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  76%|███▊ | 3.75G/4.92G [00:20<00:05, 228MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  77%|███▊ | 3.79G/4.92G [00:20<00:05, 220MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  78%|███▉ | 3.82G/4.92G [00:21<00:04, 222MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  78%|███▉ | 3.85G/4.92G [00:21<00:04, 226MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  79%|███▉ | 3.88G/4.92G [00:21<00:04, 227MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  80%|███▉ | 3.91G/4.92G [00:21<00:04, 225MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  80%|████ | 3.94G/4.92G [00:21<00:04, 227MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  81%|████ | 3.97G/4.92G [00:21<00:04, 224MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  81%|████ | 4.01G/4.92G [00:21<00:04, 224MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  82%|████ | 4.04G/4.92G [00:22<00:04, 220MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  83%|████▏| 4.07G/4.92G [00:22<00:04, 205MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  83%|████▏| 4.09G/4.92G [00:22<00:04, 200MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  84%|████▏| 4.11G/4.92G [00:22<00:04, 200MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  84%|████▏| 4.14G/4.92G [00:22<00:03, 205MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  85%|████▏| 4.17G/4.92G [00:22<00:03, 197MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  85%|████▎| 4.19G/4.92G [00:22<00:03, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  86%|████▎| 4.22G/4.92G [00:22<00:03, 193MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  86%|████▎| 4.24G/4.92G [00:23<00:03, 195MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  87%|████▎| 4.27G/4.92G [00:23<00:03, 207MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  87%|████▎| 4.30G/4.92G [00:23<00:02, 214MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  88%|████▍| 4.33G/4.92G [00:23<00:02, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  89%|████▍| 4.36G/4.92G [00:23<00:02, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  89%|████▍| 4.39G/4.92G [00:23<00:02, 207MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  90%|████▌| 4.42G/4.92G [00:23<00:02, 213MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  91%|████▌| 4.46G/4.92G [00:24<00:02, 202MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  91%|████▌| 4.48G/4.92G [00:24<00:02, 199MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  92%|████▌| 4.51G/4.92G [00:24<00:02, 202MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  92%|████▌| 4.53G/4.92G [00:24<00:01, 200MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  93%|████▋| 4.55G/4.92G [00:24<00:01, 202MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  93%|████▋| 4.58G/4.92G [00:24<00:01, 211MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  94%|████▋| 4.61G/4.92G [00:24<00:01, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  94%|████▋| 4.65G/4.92G [00:25<00:01, 214MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  95%|████▊| 4.68G/4.92G [00:25<00:01, 218MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  96%|████▊| 4.71G/4.92G [00:25<00:00, 220MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  96%|████▊| 4.74G/4.92G [00:25<00:00, 216MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  97%|████▊| 4.77G/4.92G [00:25<00:00, 216MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  98%|████▉| 4.80G/4.92G [00:25<00:00, 214MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  98%|████▉| 4.83G/4.92G [00:25<00:00, 219MB/s]\u001b[A\npytorch_model-00003-of-00004.bin:  99%|████▉| 4.87G/4.92G [00:26<00:00, 217MB/s]\u001b[A\npytorch_model-00003-of-00004.bin: 100%|█████| 4.92G/4.92G [00:26<00:00, 187MB/s]\u001b[A\nDownloading shards:  75%|██████████████████▊      | 3/4 [01:22<00:27, 27.23s/it]\npytorch_model-00004-of-00004.bin:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\npytorch_model-00004-of-00004.bin:   1%|    | 10.5M/1.17G [00:00<00:17, 66.2MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:   2%|    | 21.0M/1.17G [00:00<00:14, 77.4MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:   4%|▏    | 41.9M/1.17G [00:00<00:10, 106MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:   5%|▎    | 62.9M/1.17G [00:00<00:08, 128MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:   8%|▍    | 94.4M/1.17G [00:00<00:06, 163MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  11%|▋     | 126M/1.17G [00:00<00:06, 173MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  13%|▊     | 157M/1.17G [00:01<00:05, 190MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  15%|▉     | 178M/1.17G [00:01<00:05, 194MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  18%|█     | 210M/1.17G [00:01<00:04, 207MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  21%|█▏    | 241M/1.17G [00:01<00:04, 214MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  23%|█▍    | 273M/1.17G [00:01<00:04, 212MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  26%|█▌    | 304M/1.17G [00:01<00:03, 216MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  29%|█▋    | 336M/1.17G [00:01<00:03, 217MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  31%|█▉    | 367M/1.17G [00:01<00:03, 223MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  34%|██    | 398M/1.17G [00:02<00:03, 224MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  37%|██▏   | 430M/1.17G [00:02<00:03, 227MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  39%|██▎   | 461M/1.17G [00:02<00:03, 229MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  42%|██▌   | 493M/1.17G [00:02<00:02, 225MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  45%|██▋   | 524M/1.17G [00:02<00:02, 221MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  48%|██▊   | 556M/1.17G [00:02<00:02, 221MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  50%|███   | 587M/1.17G [00:02<00:02, 222MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  53%|███▏  | 619M/1.17G [00:03<00:02, 220MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  56%|███▎  | 650M/1.17G [00:03<00:02, 221MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  58%|███▌  | 682M/1.17G [00:03<00:02, 224MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  61%|███▋  | 713M/1.17G [00:03<00:02, 224MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  64%|███▊  | 744M/1.17G [00:03<00:01, 223MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  66%|███▉  | 776M/1.17G [00:03<00:01, 223MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  69%|████▏ | 807M/1.17G [00:03<00:01, 219MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  72%|████▎ | 839M/1.17G [00:04<00:01, 213MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  75%|████▍ | 870M/1.17G [00:04<00:01, 219MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  77%|████▋ | 902M/1.17G [00:04<00:01, 217MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  80%|████▊ | 933M/1.17G [00:04<00:01, 225MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  83%|████▉ | 965M/1.17G [00:04<00:00, 225MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  85%|█████ | 996M/1.17G [00:04<00:00, 219MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  88%|████▍| 1.03G/1.17G [00:04<00:00, 223MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  91%|████▌| 1.06G/1.17G [00:05<00:00, 221MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  93%|████▋| 1.09G/1.17G [00:05<00:00, 222MB/s]\u001b[A\npytorch_model-00004-of-00004.bin:  96%|████▊| 1.12G/1.17G [00:05<00:00, 225MB/s]\u001b[A\npytorch_model-00004-of-00004.bin: 100%|█████| 1.17G/1.17G [00:05<00:00, 211MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 4/4 [01:27<00:00, 22.00s/it]\nDownloading shards: 100%|█████████████████████████| 4/4 [01:27<00:00, 22.00s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:24<00:00, 21.13s/it]\nLoading checkpoint shards: 100%|██████████████████| 4/4 [01:24<00:00, 21.17s/it]\ngeneration_config.json: 100%|███████████████████| 143/143 [00:00<00:00, 124kB/s]\nGenerating train split: 837 examples [00:01, 592.23 examples/s]\nGenerating train split: 41 examples [00:00, 654.18 examples/s]\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\ntrainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n{'loss': 1.7623, 'grad_norm': 0.39453125, 'learning_rate': 0.0003, 'epoch': 0.05}\n{'loss': 1.3709, 'grad_norm': 0.314453125, 'learning_rate': 0.0003, 'epoch': 0.1}\n{'loss': 1.2821, 'grad_norm': 0.291015625, 'learning_rate': 0.0003, 'epoch': 0.14}\n{'loss': 1.2375, 'grad_norm': 0.365234375, 'learning_rate': 0.0003, 'epoch': 0.19}\n{'loss': 1.2193, 'grad_norm': 0.2294921875, 'learning_rate': 0.0003, 'epoch': 0.24}\n{'loss': 1.3227, 'grad_norm': 0.279296875, 'learning_rate': 0.0003, 'epoch': 0.29}\n{'loss': 1.2377, 'grad_norm': 0.28515625, 'learning_rate': 0.0003, 'epoch': 0.33}\n{'loss': 1.2225, 'grad_norm': 0.25390625, 'learning_rate': 0.0003, 'epoch': 0.38}\n{'loss': 1.2046, 'grad_norm': 0.26953125, 'learning_rate': 0.0003, 'epoch': 0.43}\n{'loss': 1.225, 'grad_norm': 0.263671875, 'learning_rate': 0.0003, 'epoch': 0.48}\n{'loss': 1.2578, 'grad_norm': 0.2578125, 'learning_rate': 0.0003, 'epoch': 0.53}\n{'loss': 1.1807, 'grad_norm': 0.28125, 'learning_rate': 0.0003, 'epoch': 0.57}  \n{'loss': 1.2679, 'grad_norm': 0.296875, 'learning_rate': 0.0003, 'epoch': 0.62} \n{'loss': 1.1802, 'grad_norm': 0.265625, 'learning_rate': 0.0003, 'epoch': 0.67} \n{'loss': 1.2489, 'grad_norm': 0.3203125, 'learning_rate': 0.0003, 'epoch': 0.72}\n{'loss': 1.2247, 'grad_norm': 0.26953125, 'learning_rate': 0.0003, 'epoch': 0.76}\n{'loss': 1.1989, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 0.81}\n{'loss': 1.1835, 'grad_norm': 0.28515625, 'learning_rate': 0.0003, 'epoch': 0.86}\n{'loss': 1.1749, 'grad_norm': 0.2421875, 'learning_rate': 0.0003, 'epoch': 0.91}\n{'loss': 1.191, 'grad_norm': 0.2421875, 'learning_rate': 0.0003, 'epoch': 0.95} \n 50%|██████████████████▌                  | 209/418 [1:29:44<1:31:11, 26.18s/it]\n  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n 10%|████▏                                       | 2/21 [00:04<00:39,  2.07s/it]\u001b[A\n 14%|██████▎                                     | 3/21 [00:08<00:52,  2.93s/it]\u001b[A\n 19%|████████▍                                   | 4/21 [00:12<00:57,  3.39s/it]\u001b[A\n 24%|██████████▍                                 | 5/21 [00:16<00:58,  3.65s/it]\u001b[A\n 29%|████████████▌                               | 6/21 [00:20<00:57,  3.82s/it]\u001b[A\n 33%|██████████████▋                             | 7/21 [00:24<00:54,  3.93s/it]\u001b[A\n 38%|████████████████▊                           | 8/21 [00:29<00:52,  4.00s/it]\u001b[A\n 43%|██████████████████▊                         | 9/21 [00:33<00:48,  4.05s/it]\u001b[A\n 48%|████████████████████▍                      | 10/21 [00:37<00:44,  4.08s/it]\u001b[A\n 52%|██████████████████████▌                    | 11/21 [00:41<00:41,  4.11s/it]\u001b[A\n 57%|████████████████████████▌                  | 12/21 [00:45<00:37,  4.12s/it]\u001b[A\n 62%|██████████████████████████▌                | 13/21 [00:49<00:33,  4.13s/it]\u001b[A\n 67%|████████████████████████████▋              | 14/21 [00:53<00:28,  4.13s/it]\u001b[A\n 71%|██████████████████████████████▋            | 15/21 [00:58<00:24,  4.13s/it]\u001b[A\n 76%|████████████████████████████████▊          | 16/21 [01:02<00:20,  4.14s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 17/21 [01:06<00:16,  4.14s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 18/21 [01:10<00:12,  4.14s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 19/21 [01:14<00:08,  4.14s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 20/21 [01:18<00:04,  4.14s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.1625100374221802, 'eval_runtime': 87.1754, 'eval_samples_per_second': 0.47, 'eval_steps_per_second': 0.241, 'epoch': 1.0}\n 50%|██████████████████▌                  | 209/418 [1:31:24<1:31:11, 26.18s/it]\n100%|███████████████████████████████████████████| 21/21 [01:23<00:00,  4.14s/it]\u001b[A\n                                                                                \u001b[A[rank0]:[2024-06-03 01:29:20,073] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.5046769270002187, 'preprocessing_with_comm': 0.01930684699982521, 'state_converting': 0.27172219900057826, <Type.ALL: 'all'>: 0.8189440220003235})\n{'loss': 1.1726, 'grad_norm': 0.2421875, 'learning_rate': 0.0003, 'epoch': 1.0} \n{'loss': 1.0869, 'grad_norm': 0.291015625, 'learning_rate': 0.0003, 'epoch': 1.05}\n{'loss': 1.0342, 'grad_norm': 0.357421875, 'learning_rate': 0.0003, 'epoch': 1.1}\n{'loss': 1.0023, 'grad_norm': 0.341796875, 'learning_rate': 0.0003, 'epoch': 1.15}\n{'loss': 1.0394, 'grad_norm': 0.349609375, 'learning_rate': 0.0003, 'epoch': 1.19}\n{'loss': 1.0184, 'grad_norm': 0.37109375, 'learning_rate': 0.0003, 'epoch': 1.24}\n{'loss': 1.0933, 'grad_norm': 0.388671875, 'learning_rate': 0.0003, 'epoch': 1.29}\n{'loss': 1.085, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 1.34}\n{'loss': 1.0323, 'grad_norm': 0.408203125, 'learning_rate': 0.0003, 'epoch': 1.38}\n{'loss': 1.0978, 'grad_norm': 0.3671875, 'learning_rate': 0.0003, 'epoch': 1.43}\n{'loss': 1.0296, 'grad_norm': 0.345703125, 'learning_rate': 0.0003, 'epoch': 1.48}\n{'loss': 1.0242, 'grad_norm': 0.412109375, 'learning_rate': 0.0003, 'epoch': 1.53}\n{'loss': 1.0508, 'grad_norm': 0.349609375, 'learning_rate': 0.0003, 'epoch': 1.58}\n{'loss': 1.0559, 'grad_norm': 0.38671875, 'learning_rate': 0.0003, 'epoch': 1.62}\n{'loss': 1.0171, 'grad_norm': 0.36328125, 'learning_rate': 0.0003, 'epoch': 1.67}\n{'loss': 1.0354, 'grad_norm': 0.3359375, 'learning_rate': 0.0003, 'epoch': 1.72}\n{'loss': 1.0502, 'grad_norm': 0.37109375, 'learning_rate': 0.0003, 'epoch': 1.77}\n{'loss': 1.0409, 'grad_norm': 0.419921875, 'learning_rate': 0.0003, 'epoch': 1.81}\n{'loss': 1.0715, 'grad_norm': 0.380859375, 'learning_rate': 0.0003, 'epoch': 1.86}\n{'loss': 1.0484, 'grad_norm': 0.400390625, 'learning_rate': 0.0003, 'epoch': 1.91}\n{'loss': 1.0172, 'grad_norm': 0.34375, 'learning_rate': 0.0003, 'epoch': 1.96}  \n100%|███████████████████████████████████████| 418/418 [3:01:06<00:00, 25.62s/it]\n  0%|                                                    | 0/21 [00:00<?, ?it/s]\u001b[A\n 10%|████▏                                       | 2/21 [00:04<00:38,  2.03s/it]\u001b[A\n 14%|██████▎                                     | 3/21 [00:08<00:51,  2.87s/it]\u001b[A\n 19%|████████▍                                   | 4/21 [00:12<00:56,  3.32s/it]\u001b[A\n 24%|██████████▍                                 | 5/21 [00:16<00:57,  3.58s/it]\u001b[A\n 29%|████████████▌                               | 6/21 [00:20<00:56,  3.74s/it]\u001b[A\n 33%|██████████████▋                             | 7/21 [00:24<00:53,  3.85s/it]\u001b[A\n 38%|████████████████▊                           | 8/21 [00:28<00:50,  3.92s/it]\u001b[A\n 43%|██████████████████▊                         | 9/21 [00:32<00:47,  3.97s/it]\u001b[A\n 48%|████████████████████▍                      | 10/21 [00:36<00:44,  4.00s/it]\u001b[A\n 52%|██████████████████████▌                    | 11/21 [00:40<00:40,  4.02s/it]\u001b[A\n 57%|████████████████████████▌                  | 12/21 [00:44<00:36,  4.04s/it]\u001b[A\n 62%|██████████████████████████▌                | 13/21 [00:48<00:32,  4.05s/it]\u001b[A\n 67%|████████████████████████████▋              | 14/21 [00:52<00:28,  4.06s/it]\u001b[A\n 71%|██████████████████████████████▋            | 15/21 [00:56<00:24,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▊          | 16/21 [01:01<00:20,  4.07s/it]\u001b[A\n 81%|██████████████████████████████████▊        | 17/21 [01:05<00:16,  4.07s/it]\u001b[A\n 86%|████████████████████████████████████▊      | 18/21 [01:09<00:12,  4.07s/it]\u001b[A\n 90%|██████████████████████████████████████▉    | 19/21 [01:13<00:08,  4.07s/it]\u001b[A\n 95%|████████████████████████████████████████▉  | 20/21 [01:17<00:04,  4.07s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.1817070245742798, 'eval_runtime': 85.5611, 'eval_samples_per_second': 0.479, 'eval_steps_per_second': 0.245, 'epoch': 2.0}\n100%|███████████████████████████████████████| 418/418 [3:02:32<00:00, 25.62s/it]\n100%|███████████████████████████████████████████| 21/21 [01:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A[rank0]:[2024-06-03 03:00:27,697] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.5119295620006596, 'preprocessing_with_comm': 0.005246414999419358, 'state_converting': 0.286523496000882, <Type.ALL: 'all'>: 0.8273109230012778})\n{'train_runtime': 10976.8291, 'train_samples_per_second': 0.153, 'train_steps_per_second': 0.038, 'train_loss': 1.1504081949662934, 'epoch': 2.0}\n100%|███████████████████████████████████████| 418/418 [3:02:56<00:00, 26.26s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n \npeft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"\n \n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  torch_dtype=torch.float16,\n  quantization_config= {\"load_in_4bit\": True},\n  device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:00:47.829010Z","iopub.execute_input":"2024-06-03T03:00:47.829340Z","iopub.status.idle":"2024-06-03T03:01:19.389482Z","shell.execute_reply.started":"2024-06-03T03:00:47.829305Z","shell.execute_reply":"2024-06-03T03:01:19.388487Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee53351681dd415ea3d32019c1bac431"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randint\n \n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = 4\nmessages = eval_dataset[rand_idx][\"messages\"][:2]\n\n# Test on sample\ninput_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id= tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\n\nprint(f\"**System Prompt:**\\n{eval_dataset[rand_idx]['messages'][0]['content']}\\n\")\nprint(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\nprint(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\nprint(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n\n# **Query:**\n# How long was the Revolutionary War?\n# **Original Answer:**\n# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783.\n# **Generated Answer:**\n# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783.","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:27:34.052452Z","iopub.execute_input":"2024-06-03T03:27:34.053170Z","iopub.status.idle":"2024-06-03T03:30:57.621936Z","shell.execute_reply.started":"2024-06-03T03:27:34.053125Z","shell.execute_reply":"2024-06-03T03:30:57.620116Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Test on sample\u001b[39;00m\n\u001b[1;32m     11\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages,add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**System Prompt:**\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00meval_dataset[rand_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2799\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1208\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1205\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:756\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:240\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"markdown","source":"##### Zip the lora file\nDownload manually from the output section in the sidebar","metadata":{}},{"cell_type":"code","source":"# !ls\n# !zip -0 -r llama-3-8b-FinGPT.zip /home/jupyter/llama-3-8b-FinGPT","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:01:27.359678Z","iopub.execute_input":"2024-06-03T03:01:27.360374Z","iopub.status.idle":"2024-06-03T03:01:27.364384Z","shell.execute_reply.started":"2024-06-03T03:01:27.360338Z","shell.execute_reply":"2024-06-03T03:01:27.363542Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"##### Merge PEFT and base model","metadata":{}},{"cell_type":"code","source":"# !ls /home/jupyter","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:01:27.365986Z","iopub.execute_input":"2024-06-03T03:01:27.366288Z","iopub.status.idle":"2024-06-03T03:01:27.376373Z","shell.execute_reply.started":"2024-06-03T03:01:27.366264Z","shell.execute_reply":"2024-06-03T03:01:27.375536Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\nfrom peft import AutoPeftModelForCausalLM\nimport torch\nfrom transformers import AutoTokenizer\n \n# Load PEFT model on CPU\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"/home/jupyter/llama-3-8b-FinGPT\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n)\n# Merge LoRA and base model and save\nmerged_model = model.merge_and_unload()\n\n# Save locally\nmerged_model.save_pretrained(\"/home/jupyter/llama-3-8b-FinGPT-Merged\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer = AutoTokenizer.from_pretrained(\"/home/jupyter/llama-3-8b-FinGPT\")\ntokenizer.save_pretrained(\"/home/jupyter/llama-3-8b-FinGPT-Merged\",safe_serialization=True)\n# !zip -0 -r llama-3-8b-FinGPT-Merged.zip /home/jupyter/llama-3-8b-FinGPT-Merged\n\n# # Publish to Huggingface\n# merged_model.push_to_hub(\"ECS289L/Stocksense-Plus-Prediction\", safe_serialization=True)\n# peft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"\n# tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n# tokenizer.push_to_hub(\"ECS289L/Stocksense-Plus-Prediction\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:01:27.377689Z","iopub.execute_input":"2024-06-03T03:01:27.377980Z","iopub.status.idle":"2024-06-03T03:03:14.948307Z","shell.execute_reply.started":"2024-06-03T03:01:27.377956Z","shell.execute_reply":"2024-06-03T03:03:14.947344Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1673530d0a6f4097b11ed34156bd94e4"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/home/jupyter/llama-3-8b-FinGPT-Merged/tokenizer_config.json',\n '/home/jupyter/llama-3-8b-FinGPT-Merged/special_tokens_map.json',\n '/home/jupyter/llama-3-8b-FinGPT-Merged/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"### Covert to GGUF","metadata":{}},{"cell_type":"code","source":"# !pip install --upgrade huggingface_hub\n# from huggingface_hub import HfApi, list_models, snapshot_download\n# snapshot_download(\"ECS289L/Stocksense-Plus-Prediction\", local_dir_use_symlinks=False, local_dir=\"/home/jupyter/llama-3-8b-FinGPT\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:03:14.949772Z","iopub.execute_input":"2024-06-03T03:03:14.950161Z","iopub.status.idle":"2024-06-03T03:03:14.954853Z","shell.execute_reply.started":"2024-06-03T03:03:14.950123Z","shell.execute_reply":"2024-06-03T03:03:14.953914Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp.git\n!cd llama.cpp && make -j 16","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:03:14.956050Z","iopub.execute_input":"2024-06-03T03:03:14.956380Z","iopub.status.idle":"2024-06-03T03:05:31.131459Z","shell.execute_reply.started":"2024-06-03T03:03:14.956340Z","shell.execute_reply":"2024-06-03T03:05:31.129964Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 26191, done.\u001b[K\nremote: Counting objects: 100% (6112/6112), done.\u001b[K\nremote: Compressing objects: 100% (262/262), done.\u001b[K\nremote: Total 26191 (delta 5973), reused 5870 (delta 5850), pack-reused 20079\u001b[K\nReceiving objects: 100% (26191/26191), 49.94 MiB | 27.99 MiB/s, done.\nResolving deltas: 100% (18707/18707), done.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"I ccache not found. Consider installing it for faster compilation.\nI llama.cpp build info: \nI UNAME_S:   Linux\nI UNAME_P:   x86_64\nI UNAME_M:   x86_64\nI CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \nI CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \nI NVCCFLAGS: -std=c++11 -O3 \nI LDFLAGS:    \nI CC:        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nI CXX:       c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c llama.cpp -o llama.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o sampling.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o grammar-parser.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/console.cpp -o console.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c sgemm.cpp -o sgemm.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\ncc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode.cpp -o unicode.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c unicode-data.cpp -o unicode-data.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/train.cpp -o train.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o ngram-cache.o\ncc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o build-info.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval  \n\n====  Run ./main -h for help.  ====\n\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  build-info.o ggml.o llama.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \nc++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server   \n","output_type":"stream"}]},{"cell_type":"code","source":"#!cd llama.cpp && python3 -m pip install -r requirements.txt\n!cd llama.cpp && python3 convert-hf-to-gguf.py /home/jupyter/llama-3-8b-FinGPT-Merged","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:05:31.133284Z","iopub.execute_input":"2024-06-03T03:05:31.133737Z","iopub.status.idle":"2024-06-03T03:07:10.908957Z","shell.execute_reply.started":"2024-06-03T03:05:31.133687Z","shell.execute_reply":"2024-06-03T03:07:10.907470Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nWriting: 100%|███████████████████████████| 16.1G/16.1G [01:28<00:00, 182Mbyte/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /home/jupyter/llama-3-8b-FinGPT-Merged","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:07:10.910989Z","iopub.execute_input":"2024-06-03T03:07:10.911407Z","iopub.status.idle":"2024-06-03T03:07:12.368032Z","shell.execute_reply.started":"2024-06-03T03:07:10.911369Z","shell.execute_reply":"2024-06-03T03:07:12.366974Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"config.json\t\t\t  model-00006-of-00009.safetensors\ngeneration_config.json\t\t  model-00007-of-00009.safetensors\nggml-model-f16.gguf\t\t  model-00008-of-00009.safetensors\nmodel-00001-of-00009.safetensors  model-00009-of-00009.safetensors\nmodel-00002-of-00009.safetensors  model.safetensors.index.json\nmodel-00003-of-00009.safetensors  special_tokens_map.json\nmodel-00004-of-00009.safetensors  tokenizer.json\nmodel-00005-of-00009.safetensors  tokenizer_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd llama.cpp && ./quantize /home/jupyter/llama-3-8b-FinGPT-Merged/ggml-model-f16.gguf /home/jupyter/llama-3-8b-FinGPT-Merged/Stocksense-Plus-Prediction-Q4_K_M.gguf Q4_K_M","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:07:12.369602Z","iopub.execute_input":"2024-06-03T03:07:12.369949Z","iopub.status.idle":"2024-06-03T03:14:51.183790Z","shell.execute_reply.started":"2024-06-03T03:07:12.369914Z","shell.execute_reply":"2024-06-03T03:14:51.182559Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"main: build = 3070 (3413ae21)\nmain: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\nmain: quantizing '/home/jupyter/llama-3-8b-FinGPT-Merged/ggml-model-f16.gguf' to '/home/jupyter/llama-3-8b-FinGPT-Merged/Stocksense-Plus-Prediction-Q4_K_M.gguf' as Q4_K_M\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/jupyter/llama-3-8b-FinGPT-Merged/ggml-model-f16.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = llama-3-8b-FinGPT-Merged\nllama_model_loader: - kv   2:                          llama.block_count u32              = 32\nllama_model_loader: - kv   3:                       llama.context_length u32              = 8192\nllama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                          general.file_type u32              = 1\nllama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128009\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type  f16:  226 tensors\n[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  20/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  21/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  22/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  24/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[  25/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  26/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  27/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  28/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  56/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  57/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  58/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  59/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  60/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  61/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  62/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[  63/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  64/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  65/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  66/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  67/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  68/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  69/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  70/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  71/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  72/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  73/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  74/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  75/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  76/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  77/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  78/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  79/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  80/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  81/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  82/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  83/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  84/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[  85/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  86/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  87/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  88/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  89/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  90/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  91/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[  92/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  93/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  94/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  95/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[  96/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[  97/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[  98/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[  99/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 100/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 137/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 138/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 139/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 140/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 141/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 142/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 143/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 144/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 145/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 218/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 219/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 220/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 221/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 222/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 223/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 224/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 225/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 281/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 282/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n[ 283/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 284/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n[ 285/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 286/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n[ 287/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 288/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n[ 289/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n[ 291/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\nllama_model_quantize_internal: model size  = 15317.02 MB\nllama_model_quantize_internal: quant size  =  4685.30 MB\n\nmain: quantize time = 456450.66 ms\nmain:    total time = 456450.66 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\n\nmodel_id = \"ECS289L/Stocksense-Plus-Prediction-GGUF\"\napi.create_repo(model_id, exist_ok=True, repo_type=\"model\")\napi.upload_file(\n    path_or_fileobj=\"/home/jupyter/llama-3-8b-FinGPT-Merged/Stocksense-Plus-Prediction-Q4_K_M.gguf\",\n    path_in_repo=\"Stocksense-Plus-Prediction-Q4_K_M.gguf\",\n    repo_id=model_id,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:14:51.185536Z","iopub.execute_input":"2024-06-03T03:14:51.185880Z","iopub.status.idle":"2024-06-03T03:17:10.681956Z","shell.execute_reply.started":"2024-06-03T03:14:51.185845Z","shell.execute_reply":"2024-06-03T03:17:10.680999Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Stocksense-Plus-Prediction-Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e51d8da9f384e96879d8db7dc5021b4"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ECS289L/Stocksense-Plus-Prediction-GGUF/commit/2f5eec45db80d8709468208bd1a48ed9742a5d66', commit_message='Upload Stocksense-Plus-Prediction-Q4_K_M.gguf with huggingface_hub', commit_description='', oid='2f5eec45db80d8709468208bd1a48ed9742a5d66', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Accuracy Test","metadata":{}},{"cell_type":"code","source":"!apt install systemctl -y\n!curl -fsSL https://ollama.com/install.sh | sh\n!systemctl start ollama\n!pip install ollama","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:17:10.683560Z","iopub.execute_input":"2024-06-03T03:17:10.683955Z","iopub.status.idle":"2024-06-03T03:17:48.398681Z","shell.execute_reply.started":"2024-06-03T03:17:10.683918Z","shell.execute_reply":"2024-06-03T03:17:48.397573Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n  adwaita-icon-theme gir1.2-glib-2.0 gir1.2-packagekitglib-1.0\n  glib-networking-common glib-networking-services gtk-update-icon-cache\n  hicolor-icon-theme humanity-icon-theme iso-codes libargon2-1 libcap2\n  libcap2-bin libcolord2 libcryptsetup12 libdconf1 libdevmapper1.02.1\n  libepoxy0 libgirepository-1.0-1 libglib2.0-bin libgstreamer1.0-0 libip4tc2\n  libjson-c4 libjson-glib-1.0-0 libjson-glib-1.0-common liblmdb0\n  libpackagekit-glib2-18 libpolkit-agent-1-0 libpolkit-gobject-1-0 libproxy1v5\n  libstemmer0d libxdamage1 libyaml-0-2 python-apt-common python3-apt\n  python3-certifi python3-chardet python3-dbus python3-gi python3-idna\n  python3-requests python3-requests-unixsocket python3-six\n  python3-software-properties python3-urllib3 ubuntu-mono\nUse 'apt autoremove' to remove them.\nThe following additional packages will be installed:\n  emacs-lucid libsystemd0 xaw3dg\nSuggested packages:\n  emacs-common-non-dfsg tini | dumb-init\nThe following packages will be REMOVED:\n  dbus-user-session dconf-gsettings-backend dconf-service emacs-gtk\n  glib-networking gsettings-desktop-schemas libappstream4 libgtk-3-0\n  libgtk-3-common libpam-systemd librest-0.7-0 libsoup-gnome2.4-1 libsoup2.4-1\n  packagekit policykit-1 software-properties-common systemd systemd-sysv\n  systemd-timesyncd\nThe following NEW packages will be installed:\n  emacs-lucid systemctl xaw3dg\nThe following packages will be upgraded:\n  libsystemd0\n1 upgraded, 3 newly installed, 19 to remove and 60 not upgraded.\nNeed to get 4086 kB of archives.\nAfter this operation, 31.1 MB disk space will be freed.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 emacs-lucid amd64 1:26.3+1-1ubuntu2 [3594 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsystemd0 amd64 245.4-4ubuntu3.23 [268 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 xaw3dg amd64 1.5+E-18.3 [149 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 systemctl all 1.4.3424-2 [75.5 kB]\nFetched 4086 kB in 2s (1831 kB/s)    \u001b[0m\u001b[33m\n\n(Reading database ... 113807 files and directories currently installed.)\nRemoving software-properties-common (0.99.9.12) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Removing packagekit (1.1.13-2ubuntu1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Removing policykit-1 (0.105-26ubuntu1.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8\u001b[1mdpkg:\u001b[0m emacs-gtk: dependency problems, but removing anyway as you requested:\n emacs depends on emacs-gtk (>= 1:26.3) | emacs-lucid (>= 1:26.3) | emacs-nox (>= 1:26.3); however:\n  Package emacs-gtk is to be removed.\n  Package emacs-lucid is not installed.\n  Package emacs-nox is not installed.\n\nRemoving emacs-gtk (1:26.3+1-1ubuntu2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Remove dictionaries-common for emacs\nremove/dictionaries-common: Purging byte-compiled files for flavour emacs\nRemove cmake-data for emacs\nRemove emacsen-common for emacs\nemacsen-common: Handling removal of emacsen flavor emacs\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package emacs-lucid.\n(Reading database ... 113705 files and directories currently installed.)\nPreparing to unpack .../emacs-lucid_1%3a26.3+1-1ubuntu2_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking emacs-lucid (1:26.3+1-1ubuntu2) ...\n(Reading database ... 113712 files and directories currently installed.).................................] \u001b8(Reading database ... \nRemoving libgtk-3-0:amd64 (3.24.20-0ubuntu1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Removing libgtk-3-common (3.24.20-0ubuntu1.1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Removing librest-0.7-0:amd64 (0.8.1-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Removing libsoup-gnome2.4-1:amd64 (2.70.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Removing systemd-timesyncd (245.4-4ubuntu3.22) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8\u001b[1mdpkg:\u001b[0m gsettings-desktop-schemas: dependency problems, but removing anyway as you requested:\n glib-networking:amd64 depends on gsettings-desktop-schemas.\n\nRemoving gsettings-desktop-schemas (3.36.0-1ubuntu1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Removing dconf-gsettings-backend:amd64 (0.36.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Removing dconf-service (0.36.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Removing dbus-user-session (1.12.16-2ubuntu2.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Removing libpam-systemd:amd64 (245.4-4ubuntu3.22) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8\u001b[1mdpkg:\u001b[0m systemd: dependency problems, but removing anyway as you requested:\n systemd-sysv depends on systemd (= 245.4-4ubuntu3.22).\n systemd-sysv depends on systemd.\n systemd-sysv depends on systemd (= 245.4-4ubuntu3.22).\n systemd-sysv depends on systemd.\n\nRemoving systemd (245.4-4ubuntu3.22) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8\u001b[1mdpkg:\u001b[0m libsoup2.4-1:amd64: dependency problems, but removing anyway as you requested:\n libappstream4:amd64 depends on libsoup2.4-1 (>= 2.56).\n\nRemoving libsoup2.4-1:amd64 (2.70.0-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Removing glib-networking:amd64 (2.64.2-1ubuntu0.1) ...\n(Reading database ... 112838 files and directories currently installed.)##########.......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8(Reading database ... \nPreparing to unpack .../libsystemd0_245.4-4ubuntu3.23_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Unpacking libsystemd0:amd64 (245.4-4ubuntu3.23) over (245.4-4ubuntu3.22) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libsystemd0:amd64 (245.4-4ubuntu3.23) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Selecting previously unselected package xaw3dg:amd64.\n(Reading database ... 112838 files and directories currently installed.)\nPreparing to unpack .../xaw3dg_1.5+E-18.3_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Unpacking xaw3dg:amd64 (1.5+E-18.3) ...\n(Reading database ... 112877 files and directories currently installed.)#################................] \u001b8(Reading database ... \nRemoving libappstream4:amd64 (0.12.10-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Removing systemd-sysv (245.4-4ubuntu3.22) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [##############################################............] \u001b8Selecting previously unselected package systemctl.\n(Reading database ... 112855 files and directories currently installed.)\nPreparing to unpack .../systemctl_1.4.3424-2_all.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Unpacking systemctl (1.4.3424-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up xaw3dg:amd64 (1.5+E-18.3) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up systemctl (1.4.3424-2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up emacs-lucid (1:26.3+1-1ubuntu2) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8update-alternatives: using /usr/bin/emacs-lucid to provide /usr/bin/emacs (emacs) in auto mode\nupdate-alternatives: warning: skip creation of /usr/share/man/man1/emacs.1.gz because associated file /usr/share/man/man1/emacs.emacs.1.gz (of link group emacs) doesn't exist\nInstall emacsen-common for emacs\nemacsen-common: Handling install of emacsen flavor emacs\nInstall cmake-data for emacs\nInstall dictionaries-common for emacs\ninstall/dictionaries-common: Byte-compiling for emacsen flavour emacs\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for mime-support (3.64ubuntu1) ...\nProcessing triggers for libglib2.0-0:amd64 (2.64.6-1~ubuntu20.04.6) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8No schema files found: removed existing output file.\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Processing triggers for libc-bin (2.31-0ubuntu9.14) ...\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for dbus (1.12.16-2ubuntu2.3) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":">>> Downloading ollama...\n######################################################################## 100.0%#=#=#                                                                          \n>>> Installing ollama to /usr/local/bin...\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting ollama\n  Downloading ollama-0.2.0-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: httpx<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from ollama) (0.27.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\nRequirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.9.0)\nDownloading ollama-0.2.0-py3-none-any.whl (9.5 kB)\nInstalling collected packages: ollama\nSuccessfully installed ollama-0.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile accuracy.py\nimport ollama\nfrom datasets import load_dataset\nimport tqdm\n\n\nmodelfile='''\nFROM /home/jupyter/llama-3-8b-FinGPT-Merged/Stocksense-Plus-Prediction-Q4_K_M.gguf\nSYSTEM You are a seasoned stock market analyst. Your task is to predict the companies' stock price movement for this week based on this week's positive headlines and negative headlines. Give me answer in the format of {increased/decreased/flat} in {X}%\nTEMPLATE \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>\"\nPARAMETER num_keep 24\nPARAMETER stop <|start_header_id|>\nPARAMETER stop <|end_header_id|>\nPARAMETER stop <|eot_id|>\n'''\n\n# modelfile='''\n# FROM llama3:latest\n# SYSTEM What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n# TEMPLATE \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n# {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n# {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n# {{ .Response }}<|eot_id|>\"\n# PARAMETER num_keep 24\n# PARAMETER stop <|start_header_id|>\n# PARAMETER stop <|end_header_id|>\n# PARAMETER stop <|eot_id|>\n# '''\n\nollama.create(model='stocksense-plus-test', modelfile=modelfile)\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nuserPrompts = []\nanswers = []\ngeneratedAnswers = []\ncorrectResponse = 0\n\nfor d in dataset:\n    userPrompts.append([{'role': 'user', 'content': d['messages'][1]['content']+ \"Just me answer in the format of {increased/decreased/flat} in {X}%. Don't say other things.\"}])\n    # userPrompts.append([{'role': 'user', 'content': d['messages'][1]['content']}])\n    groundTruth = d['messages'][2]['content'].split(' ')\n    answers.append((groundTruth[0], groundTruth[2]))\n\n\nfor _ in range(3):\n    for idx, prompts in enumerate(userPrompts):\n        response = ollama.chat(model='stocksense-plus-test', messages=prompts)\n        print(response['message']['content'], answers[idx])\n        upDown = response['message']['content'].split(' ')[0].lower()\n        percentage = response['message']['content'].split(' ')[2]\n        # print(upDown.lower(), percentage)\n        if answers[idx][0] == upDown:\n            correctResponse += 1\n\n# print(answers)\nprint(correctResponse/(len(answers) * 3) * 100, '%')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T03:36:47.038339Z","iopub.execute_input":"2024-06-03T03:36:47.039374Z","iopub.status.idle":"2024-06-03T03:36:47.048608Z","shell.execute_reply.started":"2024-06-03T03:36:47.039319Z","shell.execute_reply":"2024-06-03T03:36:47.047543Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Overwriting accuracy.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python accuracy.py","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-03T03:47:44.536757Z","iopub.execute_input":"2024-06-03T03:47:44.537726Z","iopub.status.idle":"2024-06-03T03:53:01.994183Z","shell.execute_reply.started":"2024-06-03T03:47:44.537680Z","shell.execute_reply":"2024-06-03T03:53:01.992776Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"increased in 3.05% ('decreased', '5.35%')\nincreased in 1.34% ('decreased', '5.3%')\nincreased in 1.39% ('decreased', '1.58%')\nincreased in 0.46% ('increased', '2.73%')\nincreased in 0.6% ('decreased', '0.52%')\ndecreased in 0.37% ('increased', '1.69%')\nincreased in 5.45% ('increased', '1.23%')\nincreased in 6.02% ('decreased', '1.28%')\nincreased in 0.25% ('increased', '0.75%')\nincreased in 0.45% ('decreased', '2.49%')\nincreased in 0.55% ('increased', '3.17%')\ndecreased in 2.01% ('decreased', '1.28%')\ndecreased in 1.0% ('decreased', '5.84%')\ndecreased in 0.43% ('decreased', '1.07%')\ndecreased in 6.33% ('increased', '11.33%')\nincreased in 0.95% ('decreased', '1.46%')\nincreased in 1.6% ('increased', '2.2%')\nincreased in 2.25% ('increased', '1.83%')\ndecreased in 1.01% ('decreased', '1.49%')\ndecreased in 2.09% ('increased', '2.21%')\nincreased in 2.31% ('increased', '3.84%')\nincreased in 2.4% ('decreased', '3.32%')\ndecreased in 5.42% ('increased', '0.33%')\nincreased in 1.22% ('decreased', '0.11%')\nincreased in 0.72% ('increased', '2.13%')\ndecreased in 2.64% ('increased', '2.74%')\nincreased in 2.49% ('decreased', '1.26%')\ndecreased in 1.65% ('increased', '1.72%')\nincreased in 0.65% ('decreased', '0.34%')\nincreased in 0.33% ('increased', '0.04%')\nincreased in 1.26% ('increased', '0.24%')\nincreased in 2.51% ('increased', '8.95%')\nincreased in 4.47% ('increased', '1.32%')\nincreased in 3.7% ('increased', '5.6%')\ndecreased in 4.34% ('increased', '0.07%')\nincreased in 0.27% ('decreased', '3.49%')\nincreased in 3.6% ('increased', '1.65%')\nincreased in 0.98% ('increased', '1.47%')\nincreased in 3.71% ('decreased', '3.56%')\ndecreased in 2.51% ('decreased', '7.49%')\ndecreased in 0.13% ('increased', '3.03%')\nincreased in 1.47% ('increased', '0.78%')\nincreased in 1.34% ('increased', '3.82%')\nincreased in 4.11% ('increased', '1.24%')\nincreased in 0.65% ('decreased', '4.28%')\ndecreased in 1.8% ('decreased', '0.04%')\ndecreased in 1.65% ('decreased', '2.03%')\nincreased in 0.22% ('decreased', '0.36%')\ndecreased in 1.7% ('increased', '7.12%')\nincreased in 2.0% ('increased', '2.94%')\ndecreased in 5.9% ('increased', '0.3%')\nincreased in 0.67% ('decreased', '1.9%')\nincreased in 0.15% ('decreased', '2.35%')\nincreased in 0.11% ('decreased', '0.22%')\nincreased in 2.37% ('decreased', '4.44%')\nincreased in 0.22% ('increased', '0.42%')\nincreased in 0.51% ('decreased', '0.3%')\ndecreased in 4.04% ('decreased', '0.33%')\ndecreased in 3.71% ('increased', '1.58%')\ndecreased in 1.11% ('increased', '1.21%')\nincreased in 1.01% ('decreased', '3.4%')\nincreased in 4.14% ('decreased', '2.7%')\ndecreased in 1.31% ('increased', '0.57%')\nincreased in 9.23% ('increased', '5.02%')\nincreased in 1.22% ('decreased', '0.76%')\ndecreased in 0.6% ('increased', '2.58%')\nincreased in 2.49% ('decreased', '2.94%')\nincreased in 3.01% ('decreased', '1.31%')\nincreased in 2.51% ('increased', '0.41%')\nincreased in 1.3% ('decreased', '2.37%')\ndecreased in 0.1% ('decreased', '0.89%')\nincreased in 3.71% ('increased', '3.01%')\nincreased in 4.34% ('decreased', '3.2%')\ndecreased in 2.25% ('decreased', '0.91%')\ndecreased in 0.47% ('increased', '0.49%')\nincreased in 1.98% ('decreased', '2.41%')\ndecreased in 1.2% ('decreased', '0.16%')\nincreased in 0.27% ('increased', '9.72%')\nincreased in 2.49% ('decreased', '0.65%')\nincreased in 0.33% ('decreased', '4.18%')\nincreased in 2.45% ('increased', '2.25%')\ndecreased in 1.2% ('decreased', '2.13%')\nincreased in 1.08% ('decreased', '0.48%')\nincreased in 4.23% ('decreased', '0.84%')\nincreased in 3.11% ('increased', '0.98%')\nincreased in 0.5% ('decreased', '2.72%')\nincreased in 6.3% ('increased', '0.91%')\nincreased in 0.55% ('increased', '1.23%')\nincreased in 2.27% ('increased', '1.71%')\nincreased in 1.9% ('increased', '0.86%')\nincreased in 1.33% ('increased', '5.39%')\nincreased in 1.45% ('increased', '3.03%')\ndecreased in 2.6% ('decreased', '0.47%')\ndecreased in 0.98% ('decreased', '2.92%')\nincreased in 1.23% ('increased', '0.23%')\nincreased in 2.37% ('decreased', '3.43%')\ndecreased in 1.3% ('decreased', '1.99%')\nincreased in 1.39% ('increased', '1.2%')\nincreased in 2.47% ('increased', '0.55%')\nincreased in 3.65% ('increased', '0.78%')\nincreased in 1.98% ('decreased', '5.35%')\ndecreased in 6.78% ('decreased', '5.3%')\nincreased in 1.62% ('decreased', '1.58%')\ndecreased in 0.6% ('increased', '2.73%')\nincreased in 0.11% ('decreased', '0.52%')\nincreased in 3.08% ('increased', '1.69%')\ndecreased in 1.0% ('increased', '1.23%')\nincreased in 1.5% ('decreased', '1.28%')\nincreased in 4.25% ('increased', '0.75%')\ndecreased in 0.37% ('decreased', '2.49%')\nincreased in 1.42% ('increased', '3.17%')\nincreased in 0.51% ('decreased', '1.28%')\ndecreased in 0.67% ('decreased', '5.84%')\nincreased in 1.14% ('decreased', '1.07%')\ndecreased in 7.63% ('increased', '11.33%')\ndecreased in 0.67% ('decreased', '1.46%')\nincreased in 4.11% ('increased', '2.2%')\nincreased in 1.6% ('increased', '1.83%')\nincreased in 0.51% ('decreased', '1.49%')\ndecreased in 0.02% ('increased', '2.21%')\ndecreased in 0.03% ('increased', '3.84%')\nincreased in 3.1% ('decreased', '3.32%')\ndecreased in 1.65% ('increased', '0.33%')\nincreased in 1.27% ('decreased', '0.11%')\ndecreased in 2.14% ('increased', '2.13%')\nincreased in 0.21% ('increased', '2.74%')\ndecreased in 0.3% ('decreased', '1.26%')\nincreased in 2.01% ('increased', '1.72%')\nincreased in 7.31% ('decreased', '0.34%')\ndecreased in 1.13% ('increased', '0.04%')\nincreased in 0.67% ('increased', '0.24%')\nincreased in 0.48% ('increased', '8.95%')\nincreased in 0.6% ('increased', '1.32%')\nincreased in 2.9% ('increased', '5.6%')\ndecreased in 4.65% ('increased', '0.07%')\ndecreased in 5.4% ('decreased', '3.49%')\nincreased in 0.11% ('increased', '1.65%')\ndecreased in 0.22% ('increased', '1.47%')\nincreased in 4.65% ('decreased', '3.56%')\nincreased in 0.64% ('decreased', '7.49%')\ndecreased in 0.62% ('increased', '3.03%')\nincreased in 0.13% ('increased', '0.78%')\nincreased in 2.1% ('increased', '3.82%')\nincreased in 2.37% ('increased', '1.24%')\ndecreased in 3.54% ('decreased', '4.28%')\nincreased in 1.31% ('decreased', '0.04%')\nincreased in 0.27% ('decreased', '2.03%')\ndecreased in 0.25% ('decreased', '0.36%')\nincreased in 0.72% ('increased', '7.12%')\nincreased in 0.43% ('increased', '2.94%')\ndecreased in 3.71% ('increased', '0.3%')\nincreased in 0.42% ('decreased', '1.9%')\ndecreased in 6.85% ('decreased', '2.35%')\ndecreased in 1.06% ('decreased', '0.22%')\nincreased in 3.27% ('decreased', '4.44%')\nincreased in 1.6% ('increased', '0.42%')\nincreased in 0.22% ('decreased', '0.3%')\nincreased in 1.5% ('decreased', '0.33%')\nincreased in 1.4% ('increased', '1.58%')\nincreased in 1.2% ('increased', '1.21%')\nincreased in 1.2% ('decreased', '3.4%')\nincreased in 0.43% ('decreased', '2.7%')\nincreased in 4.7% ('increased', '0.57%')\nincreased in 7.51% ('increased', '5.02%')\nincreased in 1.95% ('decreased', '0.76%')\nincreased in 2.51% ('increased', '2.58%')\nincreased in 0.42% ('decreased', '2.94%')\ndecreased in 1.7% ('decreased', '1.31%')\ndecreased in 3.6% ('increased', '0.41%')\ndecreased in 1.25% ('decreased', '2.37%')\ndecreased in 2.1% ('decreased', '0.89%')\nincreased in 0.1% ('increased', '3.01%')\nincreased in 3.2% ('decreased', '3.2%')\ndecreased in 2.14% ('decreased', '0.91%')\ndecreased in 1.98% ('increased', '0.49%')\nincreased in 0.72% ('decreased', '2.41%')\nincreased in 1.67% ('decreased', '0.16%')\nincreased in 0.14% ('increased', '9.72%')\nincreased in 0.08% ('decreased', '0.65%')\nincreased in 1.6% ('decreased', '4.18%')\nincreased in 1.47% ('increased', '2.25%')\nincreased in 2.4% ('decreased', '2.13%')\ndecreased in 4.98% ('decreased', '0.48%')\nincreased in 3.02% ('decreased', '0.84%')\ndecreased in 1.14% ('increased', '0.98%')\nincreased in 0.51% ('decreased', '2.72%')\ndecreased in 5.8% ('increased', '0.91%')\ndecreased in 3.33% ('increased', '1.23%')\nincreased in 1.02% ('increased', '1.71%')\nincreased in 0.46% ('increased', '0.86%')\nincreased in 0.46% ('increased', '5.39%')\ndecreased in 2.47% ('increased', '3.03%')\nincreased in 0.13% ('decreased', '0.47%')\nincreased in 0.91% ('decreased', '2.92%')\nincreased in 0.23% ('increased', '0.23%')\ndecreased in 3.9% ('decreased', '3.43%')\nincreased in 5.26% ('decreased', '1.99%')\nincreased in 2.9% ('increased', '1.2%')\nincreased in 1.06% ('increased', '0.55%')\nincreased in 0.33% ('increased', '0.78%')\ndecreased in 1.91% ('decreased', '5.35%')\ndecreased in 5.4% ('decreased', '5.3%')\nincreased in 0.48% ('decreased', '1.58%')\nincreased in 4.14% ('increased', '2.73%')\ndecreased in 1.47% ('decreased', '0.52%')\nincreased in 0.37% ('increased', '1.69%')\ndecreased in 0.63% ('increased', '1.23%')\nincreased in 0.33% ('decreased', '1.28%')\nincreased in 1.31% ('increased', '0.75%')\nincreased in 0.64% ('decreased', '2.49%')\nincreased in 1.73% ('increased', '3.17%')\nincreased in 3.7% ('decreased', '1.28%')\nincreased in 0.14% ('decreased', '5.84%')\nincreased in 1.65% ('decreased', '1.07%')\ndecreased in 5.97% ('increased', '11.33%')\nincreased in 1.64% ('decreased', '1.46%')\nincreased in 4.21% ('increased', '2.2%')\nincreased in 0.42% ('increased', '1.83%')\nincreased in 1.27% ('decreased', '1.49%')\nincreased in 1.01% ('increased', '2.21%')\nincreased in 2.26% ('increased', '3.84%')\nincreased in 2.49% ('decreased', '3.32%')\nincreased in 2.6% ('increased', '0.33%')\nincreased in 2.4% ('decreased', '0.11%')\nincreased in 0.47% ('increased', '2.13%')\nincreased in 0.67% ('increased', '2.74%')\ndecreased in 2.21% ('decreased', '1.26%')\nincreased in 0.55% ('increased', '1.72%')\nincreased in 6.25% ('decreased', '0.34%')\ndecreased in 5.7% ('increased', '0.04%')\nincreased in 2.6% ('increased', '0.24%')\nincreased in 0.91% ('increased', '8.95%')\nincreased in 3.1% ('increased', '1.32%')\nincreased in 1.51% ('increased', '5.6%')\ndecreased in 1.67% ('increased', '0.07%')\nincreased in 2.37% ('decreased', '3.49%')\nincreased in 2.42% ('increased', '1.65%')\nincreased in 2.6% ('increased', '1.47%')\nincreased in 3.62% ('decreased', '3.56%')\nincreased in 0.65% ('decreased', '7.49%')\nincreased in 0.43% ('increased', '3.03%')\nincreased in 4.26% ('increased', '0.78%')\nincreased in 6.23% ('increased', '3.82%')\nincreased in 3.6% ('increased', '1.24%')\nincreased in 1.67% ('decreased', '4.28%')\nincreased in 2.1% ('decreased', '0.04%')\nincreased in 2.6% ('decreased', '2.03%')\ndecreased in 0.15% ('decreased', '0.36%')\nincreased in 0.47% ('increased', '7.12%')\nincreased in 2.47% ('increased', '2.94%')\ndecreased in 5.51% ('increased', '0.3%')\nincreased in 2.64% ('decreased', '1.9%')\nincreased in 2.4% ('decreased', '2.35%')\nincreased in 2.33% ('decreased', '0.22%')\nincreased in 2.11% ('decreased', '4.44%')\nincreased in 0.27% ('increased', '0.42%')\nincreased in 0.43% ('decreased', '0.3%')\nincreased in 1.13% ('decreased', '0.33%')\nincreased in 0.27% ('increased', '1.58%')\nincreased in 2.19% ('increased', '1.21%')\ndecreased in 1.27% ('decreased', '3.4%')\nincreased in 2.42% ('decreased', '2.7%')\nincreased in 0.23% ('increased', '0.57%')\ndecreased in 3.6% ('increased', '5.02%')\ndecreased in 1.11% ('decreased', '0.76%')\nincreased in 0.46% ('increased', '2.58%')\ndecreased in 1.37% ('decreased', '2.94%')\nincreased in 1.65% ('decreased', '1.31%')\nincreased in 5.35% ('increased', '0.41%')\ndecreased in 2.1% ('decreased', '2.37%')\nincreased in 3.8% ('decreased', '0.89%')\nincreased in 1.31% ('increased', '3.01%')\ndecreased in 2.47% ('decreased', '3.2%')\ndecreased in 6.02% ('decreased', '0.91%')\nincreased in 2.1% ('increased', '0.49%')\nincreased in 2.47% ('decreased', '2.41%')\nincreased in 2.21% ('decreased', '0.16%')\nincreased in 2.03% ('increased', '9.72%')\nincreased in 4.27% ('decreased', '0.65%')\nincreased in 1.64% ('decreased', '4.18%')\ndecreased in 0.1% ('increased', '2.25%')\nincreased in 2.3% ('decreased', '2.13%')\nincreased in 1.27% ('decreased', '0.48%')\ndecreased in 0.14% ('decreased', '0.84%')\nincreased in 3.19% ('increased', '0.98%')\nincreased in 3.6% ('decreased', '2.72%')\nincreased in 3.33% ('increased', '0.91%')\nincreased in 3.71% ('increased', '1.23%')\nincreased in 0.14% ('increased', '1.71%')\nincreased in 1.2% ('increased', '0.86%')\nincreased in 5.45% ('increased', '5.39%')\nincreased in 2.14% ('increased', '3.03%')\nincreased in 1.21% ('decreased', '0.47%')\ndecreased in 4.11% ('decreased', '2.92%')\nincreased in 4.23% ('increased', '0.23%')\nincreased in 0.48% ('decreased', '3.43%')\ndecreased in 2.01% ('decreased', '1.99%')\nincreased in 0.15% ('increased', '1.2%')\ndecreased in 1.4% ('increased', '0.55%')\nincreased in 1.06% ('increased', '0.78%')\n52.33333333333333 %\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Useful sources\n- https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n- https://www.philschmid.de/fsdp-qlora-llama3\n\n- https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa\n- https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset","metadata":{}}]}