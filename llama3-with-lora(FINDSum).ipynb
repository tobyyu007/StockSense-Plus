{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train llama3 with LoRA\n[Source of this notebook](https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa)\n\nThis notebook is designed for Kaggle notebook with 2 Nvidia T4 GPUs","metadata":{}},{"cell_type":"markdown","source":"### Enviornment setup\n- Set your `HF_TOKEN` at `Add-ons -> Secrets`","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")\n# HF_TOKEN_WRITE = user_secrets.get_secret(\"HF_TOKEN_WRITE\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-05T00:07:36.173034Z","iopub.execute_input":"2024-05-05T00:07:36.173473Z","iopub.status.idle":"2024-05-05T00:07:36.261309Z","shell.execute_reply.started":"2024-05-05T00:07:36.173438Z","shell.execute_reply":"2024-05-05T00:07:36.259544Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Install Pytorch for FSDP and FA/SDPA\n%pip install \"torch==2.2.2\" tensorboard\n \n# Install Hugging Face libraries\n%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T20:25:04.274530Z","iopub.execute_input":"2024-05-04T20:25:04.275134Z","iopub.status.idle":"2024-05-04T20:28:56.144262Z","shell.execute_reply.started":"2024-05-04T20:25:04.275085Z","shell.execute_reply":"2024-05-04T20:28:56.142361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $HF_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-05-05T00:07:38.254587Z","iopub.execute_input":"2024-05-05T00:07:38.254994Z","iopub.status.idle":"2024-05-05T00:07:39.857217Z","shell.execute_reply.started":"2024-05-05T00:07:38.254949Z","shell.execute_reply":"2024-05-05T00:07:39.856059Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load and prepare the dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n \n# Convert dataset to OAI messages\nsystem_message = \"\"\"You are a seasoned stock market analyst. What is the summary of this financial text\"\"\"\n\ndef create_conversation(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": sample[\"document\"]},\n            {\"role\": \"assistant\", \"content\": sample[\"summary\"]}\n        ]\n    }\n#     return {\n#         \"messages\": [\n#             {\"role\": \"system\", \"content\": sample[\"instruction\"]},\n#             {\"role\": \"user\", \"content\": sample[\"input\"]},\n#             {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n#         ]\n#     }\n \n# Load dataset from the hub\ndataset = load_dataset(\"tobyyu007/Financial\", split=\"train\")\n\ndataset = dataset.select(range(0, 2500))\n \n# Convert dataset to OAI messages\ndataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n# split dataset into 10,000 training samples and 2,500 test samples\ndataset = dataset.train_test_split(test_size=500/2500, seed=42)\n\nprint(dataset[\"train\"][345][\"messages\"])\n\n# save datasets to disk\ndataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\ndataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T00:15:52.876395Z","iopub.execute_input":"2024-05-05T00:15:52.877094Z","iopub.status.idle":"2024-05-05T00:15:54.915718Z","shell.execute_reply.started":"2024-05-05T00:15:52.877052Z","shell.execute_reply":"2024-05-05T00:15:54.914144Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f80e2e75f2174e699bec593056bdaeaf"}},"metadata":{}},{"name":"stdout","text":"[{'content': 'You are a seasoned stock market analyst. What is the summary of this financial text', 'role': 'system'}, {'content': \"we sold the 13 stations previously owned or operated by tribune for $ 1.008 billion in cash , including working capital adjustments . we sold the eight stations that we previously owned for $ 358.6 million in cash , including working capital adjustments . these divestitures resulted in a net gain on disposal of $ 96.1 million . the cash consideration , the repayment of tribune debt , including premium and accrued interest , and the related fees and expenses were funded through a combination of cash on hand of nexstar and tribune , proceeds from the station divestitures , new term loan borrowings and the issuance of new notes ( see “ debt transactions ” below ) . see also note 3—acquisitions and dispositions and note 9—debt to our consolidated financial statements in part iv , item 15 ( a ) of this annual report on form 10-k for additional information about the merger and recently issued debt . 46 future acquisitions and dispositions on november 5 , 2019 , we entered into purchase and sale agreements with fox whereby we will purchase the fox affiliate wjzy and the mynetworktv affiliate wmyt in the charlotte , nc market from fox for approximately $ 45 million in cash , and will sell to fox the fox affiliate kcpq and the mynetworktv affiliate kzjo in the seattle , wa market , as well as the fox affiliate witi in the milwaukee , wi market , for approximately $ 350 million in cash , subject to customary adjustments . the transaction , which has received fcc approval , closed on march 2 , 2020. on january 14 , 2020 , we sold our sports betting information website business to star enterprises ltd. , a subsidiary of alto holdings , ltd. , for total cash consideration of $ 14.4 million . on january 27 , 2020 , we and sinclair agreed to settle the outstanding lawsuit between tribune and sinclair in connection with their terminated merger agreement . tribune is an entity acquired by nexstar in september 2019. as part of the resolution , sinclair has agreed to sell to us television station wdky-tv in the lexington , ky dma , subject to fcc approval and other customary conditions . sinclair has also sold to us certain non-license assets associated with television station kgbt-tv in the harlingen-weslaco-brownsville-mcallen , texas dma . we and sinclair have also modified an existing agreement regarding carriage of certain of sinclair 's digital networks by stations we acquired in connection with the tribune acquisition . finally , on january 28 , 2020 , sinclair made a cash payment to nexstar in an amount that represents the amount of $ 60.0 million plus the payments made or to be made by nexstar with respect to wdky and the kgbt non-license assets purchases . 2019 debt transactions on july 3 , 2019 , we completed the sale and issuance of our $ 1.120 billion 5.625 % notes due 2027. on september 19 , 2019 , we borrowed $ 3.065 billion in new term loan b , issued at 99.21 % , and $ 675.0 million in new term loan a , issued at 99.31 % . the proceeds from these transactions , plus proceeds from the sale of certain television station assets and cash on hand , were used to finance the purchase price of our merger with tribune and to pay the related fees and expenses . on november 22 , 2019 , we issued an additional $ 665.0 million aggregate principal amount of 5.625 % notes due 2027 at an offering price of 104.875 % , which resulted in a debt premium of $ 27.4 million after giving effect to certain fees relating thereto . the proceeds from the notes were used to redeem our $ 400.0 million aggregate principal amount of 5.875 % senior unsecured notes due 2022 ( the “ 5.875 % notes ” ) and our $ 275.0 million aggregate principal amount of 6.125 % senior unsecured notes due 2022 ( the “ 6.125 % notes ” ) . on november 29 , 2019 , mission paid the outstanding principal balances of marshall 's loans to third party bank lenders totaling $ 48.9 million , plus accrued and unpaid interest . after making the payment , mission became marshall 's new lender under the same marshall credit agreement . in 2019 , we prepaid a total of $ 180.0 million in principal balance under our term loan b , funded by cash on hand . through december 2019 , the company repaid scheduled maturities of $ 47.3 million under its term loan a and term loan b . 47 overview of operations as of december 31 , 2019 , we owned , operated , programmed or provided sales and other services to 197 full power television stations , including those owned by vies , in 115 markets in the states of alabama , arkansas , california , colorado , connecticut , district of columbia , florida , georgia , hawaii , illinois , indiana , iowa , kansas , louisiana , maryland , massachusetts , michigan , mississippi , missouri , montana , nevada , new mexico , new york , north carolina , north dakota , ohio , oklahoma , oregon , pennsylvania , rhode island , south carolina , south dakota , tennessee , texas , utah , vermont , virginia , washington , west virginia , and wisconsin . the stations are affiliates of abc , nbc , fox , cbs , the cw , mntv and other broadcast television networks . through various local service agreements , we provided sales , programming and other services to 36 full power television stations owned by independent third parties , of which 32 full power television stations are vies that are consolidated into our financial statements . story_separator_special_tag station direct operating expenses , consisting primarily of news , engineering , programming and selling , general and administrative expenses ( net of trade expense ) were $ 1.872 billion for the year ended december 31 , 2019 , compared to $ 1.570 billion for the same period in 2018 , an increase of $ 302.0 million , or 19.2 % . the increase was primarily due to expenses of our newly acquired stations and entities , mainly tribune , of $ 247.3 million ( including network and programming costs of $ 157.0 million ) , partially offset by a decrease of $ 18.0 million related to our station divestitures . additionally , our legacy stations ' programming costs increased by $ 96.6 million primarily due to network affiliation renewals and annual increases in our network affiliation costs . these increases were partially offset by an $ 18.6 million decrease in the operating expenses of our digital products due primarily to marketplace changes and challenges that led to lower revenue . depreciation of property and equipment was $ 123.4 million for the year ended december 31 , 2019 , compared to $ 109.8 million for the same period in 2018 , an increase of $ 13.6 million , or 12.4 % . this was primarily due to incremental depreciation related to assets acquired in the merger of $ 9.0 million and increased depreciation from related station repacking activities . amortization of intangible assets was $ 200.3 million for the year ended december 31 , 2019 , compared to $ 149.4 million for the same period in 2018 , an increase of $ 50.9 million , or 34.1 % . this was primarily due to increased amortization related to intangible assets acquired in the merger of $ 59.9 million , partially offset by decreases in amortization from certain fully amortized assets . 51 amortization of broadcast rights , excluding barter w as $ 85.0 million for the year ended december 31 , 2019 , compared to $ 61.3 million for the same period in 2018 , an increase of $ 23.7 million , or 38.6 % . this was primarily attributable to incremental amortization resulting from new broadcast rights acquired through the merger of $ 30 . 5 million . th is i ncrease w as partially offset by a reduction in amortization costs on our legacy stations due to renegotiation of certain film contracts which resulted in reduced distribution rates . certain of the company 's stations were assigned to new channels ( “ repack ” ) in connection with the fcc 's process of repurposing a portion of the broadcast television spectrum for wireless broadband use . the company 's stations are currently spending costs , mainly capital expenditures , to construct and license the necessary technical modifications to operate on their newly assigned channels and to vacate their former channels no later than july 13 , 2020. subject to fund limitations , the fcc reimburses television broadcasters , mvpds and other parties for costs reasonably incurred due to the repack . in 2019 and 2018 , we received a total of $ 70.4 million and $ 29.4 million , respectively , in reimbursements from the fcc which we recognized as operating income . in the third quarter of 2019 , we recorded a $ 63.3 million goodwill and intangible assets impairment on our digital reporting unit due to deterioration in customer relationships , mainly driven by marketplace changes on select demand-side platform customers , that led to a long-term projected decrease in operating results . in connection with the merger , we sold the assets of 21 full power television stations in 16 markets , eight of which were previously owned by us and 13 of which were previously owned and operated by tribune . we sold the tribune stations for $ 1.008 billion in cash , including working capital adjustments , and we sold our stations for $ 358.6 million in cash , including working capital adjustments . these divestitures resulted in a net gain on disposal of $ 96.1 million . income on equity investments , net in connection with our merger with tribune completed on september 19 , 2019 , we acquired a 31.3 % ownership stake in tv food network . from the date of acquisition to december 31 , 2019 , nexstar recognized equity in income from this investment of $ 20.5 million , along with loss from other equity method investments of $ 2.6 million . interest expense , net interest expense , net was $ 304.3 million for the year ended december 31 , 2019 , compared to $ 221.0 million for the same period in 2018 , an increase of $ 83.4 million , or 37.7 % , primarily due to interest on new borrowings of $ 87.0 million and one time fees associated with the financing of our merger with tribune of $ 26.6 million . these increases were partially offset by decreases in debt related interest expense of $ 23.7 million , primarily due to prepayments and scheduled repayments of term loans and redemption of bonds , and interest income we earned from an escrow deposit during the third quarter of 2019 of $ 4.9 million and a reduction in interest from our existing term loans due to principal prepayments and scheduled repayments . story_separator_special_tag style= `` background-color : # ffffff ; margin-top:10pt ; margin-bottom:0pt ; text-indent:5.15 % ; color : # 212529 ; font-family : times new roman ; font-size:10pt ; font-weight : normal ; font-style : normal ; text-transform : none ; font-variant : normal ; `` > corporate expenses , related to costs associated with the centralized management of our stations , were $ 110.9 million for the year ended december 31 , 2018 , compared to $ 138.4 million for the same period in 2017 , a decrease of $ 27.5 million , or 19.9 % . this was primarily attributable to a decrease\\n\", 'role': 'user'}, {'content': \"loss on extinguishment of debt loss on extinguishment of debt was $ 10.3 million for the year ended december 31 , 2019 , compared to $ 12.1 million for the same period in 2018 , a decrease of $ 1.8 million , or 15.0 % . in november 2019 , we redeemed our $ 400.0 million 5.875 % notes and our $ 275.0 million 6.125 % notes . we also made prepayments of our outstanding term loans during 2019. these transactions resulted in total loss on extinguishment of debt of $ 10.3 million . in october 2018 , the company refinanced its then existing term loans and revolving loans . we also made various prepayments of outstanding term loans during 2018. these transactions resulted in total loss on extinguishment of debt of $ 12.1 million . income taxes income tax expense was $ 137.0 million for the year ended december 31 , 2019 , compared to an income tax expense of $ 144.7 million for the same period in 2018 , a decrease in income tax expense of $ 7.7 million . the effective tax rates during the years ended december 31 , 2019 and 2018 were 36.8 % and 27.1 % , respectively . in 2019 , we recognized the tax impact of the divested stations previously owned by us including an income tax expense of $ 10.3 million , or an increase to the effective tax rate of 2.8 % , attributable to nondeductible goodwill written off as a result of the sale . we also recognized an impairment loss on our reporting unit 's goodwill and intangible assets ( see note 4 ) . the impairment loss related to goodwill is not deductible for purposes of calculating the tax provision resulting in an income tax expense of $ 8.9 million , or an increase to the effective tax rate of 2.4 % . valuation allowance increased by $ 19.9 million , or an increase to the effective tax rate of 5.3 % , primarily due to the company 's belief , based upon consideration of positive and negative evidence , that certain deferred tax assets related to one of the vies were not likely to be realized .\\n\", 'role': 'assistant'}]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46ead85945b141fbabee38bcbe66e53e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089e2fbdf7f146c4ae83827fa0438477"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"6833415"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load base model and setup training parameters","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load jsonl data from disk\ndataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.591194Z","iopub.execute_input":"2024-05-04T19:21:51.591717Z","iopub.status.idle":"2024-05-04T19:21:51.769964Z","shell.execute_reply.started":"2024-05-04T19:21:51.591691Z","shell.execute_reply":"2024-05-04T19:21:51.768976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile llama_3_8b_fsdp_qlora.yaml\n# script parameters\nmodel_id: \"meta-llama/Meta-Llama-3-8B-Instruct\" # Hugging Face model id\ndataset_path: \".\"                      # path to dataset\nmax_seq_len:  2048 # 2048              # max sequence length for model and packing of the dataset\n# training parameters\noutput_dir: \"/home/jupyter/llama-3-8b-FinGPT\" # Temporary output directory for model checkpoints\nreport_to: \"tensorboard\"               # report metrics to tensorboard\nlearning_rate: 0.0001                  # learning rate 2e-4\nlr_scheduler_type: \"constant\"          # learning rate scheduler\nnum_train_epochs: 2                    # number of training epochs\nper_device_train_batch_size: 1         # batch size per device during training\nper_device_eval_batch_size: 1          # batch size for evaluation\ngradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\noptim: adamw_torch                     # use torch adamw optimizer\nlogging_steps: 10                      # log every 10 steps\nsave_strategy: epoch                   # save checkpoint every epoch\nevaluation_strategy: epoch             # evaluate every epoch\nmax_grad_norm: 0.3                     # max gradient norm\nwarmup_ratio: 0.03                     # warmup ratio\nbf16: false                             # use bfloat16 precision\ntf32: false                             # use tf32 precision\ngradient_checkpointing: true           # use gradient checkpointing to save memory\nhub_private_repo: true\n# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\nfsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\nfsdp_config:\n  backward_prefetch: \"backward_pre\"\n  forward_prefetch: \"true\"\n  use_orig_params: \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.771329Z","iopub.execute_input":"2024-05-04T19:21:51.772284Z","iopub.status.idle":"2024-05-04T19:21:51.780646Z","shell.execute_reply.started":"2024-05-04T19:21:51.772246Z","shell.execute_reply":"2024-05-04T19:21:51.779589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile run_fsdp_qlora.py\nimport logging\nfrom dataclasses import dataclass, field\nimport os\nimport random\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, TrainingArguments\nfrom trl.commands.cli_utils import  TrlParser\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n        set_seed,\n\n)\nfrom trl import setup_chat_format\nfrom peft import LoraConfig\n\n\nfrom trl import (\n   SFTTrainer)\n\n# Comment in if you want to use the Llama 3 instruct template but make sure to add modules_to_save\n# LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n\n# Anthropic/Vicuna like template without the need for special tokens\nLLAMA_3_CHAT_TEMPLATE = (\n    \"{% for message in messages %}\"\n        \"{% if message['role'] == 'system' %}\"\n            \"{{ message['content'] }}\"\n        \"{% elif message['role'] == 'user' %}\"\n            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n        \"{% elif message['role'] == 'assistant' %}\"\n            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n        \"{% endif %}\"\n    \"{% endfor %}\"\n    \"{% if add_generation_prompt %}\"\n    \"{{ '\\n\\nAssistant: ' }}\"\n    \"{% endif %}\"\n)\n\n\n# ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n\n@dataclass\nclass ScriptArguments:\n    dataset_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"Path to the dataset\"\n        },\n    )\n    model_id: str = field(\n        default=None, metadata={\"help\": \"Model ID to use for SFT training\"}\n    )\n    max_seq_length: int = field(\n        default=512, metadata={\"help\": \"The maximum sequence length for SFT Trainer\"}\n    )\n\n\ndef training_function(script_args, training_args):\n    ################\n    # Dataset\n    ################\n    \n    train_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"train_dataset.json\"),\n        split=\"train\",\n    )\n    test_dataset = load_dataset(\n        \"json\",\n        data_files=os.path.join(script_args.dataset_path, \"test_dataset.json\"),\n        split=\"train\",\n    )\n\n    ################\n    # Model & Tokenizer\n    ################\n\n    # Tokenizer        \n    tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n    \n    # template dataset\n    def template_dataset(examples):\n        return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n    \n    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n    \n    # print random sample\n    with training_args.main_process_first(\n        desc=\"Log a few random samples from the processed training set\"\n    ):\n        for index in random.sample(range(len(train_dataset)), 2):\n            print(train_dataset[index][\"text\"])\n\n    # Model    \n    torch_dtype = torch.bfloat16\n    quant_storage_dtype = torch.bfloat16\n\n    quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch_dtype,\n            bnb_4bit_quant_storage=quant_storage_dtype,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        script_args.model_id,\n        quantization_config=quantization_config,\n        attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n        torch_dtype=quant_storage_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,  # this is needed for gradient checkpointing\n    )\n    \n    if training_args.gradient_checkpointing:\n        model.gradient_checkpointing_enable()\n\n    ################\n    # PEFT\n    ################\n\n    # LoRA config based on QLoRA paper & Sebastian Raschka experiment\n    peft_config = LoraConfig(\n        lora_alpha=8,\n        lora_dropout=0.05,\n        r=16,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n#         modules_to_save = [\"lm_head\", \"embed_tokens\"] # add if you want to use the Llama 3 instruct template\n    )\n\n    ################\n    # Training\n    ################\n    trainer = SFTTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        dataset_text_field=\"text\",\n        eval_dataset=test_dataset,\n        peft_config=peft_config,\n        max_seq_length=script_args.max_seq_length,\n        tokenizer=tokenizer,\n        hub_private_repo=True,\n        packing=True,\n        dataset_kwargs={\n            \"add_special_tokens\": False,  # We template with special tokens\n            \"append_concat_token\": False,  # No need to add additional separator token\n        },\n    )\n    if trainer.accelerator.is_main_process:\n        trainer.model.print_trainable_parameters()\n\n    ##########################\n    # Train model\n    ##########################\n    checkpoint = None\n    if training_args.resume_from_checkpoint is not None:\n        checkpoint = training_args.resume_from_checkpoint\n    trainer.train(resume_from_checkpoint=checkpoint)\n\n    ##########################\n    # SAVE MODEL FOR SAGEMAKER\n    ##########################\n    if trainer.is_fsdp_enabled:\n        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n    trainer.save_model()\n    \nif __name__ == \"__main__\":\n    parser = TrlParser((ScriptArguments, TrainingArguments))\n    script_args, training_args = parser.parse_args_and_config()    \n    \n    # set use reentrant to False\n    if training_args.gradient_checkpointing:\n        training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n    # set seed\n    set_seed(training_args.seed)\n  \n    # launch training\n    training_function(script_args, training_args)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.782157Z","iopub.execute_input":"2024-05-04T19:21:51.782484Z","iopub.status.idle":"2024-05-04T19:21:51.799266Z","shell.execute_reply.started":"2024-05-04T19:21:51.782453Z","shell.execute_reply":"2024-05-04T19:21:51.798366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"markdown","source":"##### Release unreferenced memory in Python","metadata":{}},{"cell_type":"code","source":"import gc\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T19:21:51.800223Z","iopub.execute_input":"2024-05-04T19:21:51.800484Z","iopub.status.idle":"2024-05-04T19:21:51.890728Z","shell.execute_reply.started":"2024-05-04T19:21:51.800461Z","shell.execute_reply":"2024-05-04T19:21:51.889639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Start training with torchrun","metadata":{}},{"cell_type":"code","source":"!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=2 ./run_fsdp_qlora.py --config llama_3_8b_fsdp_qlora.yaml","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T19:21:51.891967Z","iopub.execute_input":"2024-05-04T19:21:51.892315Z","iopub.status.idle":"2024-05-04T20:08:03.015934Z","shell.execute_reply.started":"2024-05-04T19:21:51.892287Z","shell.execute_reply":"2024-05-04T20:08:03.014742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n \npeft_model_id = \"/home/jupyter/llama-3-8b-FinGPT\"\n \n# Load Model with PEFT adapter\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n  peft_model_id,\n  torch_dtype=torch.float16,\n  quantization_config= {\"load_in_4bit\": True},\n  device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:08:06.899019Z","iopub.execute_input":"2024-05-04T20:08:06.899661Z","iopub.status.idle":"2024-05-04T20:08:51.127302Z","shell.execute_reply.started":"2024-05-04T20:08:06.899614Z","shell.execute_reply":"2024-05-04T20:08:51.126310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom random import randint\n \n\n# Load our test dataset\neval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\nrand_idx = 2\nmessages = eval_dataset[rand_idx][\"messages\"][:2]\n \n# Test on sample\ninput_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id= tokenizer.eos_token_id,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\n\nprint(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\nprint(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\nprint(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n\n# **Query:**\n# How long was the Revolutionary War?\n# **Original Answer:**\n# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783.\n# **Generated Answer:**\n# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783.","metadata":{"execution":{"iopub.status.busy":"2024-05-04T20:12:38.449378Z","iopub.execute_input":"2024-05-04T20:12:38.450119Z","iopub.status.idle":"2024-05-04T20:12:42.365284Z","shell.execute_reply.started":"2024-05-04T20:12:38.450077Z","shell.execute_reply":"2024-05-04T20:12:42.364370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the model","metadata":{}},{"cell_type":"markdown","source":"##### Zip the lora file\nDownload manually from the output section in the sidebar","metadata":{}},{"cell_type":"code","source":"# !ls\n# !zip -0 -r llama-3-8b-FinGPT.zip /home/jupyter/llama-3-8b-FinGPT","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Merge PEFT and base model","metadata":{}},{"cell_type":"code","source":"# # #### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n# from peft import AutoPeftModelForCausalLM\n# import torch\n \n# # Load PEFT model on CPU\n# model = AutoPeftModelForCausalLM.from_pretrained(\n#     \"/home/jupyter/llama-3-8b-FinGPT\",\n#     torch_dtype=torch.float16,\n#     low_cpu_mem_usage=True,\n# )\n# # Merge LoRA and base model and save\n# merged_model = model.merge_and_unload()\n\n# # Save locally\n# merged_model.save_pretrained(\"/home/jupyter/llama-3-8b-FinGPT-Merged\",safe_serialization=True, max_shard_size=\"2GB\")\n\n# # Publish to Huggingface\n# merged_model.push_to_hub(\"my-awesome-model\", safe_serialization=True, max_shard_size=\"2GB\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T21:12:36.992766Z","iopub.execute_input":"2024-05-04T21:12:36.994111Z","iopub.status.idle":"2024-05-04T21:21:41.707975Z","shell.execute_reply.started":"2024-05-04T21:12:36.994064Z","shell.execute_reply":"2024-05-04T21:21:41.704732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Useful sources\n- https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\n- https://www.philschmid.de/fsdp-qlora-llama3\n\n- https://www.philschmid.de/fsdp-qlora-llama3#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa\n- https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset","metadata":{}}]}